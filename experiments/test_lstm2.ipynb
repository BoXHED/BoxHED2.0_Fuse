{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import print_function\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import copy\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# from modeling_xlnet import XLNetModel, XLNetPreTrainedModel\n",
    "# from modeling_utils import PreTrainedModel, prune_linear_layer, SequenceSummary, PoolerAnswerClass, PoolerEndLogits, PoolerStartLogits\n",
    "               \n",
    "class clinical_xlnet_lstm_FAST(nn.Sequential):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "            super(clinical_xlnet_lstm_FAST, self).__init__()\n",
    "            \n",
    "            self.intermediate_size = 1536\n",
    "            self.num_attention_heads = 12\n",
    "            self.attention_probs_dropout_prob = 0.1\n",
    "            self.hidden_dropout_prob = 0.1\n",
    "            self.hidden_size_encoder = 64\n",
    "            self.n_layers = 2\n",
    "            self.hidden_size_xlnet = 64\n",
    "            \n",
    "            self.encoder = nn.LSTM(input_size = self.hidden_size_encoder, hidden_size = self.hidden_size_encoder, num_layers = 2, bidirectional = True, batch_first = False)    \n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Dropout(p=self.hidden_dropout_prob),\n",
    "                nn.Linear(self.hidden_size_encoder*2, 32),\n",
    "                nn.ReLU(True),\n",
    "                #output layer\n",
    "                nn.Linear(32, 1)\n",
    "            )\n",
    "            \n",
    "    def forward(self, xlnet_outputs):\n",
    "           \n",
    "            self.encoder.flatten_parameters()\n",
    "            output, (a, b) = self.encoder(xlnet_outputs)\n",
    "\n",
    "            print('a.shape', a.shape)\n",
    "            print('b.shape', b.shape)\n",
    "    \n",
    "            last_layer = output[-1]\n",
    "            print('last_layer shape:', last_layer.shape)\n",
    "            score = self.decoder(last_layer)\n",
    "            \n",
    "            return score\n",
    "               \n",
    "        \n",
    "# class clinical_xlnet_seq(XLNetPreTrainedModel):\n",
    "#     '''\n",
    "#     Uses the XLNetModel transformer and a linear decoder\n",
    "#     '''\n",
    "    \n",
    "#     def __init__(self, config):\n",
    "#             super(clinical_xlnet_seq, self).__init__(config)\n",
    "           \n",
    "#             self.hidden_size_xlnet = 768\n",
    "            \n",
    "#             self.transformer = XLNetModel(config)\n",
    "#             self.sequence_summary = SequenceSummary(config)\n",
    "         \n",
    "#             self.decoder = nn.Sequential(\n",
    "#                 nn.Linear(self.hidden_size_xlnet, 32),\n",
    "#                 nn.ReLU(True),\n",
    "#                 #output layer\n",
    "#                 nn.Linear(32, 1)\n",
    "#             )\n",
    "            \n",
    "#     def forward(self, input_ids, seg_ids, masks):\n",
    "#             output = self.sequence_summary(self.transformer(input_ids, token_type_ids = seg_ids, attention_mask = masks)[0])\n",
    "            \n",
    "#             score = self.decoder(output)\n",
    "            \n",
    "#             return score, output        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv('/home/ugrads/a/aa_ron_su/BoXHED_Fuse/JSS_SUBMISSION_NEW/data/final/Clinical-T5-Base_rad_out/3/from_epoch9/till_end_mimic_iv_extra_features_test.csv')\n",
    "# train = pd.read_csv('/home/ugrads/a/aa_ron_su/BoXHED_Fuse/JSS_SUBMISSION_NEW/data/final/Clinical-T5-Base_rad_out/3/from_epoch9/till_end_mimic_iv_extra_features_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ugrads/a/aa_ron_su/BoXHED_Fuse/experiments/test_lstm2.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcse-stmi-s1.cse.tamu.edu/home/ugrads/a/aa_ron_su/BoXHED_Fuse/experiments/test_lstm2.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcse-stmi-s1.cse.tamu.edu/home/ugrads/a/aa_ron_su/BoXHED_Fuse/experiments/test_lstm2.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m emb_df \u001b[39m=\u001b[39m test[[\u001b[39m'\u001b[39m\u001b[39memb\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m64\u001b[39m)]]\u001b[39m.\u001b[39miloc[:\u001b[39m10\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcse-stmi-s1.cse.tamu.edu/home/ugrads/a/aa_ron_su/BoXHED_Fuse/experiments/test_lstm2.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m emb_df \u001b[39m=\u001b[39m emb_df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: torch\u001b[39m.\u001b[39mtensor(x, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcse-stmi-s1.cse.tamu.edu/home/ugrads/a/aa_ron_su/BoXHED_Fuse/experiments/test_lstm2.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m emb_df\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "emb_df = test[['emb' + str(i) for i in range(64)]].iloc[:10]\n",
    "emb_df = emb_df.apply(lambda x: torch.tensor(x, dtype=torch.float), axis=1)\n",
    "emb_df\n",
    "\n",
    "embs = torch.stack(tuple(emb_df))\n",
    "embs_batch = torch.stack((embs, embs, embs, embs))\n",
    "embs_batch = embs_batch.permute(1,0,2)\n",
    "print('embs_batch shape:', embs_batch.shape)\n",
    "clin_lstm = clinical_xlnet_lstm_FAST()\n",
    "'''\n",
    "input: (L, N, H_in)\n",
    "    L:    Sequence Length\n",
    "    N:    batch size\n",
    "    H_in: input size\n",
    "\n",
    "    In my case, I have sequence length 10, batch size 4, and input size 64\n",
    "\n",
    "output: (L, D*H_out)\n",
    "'''\n",
    "output = clin_lstm(embs_batch)\n",
    "print('output shape:', output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 64])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs.shape\n",
    "# sequence length: 10\n",
    "# batch size: 1\n",
    "# input size: 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torch import nn \n",
    "from torch.utils.data import SequentialSampler\n",
    "\n",
    "import argparse\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, f1_score, auc, average_precision_score, confusion_matrix, classification_report\n",
    "# from sklearn.utils.fixes import signature\n",
    "from sklearn.model_selection import KFold\n",
    "torch.manual_seed(1)    # reproducible torch:2 np:3\n",
    "np.random.seed(1)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# from models_xlnet import clinical_xlnet_seq, clinical_xlnet_lstm_FAST\n",
    "# from stream_xlnet import Data_Encoder_Seq, Data_Encoder_FAST\n",
    "# from configuration_xlnet import XLNetConfig\n",
    "# from Ranger import Ranger\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_finetune(lr: float, batch_size: int, train_epoch: int, dataFolder: str, prediction_label : str):\n",
    "    '''\n",
    "    finetune the model. Here, we are finetuning the LSTM!\n",
    "\n",
    "    args:\n",
    "        dataFolder: holds train.csv, val.csv, test.csv, train_doc_emb.pt, val_doc_emb.pt, and test_doc_emb.pt\n",
    "            train.csv:\n",
    "                ['HADM_ID','Label']\n",
    "            train_doc_emb.pt:\n",
    "                (768 x 1) embedding ???\n",
    "                'HADM_ID' ???\n",
    "        prediction_label:\n",
    "    '''\n",
    "    lr = lr\n",
    "    BATCH_SIZE = batch_size\n",
    "    train_epoch = train_epoch\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    model = clinical_xlnet_lstm_FAST()\n",
    "    model.cuda()\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model, dim = 0)\n",
    "            \n",
    "   #\n",
    "    print('--- Data Preparation ---')\n",
    "    \n",
    "    params = {'batch_size': BATCH_SIZE,\n",
    "              'shuffle': True,\n",
    "              'num_workers': 0, \n",
    "              'drop_last': True}\n",
    "\n",
    "    df_train = pd.read_csv(dataFolder + '/train.csv')\n",
    "    df_val = pd.read_csv(dataFolder + '/val.csv')\n",
    "    df_test = pd.read_csv(dataFolder + '/test.csv')\n",
    "    \n",
    "    doc_train = torch.load(dataFolder + '/train_doc_emb.pt')\n",
    "    doc_val = torch.load(dataFolder + '/val_doc_emb.pt')\n",
    "    doc_test = torch.load(dataFolder + '/test_doc_emb.pt')\n",
    "    \n",
    "    def doc_emb_to_df(doc, df):\n",
    "        output_seq = [torch.unsqueeze(doc[i],0) for i in range(doc.shape[0])]\n",
    "        df = df.assign(DOC_EMB = output_seq)\n",
    "        return df\n",
    "    \n",
    "    df_train = doc_emb_to_df(doc_train, df_train)\n",
    "    df_val = doc_emb_to_df(doc_val, df_val)\n",
    "    df_test = doc_emb_to_df(doc_test, df_test)\n",
    "    \n",
    "    if prediction_label == 'PMV':\n",
    "        train_unique = df_train[['HADM_ID','Label']].drop_duplicates().reset_index(drop = True)\n",
    "        val_unique = df_val[['HADM_ID','Label']].drop_duplicates().reset_index(drop = True)\n",
    "        test_unique = df_test[['HADM_ID','Label']].drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "        training_set = Data_Encoder_FAST(train_unique.HADM_ID.values, train_unique.Label.values, df_train)\n",
    "        training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "        validation_set = Data_Encoder_FAST(val_unique.HADM_ID.values, val_unique.Label.values, df_val)\n",
    "        validation_generator = data.DataLoader(validation_set, **params)\n",
    "\n",
    "        testing_set = Data_Encoder_FAST(test_unique.HADM_ID.values, test_unique.Label.values, df_test)\n",
    "        testing_generator = data.DataLoader(testing_set, **params)\n",
    "    \n",
    "    elif prediction_label == 'Mortality':\n",
    "        train_unique = df_train[['HADM_ID','DEATH_90']].drop_duplicates().reset_index(drop = True)\n",
    "        val_unique = df_val[['HADM_ID','DEATH_90']].drop_duplicates().reset_index(drop = True)\n",
    "        test_unique = df_test[['HADM_ID','DEATH_90']].drop_duplicates().reset_index(drop = True)\n",
    "        \n",
    "        training_set = Data_Encoder_FAST(train_unique.HADM_ID.values, train_unique.DEATH_90.values, df_train)\n",
    "        training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "        validation_set = Data_Encoder_FAST(val_unique.HADM_ID.values, val_unique.DEATH_90.values, df_val)\n",
    "        validation_generator = data.DataLoader(validation_set, **params)\n",
    "    \n",
    "        testing_set = Data_Encoder_FAST(test_unique.HADM_ID.values, test_unique.DEATH_90.values, df_test)\n",
    "        testing_generator = data.DataLoader(testing_set, **params)\n",
    "    else:\n",
    "        print(\"Please modify the label value for your own downstream prediction task.\")\n",
    "    \n",
    "    \n",
    "    opt = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    # early stopping\n",
    "    max_auc = 0\n",
    "    model_max = copy.deepcopy(model)\n",
    "   \n",
    "    print('--- Go for Training ---')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    for epo in range(train_epoch):\n",
    "        model.train()\n",
    "        for i, (output, label) in enumerate(training_generator):\n",
    "            score = model(output.cuda())\n",
    "       \n",
    "            label = Variable(torch.from_numpy(np.array(label)).float()).cuda()\n",
    "            \n",
    "            loss_fct = torch.nn.BCELoss()\n",
    "            m = torch.nn.Sigmoid()\n",
    "            n = torch.squeeze(m(score))\n",
    "            \n",
    "            loss = loss_fct(n, label)\n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "           \n",
    "        # every epoch test\n",
    "        with torch.set_grad_enabled(False):\n",
    "            auc, auprc, logits, loss = test_finetune(validation_generator, model)\n",
    "            if auc > max_auc:\n",
    "                model_max = copy.deepcopy(model)\n",
    "                max_auc = auc\n",
    "                 \n",
    "            print('Validation at Epoch '+ str(epo + 1) + ' , AUROC: '+ str(auc) + ' , AUPRC: ' + str(auprc))\n",
    "    \n",
    "    print('--- Go for Testing ---')\n",
    "    try:\n",
    "        with torch.set_grad_enabled(False):\n",
    "            auc, auprc, logits, loss = test_finetune(testing_generator, model_max)\n",
    "            print('Testing AUROC: ' + str(auc) + ' , AUPRC: ' + str(auprc) + ' , Test loss: '+str(loss))\n",
    "    except:\n",
    "        print('testing failed')\n",
    "    return model_max, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "args.Learning_Rate_Finetune = 2e-5\n",
    "args.Batch_Size_Finetune = 128\n",
    "args.Training_Epoch_Finetune = 20\n",
    "dataFolder = 'FIXME'\n",
    "args.prediction_label = 'delta_in_2_days'\n",
    "model_max, loss_history =  main_finetune(args.Learning_Rate_Finetune, args.Batch_Size_Finetune, args.Training_Epoch_Finetune, dataFolder, args.prediction_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clinical1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
