{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torch import nn \n",
    "from torch.utils.data import SequentialSampler\n",
    "\n",
    "import argparse\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, f1_score, auc, average_precision_score, confusion_matrix, classification_report\n",
    "from sklearn.utils.fixes import signature\n",
    "from sklearn.model_selection import KFold\n",
    "torch.manual_seed(1)    # reproducible torch:2 np:3\n",
    "np.random.seed(1)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# from models_xlnet import clinical_xlnet_seq, clinical_xlnet_lstm_FAST\n",
    "# from stream_xlnet import Data_Encoder_Seq, Data_Encoder_FAST\n",
    "# from configuration_xlnet import XLNetConfig\n",
    "# from Ranger import Ranger\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_finetune(lr, batch_size, train_epoch, dataFolder, prediction_label):\n",
    "    '''\n",
    "    finetune the model. Here, we are finetuning the LSTM!\n",
    "    '''\n",
    "    lr = lr\n",
    "    BATCH_SIZE = batch_size\n",
    "    train_epoch = train_epoch\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    model = clinical_xlnet_lstm_FAST()\n",
    "    model.cuda()\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model, dim = 0)\n",
    "            \n",
    "   #\n",
    "    print('--- Data Preparation ---')\n",
    "    \n",
    "    params = {'batch_size': BATCH_SIZE,\n",
    "              'shuffle': True,\n",
    "              'num_workers': 0, \n",
    "              'drop_last': True}\n",
    "\n",
    "    df_train = pd.read_csv(dataFolder + '/train.csv')\n",
    "    df_val = pd.read_csv(dataFolder + '/val.csv')\n",
    "    df_test = pd.read_csv(dataFolder + '/test.csv')\n",
    "    \n",
    "    doc_train = torch.load(dataFolder + '/train_doc_emb.pt')\n",
    "    doc_val = torch.load(dataFolder + '/val_doc_emb.pt')\n",
    "    doc_test = torch.load(dataFolder + '/test_doc_emb.pt')\n",
    "    \n",
    "    def doc_emb_to_df(doc, df):\n",
    "        output_seq = [torch.unsqueeze(doc[i],0) for i in range(doc.shape[0])]\n",
    "        df = df.assign(DOC_EMB = output_seq)\n",
    "        return df\n",
    "    \n",
    "    df_train = doc_emb_to_df(doc_train, df_train)\n",
    "    df_val = doc_emb_to_df(doc_val, df_val)\n",
    "    df_test = doc_emb_to_df(doc_test, df_test)\n",
    "    \n",
    "    if prediction_label == 'PMV':\n",
    "        train_unique = df_train[['HADM_ID','Label']].drop_duplicates().reset_index(drop = True)\n",
    "        val_unique = df_val[['HADM_ID','Label']].drop_duplicates().reset_index(drop = True)\n",
    "        test_unique = df_test[['HADM_ID','Label']].drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "        training_set = Data_Encoder_FAST(train_unique.HADM_ID.values, train_unique.Label.values, df_train)\n",
    "        training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "        validation_set = Data_Encoder_FAST(val_unique.HADM_ID.values, val_unique.Label.values, df_val)\n",
    "        validation_generator = data.DataLoader(validation_set, **params)\n",
    "\n",
    "        testing_set = Data_Encoder_FAST(test_unique.HADM_ID.values, test_unique.Label.values, df_test)\n",
    "        testing_generator = data.DataLoader(testing_set, **params)\n",
    "    \n",
    "    elif prediction_label == 'Mortality':\n",
    "        train_unique = df_train[['HADM_ID','DEATH_90']].drop_duplicates().reset_index(drop = True)\n",
    "        val_unique = df_val[['HADM_ID','DEATH_90']].drop_duplicates().reset_index(drop = True)\n",
    "        test_unique = df_test[['HADM_ID','DEATH_90']].drop_duplicates().reset_index(drop = True)\n",
    "        \n",
    "        training_set = Data_Encoder_FAST(train_unique.HADM_ID.values, train_unique.DEATH_90.values, df_train)\n",
    "        training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "        validation_set = Data_Encoder_FAST(val_unique.HADM_ID.values, val_unique.DEATH_90.values, df_val)\n",
    "        validation_generator = data.DataLoader(validation_set, **params)\n",
    "    \n",
    "        testing_set = Data_Encoder_FAST(test_unique.HADM_ID.values, test_unique.DEATH_90.values, df_test)\n",
    "        testing_generator = data.DataLoader(testing_set, **params)\n",
    "    else:\n",
    "        print(\"Please modify the label value for your own downstream prediction task.\")\n",
    "    \n",
    "    \n",
    "    opt = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    # early stopping\n",
    "    max_auc = 0\n",
    "    model_max = copy.deepcopy(model)\n",
    "   \n",
    "    print('--- Go for Training ---')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    for epo in range(train_epoch):\n",
    "        model.train()\n",
    "        for i, (output, label) in enumerate(training_generator):\n",
    "            score = model(output.cuda())\n",
    "       \n",
    "            label = Variable(torch.from_numpy(np.array(label)).float()).cuda()\n",
    "            \n",
    "            loss_fct = torch.nn.BCELoss()\n",
    "            m = torch.nn.Sigmoid()\n",
    "            n = torch.squeeze(m(score))\n",
    "            \n",
    "            loss = loss_fct(n, label)\n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "           \n",
    "        # every epoch test\n",
    "        with torch.set_grad_enabled(False):\n",
    "            auc, auprc, logits, loss = test_finetune(validation_generator, model)\n",
    "            if auc > max_auc:\n",
    "                model_max = copy.deepcopy(model)\n",
    "                max_auc = auc\n",
    "                 \n",
    "            print('Validation at Epoch '+ str(epo + 1) + ' , AUROC: '+ str(auc) + ' , AUPRC: ' + str(auprc))\n",
    "    \n",
    "    print('--- Go for Testing ---')\n",
    "    try:\n",
    "        with torch.set_grad_enabled(False):\n",
    "            auc, auprc, logits, loss = test_finetune(testing_generator, model_max)\n",
    "            print('Testing AUROC: ' + str(auc) + ' , AUPRC: ' + str(auprc) + ' , Test loss: '+str(loss))\n",
    "    except:\n",
    "        print('testing failed')\n",
    "    return model_max, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "args.Learning_Rate_Finetune = 2e-5\n",
    "args.Batch_Size_Finetune = 128\n",
    "args.Training_Epoch_Finetune = 20\n",
    "dataFolder = 'FIXME'\n",
    "args.prediction_label = 'delta_in_2_days'\n",
    "model_max, loss_history =  main_finetune(args.Learning_Rate_Finetune, args.Batch_Size_Finetune, args.Training_Epoch_Finetune, dataFolder, args.prediction_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clinical1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
