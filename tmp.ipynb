{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test mode: True\n",
      "device: cuda\n",
      "no ckpt specified. Finetuning from base\n",
      "model_out:/home/ugrads/a/aa_ron_su/physionet.org/files/clinical-t5/1.0.0/model_test/meta_ft_classify.pt\n",
      "reading notes and target from /home/ugrads/a/aa_ron_su/JSS_SUBMISSION_NEW/data/till_end_mimic_iv_extra_features_train_NOTE_TARGET1_FT_rad.csv\n",
      "tokenized note input_ids loaded from /home/ugrads/a/aa_ron_su/physionet.org/files/clinical-t5/1.0.0/tokenized_notes_rad/train_tensor.pt\n",
      "CLASSIFIER LOADED TO GPU: 841.216796875 Megabytes\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "testing = True\n",
    "# freeze_encoder = args.freeze_encoder\n",
    "GPU_NO = int('1')\n",
    "model_ckpt_path = None\n",
    "\n",
    "print(f'Test mode: {testing}')\n",
    "# print(f'Freeze encoder: {freeze_encoder}')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPU_NO)  # use the correct gpu\n",
    "device = torch.device(f'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device: {device}\")\n",
    "if model_ckpt_path:\n",
    "    print(f\"model checkpoint to use: {model_ckpt_path}\")\n",
    "else:\n",
    "    print(\"no ckpt specified. Finetuning from base\")\n",
    "# print(f\"memory reserved: {torch.cuda.memory_reserved(device=device)} bytes\")\n",
    "\n",
    "root = '/home/ugrads/a/aa_ron_su/physionet.org/files/clinical-t5/1.0.0/'\n",
    "data_path = '/data/datasets/mimiciv_notes/physionet.org/files/mimic-iv-note/2.2/note/radiology.csv'\n",
    "model_path = root + 'Clinical-T5-Base/'\n",
    "temivef_train_NOTE_TARGET1_FT_path = '/home/ugrads/a/aa_ron_su/JSS_SUBMISSION_NEW/data/till_end_mimic_iv_extra_features_train_NOTE_TARGET1_FT_rad.csv'\n",
    "\n",
    "out_dir = f\"{os.environ.get('CLINICAL_DIR')}model{'_test' if testing else ''}{'_from_ckpt1' if model_ckpt_path else ''}\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "model_out = os.path.join(out_dir,\"meta_ft_classify.pt\")\n",
    "print(f\"model_out:{model_out}\")\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "model_name = \"Clinical-T5-Base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Clinical-T5-Base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Clinical-T5-Base\")\n",
    "\n",
    "train = pd.read_csv(temivef_train_NOTE_TARGET1_FT_path)\n",
    "print(f\"reading notes and target from {temivef_train_NOTE_TARGET1_FT_path}\")\n",
    "\n",
    "train_tensor_filename = f\"{os.environ.get('CLINICAL_DIR')}tokenized_notes_rad/train_tensor.pt\"\n",
    "train_tensors = None\n",
    "if os.path.isfile(train_tensor_filename):\n",
    "    train_tensors = torch.load(train_tensor_filename)\n",
    "    print(f\"tokenized note input_ids loaded from {train_tensor_filename}\")\n",
    "# else:\n",
    "#     train_texts = train['text'].tolist()\n",
    "#     tokenized_train_notes = tokenizer(train_texts, truncation=True, padding=True, return_tensors = \"pt\")\n",
    "#     train_tensor = tokenized_train_notes.input_ids\n",
    "#     torch.save(train_tensor, train_tensor_filename)\n",
    "#     print(f\"train notes tokenized and saved to {train_tensor_filename}\")\n",
    "# test_texts = test['text'].tolist()\n",
    "# tokenized_test_notes = tokenizer(test_texts, truncation=True, padding=True, return_tensors = \"pt\")\n",
    "# print(\"test notes tokenized\")\n",
    "\n",
    "\n",
    "from T5EncoderForSequenceClassification import T5EncoderForSequenceClassification\n",
    "\n",
    "from transformers import T5Config\n",
    "\n",
    "encoder = model.get_encoder() # we only need the clinical-t5 encoder for our purposes\n",
    "\n",
    "config = T5Config(\n",
    "    hidden_size=768,\n",
    "    classifier_dropout=None,\n",
    "    num_labels=2,\n",
    "    hidden_dropout_prob=0.01,\n",
    "    last_hidden_size=64\n",
    ")\n",
    "\n",
    "classifier = (\n",
    "    T5EncoderForSequenceClassification(encoder, config).to(device) \n",
    "    if model_ckpt_path == None \n",
    "    else torch.load(model_ckpt_path)\n",
    ")\n",
    "\n",
    "print(f\"CLASSIFIER LOADED TO GPU: {torch.cuda.memory_allocated() / 2**20} Megabytes\")\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "\n",
    "# Define training data\n",
    "train_inputs = train_tensors.input_ids.to(device)\n",
    "train_labels = torch.tensor(train['delta_in_2_days'].to_numpy()).to(device)\n",
    "train_dataset = TensorDataset(train_inputs, train_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  262,     4,  4815,  ...,     0,     0,     0],\n",
       "        [ 3388, 27447,  8015,  ...,     0,     0,     0],\n",
       "        [    3,  7094, 22633,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    3, 12016,  5668,  ...,     0,     0,     0],\n",
       "        [  262,     4,   188,  ...,     0,     0,     0],\n",
       "        [ 3388, 27447,  8015,  ...,     0,     0,     0]], device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  262,     4,   188, 17684,  8015,    10,  9302,  6038,    41, 14536,\n",
       "         17098,     3,  2965,    61,  3388, 27447,  8015,    10,     3,   834,\n",
       "           834,   834,    28, 19222, 13751, 12498,   433,    58, 23666, 22410,\n",
       "         10466,    10,  4004,   222,  2252,  9413,     3,   834,   834,   834,\n",
       "           377, 13885,  2365,   134,    10,  7871,   851,   138,   903,    13,\n",
       "             8,  5738,   937,     5,   290,    19,   150, 15949, 16690,     6,\n",
       "         13577, 11733,     6,    42, 18919,  8888, 21783,   226,     5,    37,\n",
       "         16216,  8172,     7,    17, 10270, 19561,    19,  1389,     5,  7204,\n",
       "           894,    33,  1317, 16234,   516,    53,   147,     8,   646,  6748,\n",
       "            11,  4322,   646,    18, 17389,     3,  6520, 16898,     7,     5,\n",
       "           465,   339,   799,   666,     8,   269,     3,    88, 16091,     9,\n",
       "         28698,   122,    51,    19,   894,     5,     3,  5166, 22120,   134,\n",
       "          9215,    10,   465, 12498, 16076, 21783, 13241,   433,     5,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading notes and target from /home/ugrads/a/aa_ron_su/JSS_SUBMISSION_NEW/data/till_end_mimic_iv_extra_features_test_NOTE_TARGET1_FT_rad.csv\n",
      "reading notes and target from /home/ugrads/a/aa_ron_su/JSS_SUBMISSION_NEW/data/till_end_mimic_iv_extra_features_train_NOTE_TARGET1_FT_rad.csv\n",
      "CLINICAL_DIR: /home/ugrads/a/aa_ron_su/physionet.org/files/clinical-t5/1.0.0/\n",
      "train notes tokenized and saved to /home/ugrads/a/aa_ron_su/physionet.org/files/clinical-t5/1.0.0/tokenized_notes_rad/train_tensor.pt\n",
      "test notes tokenized and saved to /home/ugrads/a/aa_ron_su/physionet.org/files/clinical-t5/1.0.0/tokenized_notes_rad/test_tensor.pt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "temivef_train_NOTE_TARGET1_FT_path = '/home/ugrads/a/aa_ron_su/JSS_SUBMISSION_NEW/data/till_end_mimic_iv_extra_features_train_NOTE_TARGET1_FT_rad.csv'\n",
    "temivef_test_NOTE_TARGET1_FT_path = '/home/ugrads/a/aa_ron_su/JSS_SUBMISSION_NEW/data/till_end_mimic_iv_extra_features_test_NOTE_TARGET1_FT_rad.csv'\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "model_name = \"Clinical-T5-Base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Clinical-T5-Base\")\n",
    "\n",
    "test = pd.read_csv(temivef_test_NOTE_TARGET1_FT_path)\n",
    "print(f\"reading notes and target from {temivef_test_NOTE_TARGET1_FT_path}\")\n",
    "\n",
    "train = pd.read_csv(temivef_train_NOTE_TARGET1_FT_path)\n",
    "print(f\"reading notes and target from {temivef_train_NOTE_TARGET1_FT_path}\")\n",
    "\n",
    "print('CLINICAL_DIR:', os.environ.get('CLINICAL_DIR'))\n",
    "tensor_dir = os.path.join(os.environ.get('CLINICAL_DIR'), \"tokenized_notes_rad\")\n",
    "train_tensor_path = os.path.join(tensor_dir, \"train_tensor.pt\")\n",
    "test_tensor_path = os.path.join(tensor_dir, \"test_tensor.pt\")\n",
    "\n",
    "if not os.path.exists(tensor_dir):\n",
    "    os.makedirs(tensor_dir)\n",
    "\n",
    "\n",
    "train_texts = train['text'].tolist()\n",
    "tokenized_train_notes = tokenizer(train_texts, truncation=True, padding=True, return_tensors = \"pt\")\n",
    "train_tensors = tokenized_train_notes\n",
    "torch.save(train_tensors, train_tensor_path)\n",
    "print(f\"train notes tokenized and saved to {train_tensor_path}\")\n",
    "\n",
    "\n",
    "test_texts = test['text'].tolist()\n",
    "tokenized_test_notes = tokenizer(test_texts, truncation=True, padding=True, return_tensors = \"pt\")\n",
    "test_tensor = tokenized_test_notes\n",
    "torch.save(test_tensor, test_tensor_path)\n",
    "print(f\"test notes tokenized and saved to {test_tensor_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  262,     4,   188, 17684,  8015,    10,  9302,  6038,    41, 14536,\n",
       "         17098,     3,  2965,    61,  3388, 27447,  8015,    10,     3,   834,\n",
       "           834,   834,    28, 19222, 13751, 12498,   433,    58, 23666, 22410,\n",
       "         10466,    10,  4004,   222,  2252,  9413,     3,   834,   834,   834,\n",
       "           377, 13885,  2365,   134,    10,  7871,   851,   138,   903,    13,\n",
       "             8,  5738,   937,     5,   290,    19,   150, 15949, 16690,     6,\n",
       "         13577, 11733,     6,    42, 18919,  8888, 21783,   226,     5,    37,\n",
       "         16216,  8172,     7,    17, 10270, 19561,    19,  1389,     5,  7204,\n",
       "           894,    33,  1317, 16234,   516,    53,   147,     8,   646,  6748,\n",
       "            11,  4322,   646,    18, 17389,     3,  6520, 16898,     7,     5,\n",
       "           465,   339,   799,   666,     8,   269,     3,    88, 16091,     9,\n",
       "         28698,   122,    51,    19,   894,     5,     3,  5166, 22120,   134,\n",
       "          9215,    10,   465, 12498, 16076, 21783, 13241,   433,     5,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test mode: True\n",
      "device: cuda\n",
      "dataloaders generated\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "testing = True\n",
    "GPU_NO = 4\n",
    "print(f'Test mode: {testing}')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPU_NO)  # use the correct gpu\n",
    "device = torch.device(f'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "\n",
    "# import torch.nn as nn\n",
    "# import sys\n",
    "\n",
    "root = '/home/ugrads/a/aa_ron_su/physionet.org/files/clinical-t5/1.0.0'\n",
    "finetuned_model_path = root + '/model_from_ckpt1/meta_ft_classify.pt' # modify this line!\n",
    "temivef_train_NOTE_TARGET1_FT_path = '/home/ugrads/a/aa_ron_su/JSS_SUBMISSION_NEW/data/till_end_mimic_iv_extra_features_train_NOTE_TARGET1_FT.csv'\n",
    "temivef_test_NOTE_TARGET1_FT_path = '/home/ugrads/a/aa_ron_su/JSS_SUBMISSION_NEW/data/till_end_mimic_iv_extra_features_test_NOTE_TARGET1_FT.csv'\n",
    "temivef_train_NOTE_path = '/home/ugrads/a/aa_ron_su/JSS_SUBMISSION_NEW/data/till_end_mimic_iv_extra_features_train_NOTE.csv'\n",
    "temivef_test_NOTE_path = '/home/ugrads/a/aa_ron_su/JSS_SUBMISSION_NEW/data/till_end_mimic_iv_extra_features_test_NOTE.csv'\n",
    "outdir = f'/home/ugrads/a/aa_ron_su/JSS_SUBMISSION_NEW/data/final_rad1_{\"test\" if testing else \"\"}/' # modify this line!\n",
    "train_outpath = os.path.join(outdir, 'till_end_mimic_iv_extra_features_train.csv')\n",
    "test_outpath = os.path.join(outdir, 'till_end_mimic_iv_extra_features_test.csv') \n",
    "from transformers import AutoTokenizer\n",
    "model_name = \"Clinical-T5-Base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Clinical-T5-Base\")\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "tensor_dir = f\"{os.environ.get('CLINICAL_DIR')}tokenized_notes_rad\"\n",
    "train_tokenizer_outputs_path = os.path.join(tensor_dir, \"train_tensor.pt\")\n",
    "test_tokenizer_outputs_path = os.path.join(tensor_dir, \"test_tensor.pt\")\n",
    "\n",
    "# if not os.path.exists(tensor_dir):\n",
    "    # notes_to_extract = temivef_train_NOTE_TARGET1_FT_path['text']\n",
    "    # texts = notes_to_extract.tolist()\n",
    "    # tokenized_notes = tokenizer(texts, truncation=True, padding=True, return_tensors = \"pt\")\n",
    "    # torch.save(tokenized_notes, train_tokenizer_outputs_path)\n",
    "\n",
    "    # notes_to_extract = temivef_test_NOTE_TARGET1_FT_path['text']\n",
    "    # texts = notes_to_extract.tolist()\n",
    "    # tokenized_notes = tokenizer(texts, truncation=True, padding=True, return_tensors = \"pt\")\n",
    "    # torch.save(tokenized_notes, test_tokenizer_outputs_path)\n",
    "\n",
    "tokenized_train_notes = torch.load(train_tokenizer_outputs_path)\n",
    "tokenized_test_notes = torch.load(test_tokenizer_outputs_path)\n",
    "\n",
    "# if testing:\n",
    "#     tokenized_train_notes = tokenized_train_notes[:12]\n",
    "#     tokenized_test_notes = tokenized_test_notes[:12]\n",
    "#     print(f\"testing mode truncated tokenized_notes to length {len(tokenized_train_notes)}\")\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from time import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def generate_dataloader(tokenized_notes, batch_size, device):\n",
    "    labels = torch.tensor([-1] * len(tokenized_notes.input_ids)) # not actually used in this case, since we are not evaluating loss\n",
    "    dataset = TensorDataset(tokenized_notes.input_ids.to(device), tokenized_notes.attention_mask.to(device), labels.to(device))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=False) \n",
    "    # shuffle is false so that notes retain their order for concat with df\n",
    "    return dataloader\n",
    "\n",
    "batch_size = 6\n",
    "train_dataloader = generate_dataloader(tokenized_train_notes, batch_size, device)\n",
    "test_dataloader = generate_dataloader(tokenized_test_notes, batch_size, device)\n",
    "print(\"dataloaders generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmp.txt', 'w') as f:\n",
    "\n",
    "    # Iterate over the batches in the dataloader\n",
    "    for batch in train_dataloader:\n",
    "        input_ids_batch, attention_mask_batch, labels_batch = batch\n",
    "\n",
    "        # Write the contents of the batch to the file\n",
    "        f.write(\"Input IDs batch: {}\\n\".format(input_ids_batch))\n",
    "        f.write(\"Attention mask batch: {}\\n\".format(attention_mask_batch))\n",
    "        f.write(\"Labels batch: {}\\n\".format(labels_batch))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded classifier from /home/ugrads/a/aa_ron_su/physionet.org/files/clinical-t5/1.0.0/model_from_ckpt1/meta_ft_classify.pt\n",
      "finished train embedding extraction\n",
      "finished test embedding extraction\n"
     ]
    }
   ],
   "source": [
    "classifier = torch.load(finetuned_model_path)\n",
    "classifier.encoder.eval() # makes sure dropout does not occur\n",
    "classifier.classifier.eval()\n",
    "print(f\"loaded classifier from {finetuned_model_path}\")\n",
    "\n",
    "def extract_embeddings(dataloader, classifier):\n",
    "    with(torch.no_grad()):\n",
    "        start_time = time()\n",
    "        embeddings = []\n",
    "        for step, batch in enumerate(dataloader):\n",
    "                input_ids, attention_mask, _ = batch\n",
    "                emb = classifier.forward(input_ids, attention_mask=attention_mask, return_embeddings=True)\n",
    "                # embeddings.append(emb)\n",
    "                # print(f\"Step {step}/{len(dataloader)} | Time {time() - start_time : .2f} seconds\")\n",
    "\n",
    "    # embeddings = torch.cat(embeddings, dim=0)\n",
    "    # embeddings = embeddings.cpu()\n",
    "    # embeddings_df = pd.DataFrame(embeddings.detach().numpy()).add_prefix('emb')\n",
    "    # print(f\"Extracted {len(embeddings_df)} note embeddings. Shape: {embeddings_df.shape}\") # should be size 64\n",
    "    # return embeddings_df\n",
    "\n",
    "train_embeddings_df = extract_embeddings(train_dataloader, classifier)\n",
    "print(\"finished train embedding extraction\") \n",
    "test_embeddings_df = extract_embeddings(test_dataloader, classifier)\n",
    "print(\"finished test embedding extraction\")\n",
    "\n",
    "\n",
    "# for mode in [\"train\", \"test\"]:\n",
    "#     print(f\"reading from temivef_{mode}_NOTE_TARGET1_FT_path\")\n",
    "# train = pd.read_csv(temivef_train_NOTE_TARGET1_FT_path)\n",
    "# test = pd.read_csv(temivef_test_NOTE_TARGET1_FT_path)\n",
    "# print(f\"loaded train notes to extract from {temivef_train_NOTE_TARGET1_FT_path}\")\n",
    "# print(f\"loaded test notes to extract from {temivef_test_NOTE_TARGET1_FT_path}\")\n",
    "\n",
    "# #concat notes with ICUSTAY column for merging later\n",
    "# train_df_small = pd.concat([train[['ICUSTAY_ID', 'NOTE_ID']], train_embeddings_df], axis = 1)\n",
    "# test_df_small = pd.concat([test[['ICUSTAY_ID', 'NOTE_ID']], test_embeddings_df], axis = 1)\n",
    "\n",
    "# print(f\"concatenating train and train_embeddings_df with shape {train.shape} and {train_embeddings_df.shape} respectively\")\n",
    "# print(f\"concatenating test and test_embeddings_df with shape {test.shape} and {test_embeddings_df.shape} respectively\")\n",
    "\n",
    "\n",
    "# for mode in [\"train\", \"test\"]:\n",
    "#     print(f\"reading from temivef_{mode}_NOTE_path\")\n",
    "# train_df_big = pd.read_csv(temivef_train_NOTE_path)\n",
    "# test_df_big = pd.read_csv(temivef_test_NOTE_path)\n",
    "# print(f\"loaded train df to merge with from {temivef_train_NOTE_path}\")\n",
    "# print(f\"loaded test notes to merge with from {temivef_test_NOTE_path}\")\n",
    "\n",
    "\n",
    "# def merge_and_fill_embeddings(df_small, df_big):\n",
    "#     print(f\"BEFORE merge: len = {len(df_big)}\")\n",
    "#     out_df = df_big.merge(df_small, on = ['ICUSTAY_ID','NOTE_ID'], how = 'left')\n",
    "#     print(f\"AFTER merge: len = {len(out_df)}\")\n",
    "#     def fill_embedding_na(note_id_group):\n",
    "#         note_id_group = note_id_group.fillna(method='ffill').fillna(method='bfill')\n",
    "#         return note_id_group\n",
    "\n",
    "#     emb_cols = ['emb' + str(i) for i in range(64)]\n",
    "\n",
    "#     print(f\"BEFORE fill na: len = {len(out_df)}\")\n",
    "#     out_df[emb_cols] = out_df.groupby('NOTE_ID')[emb_cols].transform(fill_embedding_na) # transform preserves the shape of the original\n",
    "#     print(f\"AFTER fill na: len = {len(out_df)}\")\n",
    "\n",
    "\n",
    "#     print(f\"copied {len(df_small)} embeddings into rows with the correct NOTE_ID\")\n",
    "#     print(f\"len(out_df): {len(out_df)}\")\n",
    "#     print(f\"No. nonnull embedding rows in out_df: {len(out_df[pd.notna(out_df['emb0'])])}\")\n",
    "#     return out_df\n",
    "\n",
    "# train_out_df = merge_and_fill_embeddings(train_df_small, train_df_big)\n",
    "# print(\"merged and filled embeddings for train_out_df\")\n",
    "# test_out_df = merge_and_fill_embeddings(test_df_small, test_df_big)\n",
    "# print(\"merged and filled embeddings for test_out_df\")\n",
    "\n",
    "\n",
    "# # format like mimic_iv_train\n",
    "# def format_cols(df):\n",
    "#     df.drop(['text', 'NOTE_ID', 't_start_DT','INTIME'], axis=1, inplace=True)\n",
    "#     df.rename(columns = {\n",
    "#         'SUBJECT_ID':'subject',\n",
    "#         'ICUSTAY_ID':'Icustay'\n",
    "#         }, inplace = True)\n",
    "#     return df\n",
    "\n",
    "# train_out_df = format_cols(train_out_df)\n",
    "# test_out_df = format_cols(test_out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 3388, 27447,  8015,  ...,     0,     0,     0],\n",
       "        [ 8668,     3,  5359,  ...,    12,    36,     1],\n",
       "        [ 9302,  6038,     3,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [ 9302,  6038,   391,  ...,     0,     0,     0],\n",
       "        [ 9302,  6038,   391,  ...,     0,     0,     0],\n",
       "        [  205, 20931, 23936,  ...,    20,  2032,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  262,     4,  4815,  ...,     0,     0,     0],\n",
       "        [ 3388, 27447,  8015,  ...,     0,     0,     0],\n",
       "        [    3,  7094, 22633,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    3, 12016,  5668,  ...,     0,     0,     0],\n",
       "        [  262,     4,   188,  ...,     0,     0,     0],\n",
       "        [ 3388, 27447,  8015,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_notes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clinical_t5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
