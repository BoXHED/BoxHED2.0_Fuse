{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import LongformerTokenizerFast, LongformerForSequenceClassification, Trainer, TrainingArguments, LongformerConfig, T5Config\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import wandb\n",
    "import argparse\n",
    "from functools import partial\n",
    "\n",
    "from helpers import find_next_dir_index , explode_train_target, convert_to_list, merge_embs_to_seq, compute_metrics_LSTM, group_train_val\n",
    "from MyTrainer import MyTrainer \n",
    "from BoXHED_Fuse.models.ClinicalLSTM import ClinicalLSTM\n",
    "# from ..models.ClinicalLSTM import ClinicalLSTM\n",
    "\n",
    "\n",
    "\n",
    "# ===== Initialize Args ===== \n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--test', action='store_true', help='enable testing mode')\n",
    "# parser.add_argument('--use-wandb', action = 'store_true', help = 'enable wandb', default=False)\n",
    "# parser.add_argument('--gpu-no', dest = 'GPU_NO', help='use GPU_NO specified (this may be a single number or several. eg: 1 or 1,2,3,4)')\n",
    "# parser.add_argument('--note-type', dest = 'note_type', help='which notes, radiology or discharge?')\n",
    "# parser.add_argument('--num-epochs', dest = 'num_epochs', help = 'num_epochs to train')\n",
    "# parser.add_argument('--noteid-mode', dest = 'noteid_mode', help = 'kw: all or recent')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "args = argparse.Namespace\n",
    "args.test = True\n",
    "args.GPU_NO = -1\n",
    "args.note_type = 'radiology'\n",
    "args.num_epochs = 1\n",
    "args.noteid_mode = 'all'\n",
    "args.model_name = 'LSTM'\n",
    "\n",
    "args.num_epochs = int(args.num_epochs)\n",
    "\n",
    "model_name_ft1 = 'Clinical-T5-Base'\n",
    "train_embs_path = f'{os.getenv(\"BHF_ROOT\")}/JSS_SUBMISSION_NEW/data/embs{\"/testing\" if args.test else \"\"}/{model_name_ft1}_{args.note_type[:3]}_{args.noteid_mode}_out/from_epoch1/10/train_embs.pt'\n",
    "train_target_path = f'{os.getenv(\"BHF_ROOT\")}/JSS_SUBMISSION_NEW/data/targets{\"/testing\" if args.test else \"\"}/till_end_mimic_iv_extra_features_train_NOTE_TARGET_2_{args.note_type[:3]}_{args.noteid_mode}.csv'\n",
    "model_name = 'clinical_lstm'\n",
    "model_out_dir = f'{os.getenv(\"BHF_ROOT\")}/model_outputs/{model_name}_{args.note_type[:3]}_{args.noteid_mode}_out'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_train_embseq(train_embseq, train_target):\n",
    "    train_embseq = train_embseq.copy()\n",
    "    train_embseq['emb_seq_len'] = train_embseq['emb_seq'].apply(len)\n",
    "    train_target['NOTE_ID_SEQ_len'] = train_target['NOTE_ID_SEQ'].apply(len)\n",
    "    assert((train_target['NOTE_ID_SEQ_len'] == train_embseq['emb_seq_len']).all())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created all dirs in model_out_dir /home/ugrads/a/aa_ron_su/BoXHED_Fuse/BoXHED_Fuse//model_outputs/clinical_lstm_rad_all_out/21\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not os.path.exists(model_out_dir):\n",
    "    os.makedirs(model_out_dir)\n",
    "run_cntr = find_next_dir_index(model_out_dir)\n",
    "model_out_dir = os.path.join(model_out_dir, str(run_cntr))\n",
    "assert(not os.path.exists(model_out_dir))\n",
    "# os.makedirs(model_out_dir) # FIXME\n",
    "print(f'created all dirs in model_out_dir', model_out_dir)\n",
    "\n",
    "if args.test:\n",
    "    # train_target_path = os.path.join(os.path.dirname(train_target_path), 'testing', os.path.basename(train_target_path))\n",
    "    model_out_dir = os.path.join(os.path.dirname(model_out_dir), 'testing', os.path.basename(model_out_dir))\n",
    "\n",
    "\n",
    "# ===== Read Data =====\n",
    "train_embs = torch.load(train_embs_path)\n",
    "train_target = pd.read_csv(train_target_path, converters = {'NOTE_ID_SEQ': convert_to_list})\n",
    "target = 'delta_in_2_days' # FIXME\n",
    "train_target.rename(columns = {target:'label'}, inplace=True)\n",
    "# ===== Merge data into {note_embs_seq, label}, where note_embs_seq is a list of embs =====\n",
    "train_target_exploded = explode_train_target(train_target)\n",
    "train_embs_df = pd.DataFrame()\n",
    "train_embs_df['emb'] = [np.array(e) for e in train_embs]\n",
    "train_embs_df = pd.concat([train_target_exploded, train_embs_df], axis=1)\n",
    "train_embseq = merge_embs_to_seq(train_target, train_embs_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2, 64])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ICUSTAY_ID</th>\n",
       "      <th>NOTE_ID</th>\n",
       "      <th>label</th>\n",
       "      <th>NOTE_ID_SEQ</th>\n",
       "      <th>emb_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30676350.0</td>\n",
       "      <td>10003019-RR-66</td>\n",
       "      <td>0</td>\n",
       "      <td>[10003019-RR-16, 10003019-RR-17, 10003019-RR-1...</td>\n",
       "      <td>[[0.04571784197735888, 0.02954959064224078, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31090461.0</td>\n",
       "      <td>10002155-RR-52</td>\n",
       "      <td>0</td>\n",
       "      <td>[10002155-RR-22, 10002155-RR-23, 10002155-RR-2...</td>\n",
       "      <td>[[0.040107877819736744, 0.07095444332914923, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31205490.0</td>\n",
       "      <td>10001725-RR-12</td>\n",
       "      <td>0</td>\n",
       "      <td>[10001725-RR-10, 10001725-RR-11, 10001725-RR-12]</td>\n",
       "      <td>[[0.007828903479476993, 0.08233785253433663, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32128372.0</td>\n",
       "      <td>10003400-RR-66</td>\n",
       "      <td>0</td>\n",
       "      <td>[10003400-RR-49, 10003400-RR-50, 10003400-RR-5...</td>\n",
       "      <td>[[0.022228072317655084, 0.09650489123149635, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32128372.0</td>\n",
       "      <td>10003400-RR-67</td>\n",
       "      <td>0</td>\n",
       "      <td>[10003400-RR-49, 10003400-RR-50, 10003400-RR-5...</td>\n",
       "      <td>[[0.022228072317655084, 0.09650489123149635, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32128372.0</td>\n",
       "      <td>10003400-RR-68</td>\n",
       "      <td>0</td>\n",
       "      <td>[10003400-RR-49, 10003400-RR-50, 10003400-RR-5...</td>\n",
       "      <td>[[0.022228072317655084, 0.09650489123149635, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>33685454.0</td>\n",
       "      <td>10002155-RR-29</td>\n",
       "      <td>0</td>\n",
       "      <td>[10002155-RR-22, 10002155-RR-23, 10002155-RR-2...</td>\n",
       "      <td>[[0.040107877819736744, 0.07095444332914923, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>33685454.0</td>\n",
       "      <td>10002155-RR-31</td>\n",
       "      <td>0</td>\n",
       "      <td>[10002155-RR-22, 10002155-RR-23, 10002155-RR-2...</td>\n",
       "      <td>[[0.040107877819736744, 0.07095444332914923, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>33685454.0</td>\n",
       "      <td>10002155-RR-32</td>\n",
       "      <td>0</td>\n",
       "      <td>[10002155-RR-22, 10002155-RR-23, 10002155-RR-2...</td>\n",
       "      <td>[[0.040107877819736744, 0.07095444332914923, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>33685454.0</td>\n",
       "      <td>10002155-RR-33</td>\n",
       "      <td>0</td>\n",
       "      <td>[10002155-RR-22, 10002155-RR-23, 10002155-RR-2...</td>\n",
       "      <td>[[0.040107877819736744, 0.07095444332914923, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>33685454.0</td>\n",
       "      <td>10002155-RR-34</td>\n",
       "      <td>0</td>\n",
       "      <td>[10002155-RR-22, 10002155-RR-23, 10002155-RR-2...</td>\n",
       "      <td>[[0.040107877819736744, 0.07095444332914923, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>34592300.0</td>\n",
       "      <td>10001217-RR-20</td>\n",
       "      <td>0</td>\n",
       "      <td>[10001217-RR-9, 10001217-RR-10, 10001217-RR-11...</td>\n",
       "      <td>[[0.007121110218495659, -0.009922850304841089,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>34807493.0</td>\n",
       "      <td>10002428-RR-89</td>\n",
       "      <td>0</td>\n",
       "      <td>[10002428-RR-48, 10002428-RR-50, 10002428-RR-5...</td>\n",
       "      <td>[[0.050403328985508686, 0.0354461913912506, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>35514836.0</td>\n",
       "      <td>10003046-RR-6</td>\n",
       "      <td>0</td>\n",
       "      <td>[10003046-RR-6]</td>\n",
       "      <td>[[0.0641411182690233, 0.06539167494293698, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>35514836.0</td>\n",
       "      <td>10003046-RR-7</td>\n",
       "      <td>0</td>\n",
       "      <td>[10003046-RR-6, 10003046-RR-7]</td>\n",
       "      <td>[[0.0641411182690233, 0.06539167494293698, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>35514836.0</td>\n",
       "      <td>10003046-RR-8</td>\n",
       "      <td>0</td>\n",
       "      <td>[10003046-RR-6, 10003046-RR-7, 10003046-RR-8]</td>\n",
       "      <td>[[0.0641411182690233, 0.06539167494293698, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>35514836.0</td>\n",
       "      <td>10003046-RR-10</td>\n",
       "      <td>0</td>\n",
       "      <td>[10003046-RR-6, 10003046-RR-7, 10003046-RR-8, ...</td>\n",
       "      <td>[[0.0641411182690233, 0.06539167494293698, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>35514836.0</td>\n",
       "      <td>10003046-RR-9</td>\n",
       "      <td>0</td>\n",
       "      <td>[10003046-RR-6, 10003046-RR-7, 10003046-RR-8, ...</td>\n",
       "      <td>[[0.0641411182690233, 0.06539167494293698, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>37049133.0</td>\n",
       "      <td>10002930-RR-19</td>\n",
       "      <td>0</td>\n",
       "      <td>[10002930-RR-19]</td>\n",
       "      <td>[[-0.019983626449689623, 0.0013677926485353698...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>37067082.0</td>\n",
       "      <td>10001217-RR-13</td>\n",
       "      <td>0</td>\n",
       "      <td>[10001217-RR-9, 10001217-RR-10, 10001217-RR-11...</td>\n",
       "      <td>[[0.007121110218495659, -0.009922850304841089,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>37067082.0</td>\n",
       "      <td>10001217-RR-14</td>\n",
       "      <td>0</td>\n",
       "      <td>[10001217-RR-9, 10001217-RR-10, 10001217-RR-11...</td>\n",
       "      <td>[[0.007121110218495659, -0.009922850304841089,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>39060235.0</td>\n",
       "      <td>10002013-RR-52</td>\n",
       "      <td>1</td>\n",
       "      <td>[10002013-RR-32, 10002013-RR-34, 10002013-RR-3...</td>\n",
       "      <td>[[-0.009020838111375785, 0.043912610179967164,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>39060235.0</td>\n",
       "      <td>10002013-RR-53</td>\n",
       "      <td>1</td>\n",
       "      <td>[10002013-RR-32, 10002013-RR-34, 10002013-RR-3...</td>\n",
       "      <td>[[-0.009020838111375785, 0.043912610179967164,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>39553978.0</td>\n",
       "      <td>10000032-RR-45</td>\n",
       "      <td>0</td>\n",
       "      <td>[10000032-RR-14, 10000032-RR-15, 10000032-RR-1...</td>\n",
       "      <td>[[-0.02616058236401081, 0.05037255446348286, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>39765666.0</td>\n",
       "      <td>10000980-RR-62</td>\n",
       "      <td>0</td>\n",
       "      <td>[10000980-RR-52, 10000980-RR-55, 10000980-RR-5...</td>\n",
       "      <td>[[0.04948653200219413, 0.026474391738920366, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ICUSTAY_ID         NOTE_ID  label  \\\n",
       "0   30676350.0  10003019-RR-66      0   \n",
       "1   31090461.0  10002155-RR-52      0   \n",
       "2   31205490.0  10001725-RR-12      0   \n",
       "3   32128372.0  10003400-RR-66      0   \n",
       "4   32128372.0  10003400-RR-67      0   \n",
       "5   32128372.0  10003400-RR-68      0   \n",
       "6   33685454.0  10002155-RR-29      0   \n",
       "7   33685454.0  10002155-RR-31      0   \n",
       "8   33685454.0  10002155-RR-32      0   \n",
       "9   33685454.0  10002155-RR-33      0   \n",
       "10  33685454.0  10002155-RR-34      0   \n",
       "11  34592300.0  10001217-RR-20      0   \n",
       "12  34807493.0  10002428-RR-89      0   \n",
       "13  35514836.0   10003046-RR-6      0   \n",
       "14  35514836.0   10003046-RR-7      0   \n",
       "15  35514836.0   10003046-RR-8      0   \n",
       "16  35514836.0  10003046-RR-10      0   \n",
       "17  35514836.0   10003046-RR-9      0   \n",
       "18  37049133.0  10002930-RR-19      0   \n",
       "19  37067082.0  10001217-RR-13      0   \n",
       "20  37067082.0  10001217-RR-14      0   \n",
       "21  39060235.0  10002013-RR-52      1   \n",
       "22  39060235.0  10002013-RR-53      1   \n",
       "23  39553978.0  10000032-RR-45      0   \n",
       "24  39765666.0  10000980-RR-62      0   \n",
       "\n",
       "                                          NOTE_ID_SEQ  \\\n",
       "0   [10003019-RR-16, 10003019-RR-17, 10003019-RR-1...   \n",
       "1   [10002155-RR-22, 10002155-RR-23, 10002155-RR-2...   \n",
       "2    [10001725-RR-10, 10001725-RR-11, 10001725-RR-12]   \n",
       "3   [10003400-RR-49, 10003400-RR-50, 10003400-RR-5...   \n",
       "4   [10003400-RR-49, 10003400-RR-50, 10003400-RR-5...   \n",
       "5   [10003400-RR-49, 10003400-RR-50, 10003400-RR-5...   \n",
       "6   [10002155-RR-22, 10002155-RR-23, 10002155-RR-2...   \n",
       "7   [10002155-RR-22, 10002155-RR-23, 10002155-RR-2...   \n",
       "8   [10002155-RR-22, 10002155-RR-23, 10002155-RR-2...   \n",
       "9   [10002155-RR-22, 10002155-RR-23, 10002155-RR-2...   \n",
       "10  [10002155-RR-22, 10002155-RR-23, 10002155-RR-2...   \n",
       "11  [10001217-RR-9, 10001217-RR-10, 10001217-RR-11...   \n",
       "12  [10002428-RR-48, 10002428-RR-50, 10002428-RR-5...   \n",
       "13                                    [10003046-RR-6]   \n",
       "14                     [10003046-RR-6, 10003046-RR-7]   \n",
       "15      [10003046-RR-6, 10003046-RR-7, 10003046-RR-8]   \n",
       "16  [10003046-RR-6, 10003046-RR-7, 10003046-RR-8, ...   \n",
       "17  [10003046-RR-6, 10003046-RR-7, 10003046-RR-8, ...   \n",
       "18                                   [10002930-RR-19]   \n",
       "19  [10001217-RR-9, 10001217-RR-10, 10001217-RR-11...   \n",
       "20  [10001217-RR-9, 10001217-RR-10, 10001217-RR-11...   \n",
       "21  [10002013-RR-32, 10002013-RR-34, 10002013-RR-3...   \n",
       "22  [10002013-RR-32, 10002013-RR-34, 10002013-RR-3...   \n",
       "23  [10000032-RR-14, 10000032-RR-15, 10000032-RR-1...   \n",
       "24  [10000980-RR-52, 10000980-RR-55, 10000980-RR-5...   \n",
       "\n",
       "                                              emb_seq  \n",
       "0   [[0.04571784197735888, 0.02954959064224078, -0...  \n",
       "1   [[0.040107877819736744, 0.07095444332914923, 0...  \n",
       "2   [[0.007828903479476993, 0.08233785253433663, 0...  \n",
       "3   [[0.022228072317655084, 0.09650489123149635, 0...  \n",
       "4   [[0.022228072317655084, 0.09650489123149635, 0...  \n",
       "5   [[0.022228072317655084, 0.09650489123149635, 0...  \n",
       "6   [[0.040107877819736744, 0.07095444332914923, 0...  \n",
       "7   [[0.040107877819736744, 0.07095444332914923, 0...  \n",
       "8   [[0.040107877819736744, 0.07095444332914923, 0...  \n",
       "9   [[0.040107877819736744, 0.07095444332914923, 0...  \n",
       "10  [[0.040107877819736744, 0.07095444332914923, 0...  \n",
       "11  [[0.007121110218495659, -0.009922850304841089,...  \n",
       "12  [[0.050403328985508686, 0.0354461913912506, -0...  \n",
       "13  [[0.0641411182690233, 0.06539167494293698, 0.0...  \n",
       "14  [[0.0641411182690233, 0.06539167494293698, 0.0...  \n",
       "15  [[0.0641411182690233, 0.06539167494293698, 0.0...  \n",
       "16  [[0.0641411182690233, 0.06539167494293698, 0.0...  \n",
       "17  [[0.0641411182690233, 0.06539167494293698, 0.0...  \n",
       "18  [[-0.019983626449689623, 0.0013677926485353698...  \n",
       "19  [[0.007121110218495659, -0.009922850304841089,...  \n",
       "20  [[0.007121110218495659, -0.009922850304841089,...  \n",
       "21  [[-0.009020838111375785, 0.043912610179967164,...  \n",
       "22  [[-0.009020838111375785, 0.043912610179967164,...  \n",
       "23  [[-0.02616058236401081, 0.05037255446348286, -...  \n",
       "24  [[0.04948653200219413, 0.026474391738920366, -...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Validate =====\n",
    "\n",
    "validate_train_embseq(train_embseq, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "# maximum sequence length\n",
    "max_num_notes = 32\n",
    "doc_emb_size = 64 # 768\n",
    "    \n",
    "class Sequential_Dataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        'Initialization'\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        # Load data and get label        \n",
    "        y = self.df['label'].iloc[index]\n",
    "        emb_seq = torch.tensor(self.df['emb_seq'].iloc[index])\n",
    "        emb_seq_out = torch.zeros(size=(max_num_notes, doc_emb_size), dtype=torch.float)  \n",
    "      \n",
    "        if len(emb_seq) > max_num_notes:\n",
    "            emb_seq_out[:max_num_notes] = emb_seq[:max_num_notes]\n",
    "        else:\n",
    "            emb_seq_out[:len(emb_seq)] = emb_seq\n",
    "\n",
    "        return emb_seq_out.cuda(), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_finetune(data_generator, model):\n",
    "    '''\n",
    "    tests finetuned model\n",
    "    '''\n",
    "    y_pred = []\n",
    "    y_label = []\n",
    "    model.eval()\n",
    "    loss_accumulate = 0.0\n",
    "    count = 0.0\n",
    "    for i, (output, label) in enumerate(data_generator):\n",
    "        output = output.permute(1,0,2)\n",
    "        score = model(output)\n",
    "        \n",
    "        m = torch.nn.Sigmoid()\n",
    "        logits = torch.squeeze(m(score))\n",
    "        loss_fct = torch.nn.BCELoss()            \n",
    "        \n",
    "        label = torch.from_numpy(np.array(label)).float().cuda()\n",
    "\n",
    "        loss = loss_fct(logits, label)\n",
    "        \n",
    "        loss_accumulate += loss\n",
    "        count += 1\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        \n",
    "        label_ids = label.to('cpu').numpy()\n",
    "        y_label = y_label + label_ids.flatten().tolist()\n",
    "        y_pred = y_pred + logits.flatten().tolist()\n",
    "        outputs = np.asarray([1 if i else 0 for i in (np.asarray(y_pred) >= 0.5)])\n",
    "    loss = loss_accumulate/count\n",
    "\n",
    "    # return y_label, y_pred  # DEBUG\n",
    "    metrics = compute_metrics_LSTM(y_label, y_pred)\n",
    "    return metrics, y_pred, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def main_finetune(train_embseq, lr, batch_size, train_epoch):\n",
    "    '''\n",
    "    finetune the model. Here, we are finetuning the LSTM!\n",
    "    \n",
    "    \n",
    "    '''\n",
    "\n",
    "    lr = lr\n",
    "    BATCH_SIZE = batch_size\n",
    "    train_epoch = train_epoch\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    model = ClinicalLSTM()\n",
    "    model.cuda()\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model, dim = 0)\n",
    "            \n",
    "    print('--- Data Preparation ---')\n",
    "    \n",
    "    params = {'batch_size': BATCH_SIZE,\n",
    "              'shuffle': True,\n",
    "              'num_workers': 0, \n",
    "              'drop_last': True}\n",
    "    \n",
    "    train_idxs, val_idxs = group_train_val(train_embseq['ICUSTAY_ID'])\n",
    "    train_data = train_embseq.iloc[train_idxs]\n",
    "    val_data = train_embseq.iloc[val_idxs]\n",
    "    train_data = train_data[['emb_seq', 'label']]\n",
    "    val_data = val_data[['emb_seq', 'label']]\n",
    "\n",
    "\n",
    "\n",
    "    training_set = Sequential_Dataset(train_data)\n",
    "    training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "    validation_set = Sequential_Dataset(val_data)\n",
    "    validation_generator = data.DataLoader(validation_set, **params)\n",
    "    \n",
    "    \n",
    "    opt = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    # early stopping\n",
    "    max_auc = 0\n",
    "    model_max = copy.deepcopy(model)\n",
    "   \n",
    "    print('--- Go for Training ---')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    for epo in range(train_epoch):\n",
    "        model.train()\n",
    "        for i, (output, label) in enumerate(training_generator):\n",
    "            print('batch', i)\n",
    "            output = output.permute(1,0,2)\n",
    "            score = model(output.cuda())\n",
    "       \n",
    "            label = torch.from_numpy(np.array(label)).float().cuda()\n",
    "            \n",
    "            loss_fct = torch.nn.BCELoss()\n",
    "            m = torch.nn.Sigmoid()\n",
    "            n = torch.squeeze(m(score))\n",
    "            \n",
    "            loss = loss_fct(n, label)\n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "           \n",
    "        # every epoch test\n",
    "        # with torch.set_grad_enabled(False):\n",
    "        #     # return test_finetune(validation_generator, model)\n",
    "        #     metrics, logits, loss = test_finetune(validation_generator, model)\n",
    "        #     if metrics['auc']:\n",
    "        #         if metrics['auc'] > max_auc:\n",
    "        #             model_max = copy.deepcopy(model)\n",
    "        #             max_auc = metrics['auc']\n",
    "                 \n",
    "        #     print('Validation at Epoch '+ str(epo + 1) + ' , AUC: '+ str(metrics['auc']) + ' , AUPRC: ' + str(metrics['auprc']))\n",
    "    \n",
    "    # print('--- Go for Testing ---')\n",
    "    # try:\n",
    "    #     with torch.set_grad_enabled(False):\n",
    "            \n",
    "    #         auc, auprc, logits, loss = test_finetune(testing_generator, model_max)\n",
    "    #         print('Testing AUROC: ' + str(auc) + ' , AUPRC: ' + str(auprc) + ' , Test loss: '+str(loss))\n",
    "    # except:\n",
    "    #     print('testing failed')\n",
    "    return model_max, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = train_data.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['emb_seq'][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset.from_pandas(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.set_format('torch', columns=['label', 'emb_seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['label'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([45, 64])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['emb_seq'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [10003019-RR-16, 10003019-RR-17, 10003019-RR-1...\n",
       "1     [10002155-RR-22, 10002155-RR-23, 10002155-RR-2...\n",
       "2      [10001725-RR-10, 10001725-RR-11, 10001725-RR-12]\n",
       "3     [10003400-RR-49, 10003400-RR-50, 10003400-RR-5...\n",
       "4     [10003400-RR-49, 10003400-RR-50, 10003400-RR-5...\n",
       "5     [10003400-RR-49, 10003400-RR-50, 10003400-RR-5...\n",
       "6     [10002155-RR-22, 10002155-RR-23, 10002155-RR-2...\n",
       "7     [10002155-RR-22, 10002155-RR-23, 10002155-RR-2...\n",
       "8     [10002155-RR-22, 10002155-RR-23, 10002155-RR-2...\n",
       "9     [10002155-RR-22, 10002155-RR-23, 10002155-RR-2...\n",
       "10    [10002155-RR-22, 10002155-RR-23, 10002155-RR-2...\n",
       "11    [10001217-RR-9, 10001217-RR-10, 10001217-RR-11...\n",
       "12    [10002428-RR-48, 10002428-RR-50, 10002428-RR-5...\n",
       "13                                      [10003046-RR-6]\n",
       "14                       [10003046-RR-6, 10003046-RR-7]\n",
       "15        [10003046-RR-6, 10003046-RR-7, 10003046-RR-8]\n",
       "16    [10003046-RR-6, 10003046-RR-7, 10003046-RR-8, ...\n",
       "17    [10003046-RR-6, 10003046-RR-7, 10003046-RR-8, ...\n",
       "18                                     [10002930-RR-19]\n",
       "19    [10001217-RR-9, 10001217-RR-10, 10001217-RR-11...\n",
       "20    [10001217-RR-9, 10001217-RR-10, 10001217-RR-11...\n",
       "21    [10002013-RR-32, 10002013-RR-34, 10002013-RR-3...\n",
       "22    [10002013-RR-32, 10002013-RR-34, 10002013-RR-3...\n",
       "23    [10000032-RR-14, 10000032-RR-15, 10000032-RR-1...\n",
       "24    [10000980-RR-52, 10000980-RR-55, 10000980-RR-5...\n",
       "Name: NOTE_ID_SEQ, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embseq.NOTE_ID_SEQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClinicalLSTM()\n",
    "model.cuda()\n",
    "score = model(output.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClinicalLSTM(\n",
       "  (encoder): LSTM(64, 64, num_layers=2, bidirectional=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0191],\n",
       "        [0.0084]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data\n",
    "# training_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data['emb_seq'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.tensor(df['emb_seq'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "BATCH_SIZE = 2\n",
    "train_epoch = 1\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "model = ClinicalLSTM()\n",
    "model.cuda()\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model, dim = 0)\n",
    "\n",
    "params = {'batch_size': BATCH_SIZE,\n",
    "            'shuffle': True,\n",
    "            'num_workers': 0, \n",
    "            'drop_last': True}\n",
    "\n",
    "train_idxs, val_idxs = group_train_val(train_embseq['ICUSTAY_ID'])\n",
    "train_data = train_embseq.iloc[train_idxs]\n",
    "val_data = train_embseq.iloc[val_idxs]\n",
    "train_data = train_data[['emb_seq', 'label']]\n",
    "val_data = val_data[['emb_seq', 'label']]\n",
    "\n",
    "\n",
    "\n",
    "training_set = Sequential_Dataset(train_data)\n",
    "training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = Sequential_Dataset(val_data)\n",
    "validation_generator = data.DataLoader(validation_set, **params)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "# early stopping\n",
    "# max_auc = 0\n",
    "# model_max = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_generator.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (output, label) in enumerate(training_generator):\n",
    "    # print(i)\n",
    "    # print(label, output)\n",
    "    output = output.permute(1,0,2)\n",
    "    score = model(output.cuda())\n",
    "\n",
    "    label = torch.from_numpy(np.array(label)).float().cuda()\n",
    "\n",
    "    loss_fct = torch.nn.BCELoss()\n",
    "    m = torch.nn.Sigmoid()\n",
    "    n = torch.squeeze(m(score))\n",
    "\n",
    "    loss = loss_fct(n, label)\n",
    "    loss_history.append(loss)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7570, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Preparation ---\n",
      "--- Go for Training ---\n",
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n",
      "batch 6\n",
      "batch 7\n",
      "batch 8\n",
      "Error: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "Validation at Epoch 1 , AUC: None , AUPRC: -0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ===== Prepare train val split =====\n",
    "\n",
    "# ===== Train LSTM ===== \n",
    "\n",
    "# clin_lstm = ClinicalLSTM()\n",
    "lr = 1e-5 \n",
    "batch_size = 2\n",
    "train_epoch = 1\n",
    "\n",
    "out = main_finetune(train_embseq, lr, batch_size, train_epoch)\n",
    "\n",
    "\n",
    "\n",
    "# ===== Save Sequential Embeddings =====\n",
    "# extract_emb_seq()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClinicalLSTM(\n",
       "  (encoder): LSTM(64, 64, num_layers=2, bidirectional=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_set.df['emb_seq'].iloc[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision_recall_fscore_support(out[0], out[1], average='binary')\n",
    "# accuracy_score(out[0], out[1])\n",
    "# ValueError: Classification metrics can't handle a mix of binary and continuous targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(float, float)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(out[0][0]),type(out[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clinical1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
