{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils import data\n",
    "import torch\n",
    "from transformers import LongformerTokenizerFast, LongformerForSequenceClassification, Trainer, TrainingArguments, LongformerConfig, T5Config\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import wandb\n",
    "import argparse\n",
    "from functools import partial\n",
    "import traceback\n",
    "\n",
    "from BoXHED_Fuse.src.log_artifact import log_artifact\n",
    "from BoXHED_Fuse.src.helpers import find_next_dir_index, merge_embs_to_seq, convert_to_list, convert_to_nested_list, compute_metrics_LSTM, Sequential_Dataset, Sequential_Dataset_FAST, group_train_val, explode_train_target, validate_train_emb_seq\n",
    "from BoXHED_Fuse.src.MyTrainer import MyTrainer \n",
    "from BoXHED_Fuse.models.ClinicalLSTM import ClinicalLSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_finetune(data_generator, model):\n",
    "    '''\n",
    "    tests finetuned model\n",
    "    '''\n",
    "    y_pred = []\n",
    "    y_label = []\n",
    "    model.eval()\n",
    "    loss_accumulate = 0.0\n",
    "    count = 0.0\n",
    "    for i, (output, label) in enumerate(data_generator):\n",
    "        output = output.permute(1,0,2)\n",
    "        score = model(output)\n",
    "        \n",
    "        m = torch.nn.Sigmoid()\n",
    "        logits = torch.flatten(torch.squeeze(m(score)))\n",
    "        loss_fct = torch.nn.BCELoss()    \n",
    "        label = torch.from_numpy(np.array(label)).float().cuda()\n",
    "\n",
    "        loss = loss_fct(logits, label)\n",
    "        \n",
    "        loss_accumulate += loss\n",
    "        count += 1\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        \n",
    "        label_ids = label.to('cpu').numpy()\n",
    "        y_label = y_label + label_ids.flatten().tolist()\n",
    "        y_pred = y_pred + logits.flatten().tolist()\n",
    "        outputs = np.asarray([1 if i else 0 for i in (np.asarray(y_pred) >= 0.5)])\n",
    "\n",
    "    loss = loss_accumulate/count\n",
    "\n",
    "    metrics = compute_metrics_LSTM(y_label, y_pred)\n",
    "\n",
    "    return metrics, y_pred, loss.item()\n",
    "\n",
    "def finetune(train_target, train_emb, lr, batch_size, train_epoch, use_scheduler=False):\n",
    "    '''\n",
    "    finetune the LSTM.\n",
    "    '''\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    model = ClinicalLSTM()\n",
    "    model.cuda()\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model, dim = 1)\n",
    "            \n",
    "    print('--- Data Preparation ---')\n",
    "    \n",
    "    params = {'batch_size': batch_size,\n",
    "              'shuffle': True,\n",
    "              'num_workers': 0, \n",
    "              'drop_last': True}\n",
    "    \n",
    "    train_target_exploded = explode_train_target(train_target)\n",
    "    train_target_exploded.reset_index(inplace=True)\n",
    "    train_idxs, val_idxs = group_train_val(train_target['ICUSTAY_ID'])\n",
    "\n",
    "    train_data = train_target.iloc[train_idxs]\n",
    "    val_data = train_target.iloc[val_idxs]\n",
    "\n",
    "    training_set = Sequential_Dataset_FAST(train_data, train_target_exploded, train_emb)\n",
    "    training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "    validation_set = Sequential_Dataset_FAST(val_data, train_target_exploded, train_emb)\n",
    "    validation_generator = data.DataLoader(validation_set, **params)\n",
    "    \n",
    "    opt = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=lr, steps_per_epoch=len(training_generator), epochs=train_epoch)\n",
    "\n",
    "    print('--- Go for Training ---')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    tstart = time.time()\n",
    "    for epo in range(train_epoch):\n",
    "        model.train()\n",
    "        print('model.train()...')\n",
    "        for i, (output, label) in enumerate(training_generator):\n",
    "            # print(f'time for loading batch: {time.time() - t0}')\n",
    "            output = output.permute(1,0,2)\n",
    "            score = model(output.cuda())\n",
    "            print('model(output.cuda()...')\n",
    "            label = torch.from_numpy(np.array(label)).float().cuda()\n",
    "            loss_fct = torch.nn.BCELoss()\n",
    "            m = torch.nn.Sigmoid()\n",
    "            logits = torch.flatten(torch.squeeze(m(score)))\n",
    "            loss = loss_fct(logits, label)\n",
    "            loss_history.append(loss.item())\n",
    "\n",
    "            if args.use_wandb:\n",
    "                wandb.log({\n",
    "                    \"Train Step Loss\": loss.item(),\n",
    "                    'Learning Rate': opt.param_groups[0]['lr'],\n",
    "                    'Epoch': epo + (i+1) / (len(training_generator) - 1)})\n",
    "            print(\"Train Step Loss\", loss.item(),\n",
    "                    'Learning Rate', opt.param_groups[0]['lr'],\n",
    "                    'Epoch', epo + (i+1) / (len(training_generator) - 1),\n",
    "                    'Time', int(time.time() - tstart))\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            if use_scheduler:\n",
    "                scheduler.step()\n",
    "            print('backward, step, ...')\n",
    "\n",
    "           \n",
    "        # every epoch test\n",
    "        with torch.set_grad_enabled(False):\n",
    "            # return test_finetune(validation_generator, model)\n",
    "            metrics, logits, loss = test_finetune(validation_generator, model)\n",
    "                 \n",
    "            if args.use_wandb:\n",
    "                wandb.log({\"Validation Loss\": loss, \"AUC\": metrics['auc'], \"AUPRC\": metrics['auprc']})\n",
    "            print('Validation at Epoch '+ str(epo + 1) + ' , AUC: '+ str(metrics['auc']) + ' , AUPRC: ' + str(metrics['auprc']))\n",
    "\n",
    "        if not args.sweep:\n",
    "            torch.save({\n",
    "                'epoch': epo,\n",
    "                'model_state_dict' : model.state_dict(),\n",
    "                'optimizer_state_dict' : opt.state_dict(),\n",
    "                'loss': loss,\n",
    "            }, f'{model_out_dir}/model_checkpoint_epoch{epo+1}.pt')\n",
    "            print(f'saved checkpoint {epo+1} to {model_out_dir}/model_checkpoint_epoch{epo}.pt')\n",
    "\n",
    "    return loss_history\n",
    "\n",
    "def sweep_func():\n",
    "    wandb.init(project=PROJECT_NAME, name=RUN_NAME)\n",
    "    config = wandb.config\n",
    "    print('Current wandb.config:', config)\n",
    "    try:\n",
    "        finetune(train_target=train_target,\n",
    "            train_emb=train_emb,\n",
    "            lr=config.lr, \n",
    "            batch_size=config.batch_size,\n",
    "            train_epoch=10, \n",
    "            use_scheduler=config.scheduler)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Initialize Args =====   \n",
    "parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--test', action='store_true', help='enable testing mode')\n",
    "# parser.add_argument('--use-wandb', action = 'store_true', help = 'enable wandb', default=False)\n",
    "# parser.add_argument('--sweep', action = 'store_true', help = 'enable sweep, do not store checkpoints (BAYESIAN SWEEPS RUN FOREVER UNTIL MANUALLY STOPPED)', default=False)\n",
    "# parser.add_argument('--gpu-no', dest = 'GPU_NO', help='use GPU_NO specified (this may be a single number or several. eg: 1 or 1,2,3,4)')\n",
    "# parser.add_argument('--note-type', dest = 'note_type', help='which notes, radiology or discharge?')\n",
    "# parser.add_argument('--num-epochs', dest = 'num_epochs', help = 'num_epochs to train', type=int)\n",
    "# parser.add_argument('--noteid-mode', dest = 'noteid_mode', help = 'kw: all or recent')\n",
    "# parser.add_argument('--embs-dir', dest = 'embs_dir', help = 'dir of train.pt containing embeddings')\n",
    "# parser.add_argument('--batch-size', dest = 'batch_size', default=4, type=int)\n",
    "# parser.add_argument('--use-scheduler', action = 'store_true', help = 'whether or not to use scheduler', default=False)\n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.gpu_no = 3\n",
    "args.note_type = 'radiology'\n",
    "args.num_epochs=10\n",
    "args.noteid_mode = 'all'\n",
    "args.embs_dir = /home/ugrads/a/aa_ron_su/BoXHED_Fuse/BoXHED_Fuse/JSS_SUBMISSION_NEW/data/embs/testing/Clinical-T5-Base_rad_all_out/from_epoch1/10\n",
    "assert(os.path.exists(args.embs_dir))\n",
    "if args.sweep:\n",
    "    args.use_wandb = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--test] [--use-wandb] [--sweep]\n",
      "                             [--gpu-no GPU_NO] [--note-type NOTE_TYPE]\n",
      "                             [--num-epochs NUM_EPOCHS]\n",
      "                             [--noteid-mode NOTEID_MODE] [--embs-dir EMBS_DIR]\n",
      "                             [--batch-size BATCH_SIZE] [--use-scheduler]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/home/ugrads/a/aa_ron_su/.local/share/jupyter/runtime/kernel-v2-30648772856PNKHGW0S.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_target_path = f'{os.getenv(\"BHF_ROOT\")}/JSS_SUBMISSION_NEW/data/targets/till_end_mimic_iv_extra_features_train_NOTE_TARGET_2_{args.note_type[:3]}_{args.noteid_mode}.csv'\n",
    "\n",
    "MODEL_NAME = 'Clinical-LSTM'\n",
    "model_out_dir = f'{os.getenv(\"BHF_ROOT\")}/model_outputs/{MODEL_NAME}_{args.note_type[:3]}_{args.noteid_mode}_out'\n",
    "\n",
    "if args.test:\n",
    "    train_target_path = os.path.join(os.path.dirname(train_target_path), 'testing', os.path.basename(train_target_path)) \n",
    "    model_out_dir = os.path.join(os.path.dirname(model_out_dir), 'testing', os.path.basename(model_out_dir))\n",
    "\n",
    "train_emb_seq_path = f'{model_out_dir}/train_emb_seq.csv' # tmp csv for multiple runs\n",
    "\n",
    "\n",
    "if not os.path.exists(model_out_dir):\n",
    "    os.makedirs(model_out_dir)\n",
    "\n",
    "RUN_CNTR = find_next_dir_index(model_out_dir)\n",
    "model_out_dir = os.path.join(model_out_dir, str(RUN_CNTR))\n",
    "assert(not os.path.exists(model_out_dir))\n",
    "os.makedirs(model_out_dir)\n",
    "print(f'created all dirs in model_out_dir', model_out_dir)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.GPU_NO)\n",
    "assert(args.batch_size % len(args.GPU_NO.split(',')) == 0)\n",
    "\n",
    "# ===== Read Data =====\n",
    "print('reading data')\n",
    "train_emb = torch.load(f'{args.embs_dir}/train_embs.pt')\n",
    "train_target = pd.read_csv(train_target_path, converters = {'NOTE_ID_SEQ': convert_to_list})\n",
    "# ===== Merge data into {train_embs_seq, label} =====\n",
    "print('merging data into df with cols {train_embs_seq, label}')\n",
    "target = 'delta_in_2_days' \n",
    "train_target.rename(columns = {target:'label'}, inplace=True)\n",
    "\n",
    "# ===== Train LSTM ===== \n",
    "RUN_NAME = f'{MODEL_NAME}_{args.note_type[:3]}_{args.noteid_mode}_{RUN_CNTR}'\n",
    "PROJECT_NAME = 'BoXHED_Fuse'\n",
    "\n",
    "# log_artifact(artifact_path = out_trainpath,\n",
    "#             artifact_name = os.path.splitext(os.path.basename(out_trainpath))[0] + '.test' if args.test else '',\n",
    "#             artifact_description = \"MIMIC IV joined with note data for finetuning\",\n",
    "#             artifact_metadata= dict(args._get_kwargs()),\n",
    "#             project_name = os.getenv('WANDB_PROJECT_NAME'),\n",
    "#             do_filter=True,)\n",
    "if args.use_wandb:\n",
    "    # wandb.login(key=os.getenv('WANDB_KEY_PERSONAL'), relogin = True)\n",
    "    wandb.login(key=os.getenv('WANDB_KEY_TAMU'), relogin = True)\n",
    "if args.sweep:\n",
    "    sweep_configuration = {\n",
    "        \"method\" : \"bayes\",\n",
    "        \"name\": f\"sweep_{RUN_NAME}\",\n",
    "        \"metric\": {\"goal\": \"minimize\", \"name\": \"Validation Loss\"},\n",
    "        \"parameters\": {\n",
    "            \"batch_size\": {\"values\": [16, 32, 64]},\n",
    "            \"lr\": {\"max\": 1e-4, \n",
    "                    \"min\":  1e-6},\n",
    "            \"scheduler\": {\"values\" : [True, False]}\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "    sweep_id = wandb.sweep(sweep=sweep_configuration, project=PROJECT_NAME)\n",
    "    wandb.agent(sweep_id=sweep_id, function=sweep_func, count = 12)\n",
    "else:\n",
    "    if args.use_wandb:\n",
    "        wandb.init(project=PROJECT_NAME, name=RUN_NAME)\n",
    "\n",
    "    lr = 1e-5\n",
    "    batch_size = args.batch_size\n",
    "    train_epoch = args.num_epochs\n",
    "    out = finetune(train_target, train_emb, lr, batch_size, train_epoch, use_scheduler=args.use_scheduler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clinical1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
