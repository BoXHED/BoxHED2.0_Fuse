{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from transformers import LongformerTokenizerFast, LongformerForSequenceClassification, Trainer, TrainingArguments, LongformerConfig, T5Config\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3,4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Config {\n",
       "  \"d_ff\": 2048,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 512,\n",
       "  \"dense_act_fn\": \"relu\",\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"relu\",\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"is_gated_act\": false,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"model_type\": \"t5\",\n",
       "  \"num_decoder_layers\": 6,\n",
       "  \"num_heads\": 8,\n",
       "  \"num_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"relative_attention_max_distance\": 128,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"transformers_version\": \"4.24.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32128\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = T5Config()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/ugrads/a/aa_ron_su/physionet.org/files/clinical-t5/1.0.0/'\n",
    "data_path = '/data/datasets/mimiciv_notes/physionet.org/files/mimic-iv-note/2.2/note/discharge.csv'\n",
    "model_path = root + 'Clinical-T5-Base/'\n",
    "finetune_model_path = root + 'Clinical-T5-Base_ft_vent/'\n",
    "temivef_train_NOTE_TARGET1_FT_path = '/home/ugrads/a/aa_ron_su/JSS_SUBMISSION_NEW/data/till_end_mimic_iv_extra_features_train_NOTE_TARGET1_FT_rad.csv'\n",
    "model_name = \"Clinical-T5-Base\"\n",
    "out_dir = f\"{model_name}_out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/transformers/configuration_utils.py:369: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Config\n",
    "from T5EncoderForSequenceClassification import T5EncoderForSequenceClassification\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "encoder = model.get_encoder() # we only need the clinical-t5 encoder for our purposes\n",
    "\n",
    "config = T5Config(\n",
    "    hidden_size=768,\n",
    "    classifier_dropout=None,\n",
    "    num_labels=2,\n",
    "    hidden_dropout_prob=0.01,\n",
    "    last_hidden_size=64,\n",
    "    gradient_checkpointing=True\n",
    "\n",
    ")\n",
    "classifier = T5EncoderForSequenceClassification(encoder, config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading notes and target from /home/ugrads/a/aa_ron_su/JSS_SUBMISSION_NEW/data/till_end_mimic_iv_extra_features_train_NOTE_TARGET1_FT_rad.csv\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(temivef_train_NOTE_TARGET1_FT_path)\n",
    "print(f\"reading notes and target from {temivef_train_NOTE_TARGET1_FT_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_train_test(ID):\n",
    "    ID             = ID.astype(int)\n",
    "    ID_unique_srtd = np.unique(ID)\n",
    "    np.random.shuffle(ID_unique_srtd)    \n",
    "\n",
    "    num_train_ids = int(.80 * len(ID_unique_srtd))\n",
    "    train_ids = ID_unique_srtd[:num_train_ids]\n",
    "    val_ids = ID_unique_srtd[num_train_ids:]\n",
    "\n",
    "    train = ID[ID.isin(train_ids)]\n",
    "    val = ID[ID.isin(val_ids)]\n",
    "\n",
    "    assert(len(train) + len(val) == len(ID))\n",
    "    assert(len(train_ids) + len(val_ids) == len(ID_unique_srtd))\n",
    "    assert(len(train_ids) + len(val_ids) == len(ID_unique_srtd))\n",
    "\n",
    "    return list(train.index), list(val.index)\n",
    "\n",
    "train_idxs, val_idxs = group_train_test(train['ICUSTAY_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n\u001b[1;32m      2\u001b[0m target \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdelta_in_2_days\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m train \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39mrename(columns \u001b[39m=\u001b[39m {target:\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[1;32m      5\u001b[0m train_data \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39miloc[train_idxs]\n\u001b[1;32m      6\u001b[0m val_data \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39miloc[val_idxs]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "target = 'delta_in_2_days'\n",
    "train = train.rename(columns = {target:'label'})\n",
    "\n",
    "train_data = train.iloc[train_idxs]\n",
    "val_data = train.iloc[val_idxs]\n",
    "\n",
    "train_data = Dataset.from_pandas(train_data).select_columns(['text', 'label'])\n",
    "val_data = Dataset.from_pandas(val_data).select_columns(['text', 'label'])\n",
    "\n",
    "if not os.path.exists(f'{out_dir}/data_cache'):\n",
    "    # define a function that will tokenize the model, and will return the relevant inputs for the model\n",
    "    def tokenization(batched_text):\n",
    "        return tokenizer(batched_text['text'], padding = 'max_length', truncation=True, max_length = 512)\n",
    "\n",
    "    train_data = train_data.map(tokenization, batched = True, batch_size = len(train_data) // 10)\n",
    "    val_data = val_data.map(tokenization, batched = True, batch_size = len(val_data) // 10)\n",
    "\n",
    "    train_data.save_to_disk(f'{out_dir}/data_cache/tokenized_train_data')\n",
    "    val_data.save_to_disk(f'{out_dir}/data_cache/tokenized_val_data')\n",
    "\n",
    "else: \n",
    "    print(f'loading train, val from', f'{out_dir}/data_cache/')\n",
    "    train_data = train_data.load_from_disk(f'{out_dir}/data_cache/tokenized_train_data')\n",
    "    val_data = val_data.load_from_disk(f'{out_dir}/data_cache/tokenized_val_data')\n",
    "\n",
    "train_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "train_data = train_data.remove_columns('text')\n",
    "val_data = val_data.remove_columns('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define accuracy metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # argmax(pred.predictions, axis=1)\n",
    "    #pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = f'{out_dir}/results',\n",
    "    num_train_epochs = 5,\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 8,    \n",
    "    per_device_eval_batch_size= 4,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    disable_tqdm = False, \n",
    "    load_best_model_at_end=True,\n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps = 8,\n",
    "    fp16 = True,\n",
    "    logging_dir=f'{out_dir}/logs',\n",
    "    dataloader_num_workers = 0,\n",
    "    run_name = 't5_radiology_run1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the trainer class and check for available devices\n",
    "trainer = Trainer(\n",
    "    model=classifier,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data\n",
    ")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maa_ron_su\u001b[0m (\u001b[33maaron_team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ugrads/a/aa_ron_su/physionet.org/files/clinical-t5/1.0.0/wandb/run-20230521_013859-t6vit1ae</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aaron_team/1.0.0/runs/t6vit1ae' target=\"_blank\">gallant-disco-7</a></strong> to <a href='https://wandb.ai/aaron_team/1.0.0' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aaron_team/1.0.0' target=\"_blank\">https://wandb.ai/aaron_team/1.0.0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aaron_team/1.0.0/runs/t6vit1ae' target=\"_blank\">https://wandb.ai/aaron_team/1.0.0/runs/t6vit1ae</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 60967\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 4760\n",
      "  Number of trainable parameters = 110258498\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wandb.ai/aaron_team/1.0.0/runs/t6vit1ae\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='4760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/4760 : < :, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ugrads/a/aa_ron_su/physionet.org/files/clinical-t5/1.0.0/T5EncoderForSequenceClassification.py\", line 70, in forward\n    encoder_outputs = self.encoder(\n  File \"/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n    layer_outputs = layer_module(\n  File \"/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n    self_attention_outputs = self.layer[0](\n  File \"/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n    attention_output = self.SelfAttention(\n  File \"/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 542, in forward\n    attn_weights = nn.functional.dropout(\n  File \"/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/torch/nn/functional.py\", line 1252, in dropout\n    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\nRuntimeError: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 10.92 GiB total capacity; 9.99 GiB already allocated; 81.44 MiB free; 10.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m wandb\u001b[39m.\u001b[39minit()\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(wandb\u001b[39m.\u001b[39mrun\u001b[39m.\u001b[39mget_url())\n\u001b[0;32m----> 4\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/clinical1/lib/python3.10/site-packages/transformers/trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1498\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1499\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1500\u001b[0m )\n\u001b[0;32m-> 1501\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1502\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1503\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1504\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1505\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1506\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/clinical1/lib/python3.10/site-packages/transformers/trainer.py:1749\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1748\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1749\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1751\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1752\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1753\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1754\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1755\u001b[0m ):\n\u001b[1;32m   1756\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/clinical1/lib/python3.10/site-packages/transformers/trainer.py:2508\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2505\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2507\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2508\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2510\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2511\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/clinical1/lib/python3.10/site-packages/transformers/trainer.py:2540\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2538\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2539\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2540\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2541\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2542\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2543\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/clinical1/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/clinical1/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:168\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[1;32m    167\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 168\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparallel_apply(replicas, inputs, kwargs)\n\u001b[1;32m    169\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgather(outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/clinical1/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:178\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparallel_apply\u001b[39m(\u001b[39mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 178\u001b[0m     \u001b[39mreturn\u001b[39;00m parallel_apply(replicas, inputs, kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_ids[:\u001b[39mlen\u001b[39;49m(replicas)])\n",
      "File \u001b[0;32m~/miniconda3/envs/clinical1/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:86\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     84\u001b[0m     output \u001b[39m=\u001b[39m results[i]\n\u001b[1;32m     85\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m---> 86\u001b[0m         output\u001b[39m.\u001b[39mreraise()\n\u001b[1;32m     87\u001b[0m     outputs\u001b[39m.\u001b[39mappend(output)\n\u001b[1;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/clinical1/lib/python3.10/site-packages/torch/_utils.py:461\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ugrads/a/aa_ron_su/physionet.org/files/clinical-t5/1.0.0/T5EncoderForSequenceClassification.py\", line 70, in forward\n    encoder_outputs = self.encoder(\n  File \"/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n    layer_outputs = layer_module(\n  File \"/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 673, in forward\n    self_attention_outputs = self.layer[0](\n  File \"/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 579, in forward\n    attention_output = self.SelfAttention(\n  File \"/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 542, in forward\n    attn_weights = nn.functional.dropout(\n  File \"/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/torch/nn/functional.py\", line 1252, in dropout\n    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\nRuntimeError: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 10.92 GiB total capacity; 9.99 GiB already allocated; 81.44 MiB free; 10.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init()\n",
    "print(wandb.run.get_url())\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "\n",
    "# train_inputs = train_tensor\n",
    "# train_labels = torch.tensor(train['delta_in_2_days'].to_numpy())\n",
    "# train_dataset = TensorDataset(train_inputs.to(device), train_labels.to(device)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9723,) (4850,)\n",
      "(9720,) (4853,)\n",
      "(9703,) (4870,)\n"
     ]
    }
   ],
   "source": [
    "# from torch.utils.data import Subset\n",
    "# batch_size = 4\n",
    "\n",
    "\n",
    "# for fold, (train_idx, val_idx) in enumerate(kfold):\n",
    "#     print(train_idx.shape, val_idx.shape)\n",
    "#     val_set = Subset(train_dataset, val_idx)\n",
    "#     val_dataloader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "#     train_set = Subset(train_dataset, train_idx)\n",
    "#     train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "#     print(len(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes_to_extract = df['text']\n",
    "# texts = notes_to_extract.tolist()\n",
    "# tokenized_notes_to_extract = tokenizer(texts, truncation=True, padding=True, return_tensors = \"pt\")\n",
    "\n",
    "# # test_note_to_extract = notes_to_extract.iloc[0]\n",
    "# # tokenized_test_note = tokenize_function(test_note_to_extract)\n",
    "# # tokenized_test_note.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from T5EncoderForSequenceClassification import T5EncoderForSequenceClassification, T5EncoderClassificationHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.nn.utils import parameters_to_vector\n",
    "\n",
    "# num_params = len(parameters_to_vector(encoder.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109618560"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "639938"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(parameters_to_vector(classifier.classifier.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.encoder.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5EncoderClassificationHead(\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (last_dense): Linear(in_features=768, out_features=64, bias=True)\n",
       "  (dropout): Dropout(p=0.01, inplace=False)\n",
       "  (out_proj): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classifier.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "# from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# # Define your training data\n",
    "# train_inputs = tokenized_notes_to_extract.input_ids\n",
    "# # train_labels = torch.tensor(np.random.rand(len(train_inputs)))\n",
    "# train_labels = torch.tensor(df['delta_in_2_days'].to_numpy())\n",
    "# train_dataset = TensorDataset(train_inputs.to(device), train_labels.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_dataset[0][0].device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "# train_sampler = RandomSampler(train_dataset)\n",
    "# train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 16\n",
    "# num_epochs = 1\n",
    "# learning_rate = 5e-5\n",
    "# adam_epsilon = 1e-8\n",
    "# max_grad_norm = 1.0\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.AdamW(classifier.classifier.parameters(), lr=learning_rate, eps=adam_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_steps = len(train_dataloader) * num_epochs\n",
    "# scheduler = get_linear_schedule_with_warmup(\n",
    "#     optimizer,\n",
    "#     num_warmup_steps=0,\n",
    "#     num_training_steps=total_steps\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze encoder weights:\n",
    "for param in classifier.encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# classifier.classifier.train()\n",
    "# for epoch in tqdm(range(num_epochs)):\n",
    "#     for step, batch in enumerate(train_dataloader):\n",
    "#         inputs, labels = batch\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = classifier.forward(inputs, labels=labels)\n",
    "#         loss = outputs.loss\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(classifier.classifier.parameters(), max_grad_norm)\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "\n",
    "#         # Print progress every 10 steps\n",
    "#         if step % 10 == 0:\n",
    "#             print(f\"Epoch {epoch + 1}/{num_epochs} | Step {step}/{len(train_dataloader)} | Loss {loss.item():.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT NOTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single_embedding = classifier.forward(train_inputs[0:2], labels = train_labels[0:2], return_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = classifier.forward(train_inputs, labels = train_labels, return_embeddings=True)\n",
    "# embeddings_df = pd.DataFrame({'embedding': list(embeddings.detach().numpy())})\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df = pd.concat([df, embeddings_df], axis = 1)\n",
    "# df.to_csv(mimic_iv_train_NOTE_EMBEDDINGS_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_df = pd.DataFrame({'embedding': list(embeddings.detach().numpy())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_small = df.iloc[0:5].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_small = pd.concat([df_small, embeddings_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge embeddings with dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>ICUSTAY_ID</th>\n",
       "      <th>t_start</th>\n",
       "      <th>t_end</th>\n",
       "      <th>NOTE_ID</th>\n",
       "      <th>text</th>\n",
       "      <th>delta</th>\n",
       "      <th>Capillary refill rate_1.0</th>\n",
       "      <th>Capillary refill rate_nan</th>\n",
       "      <th>Ethnicity_1.0</th>\n",
       "      <th>...</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>#past_IVs</th>\n",
       "      <th>t_from_last_IV_t_start</th>\n",
       "      <th>t_from_last_IV_t_end</th>\n",
       "      <th>INTIME</th>\n",
       "      <th>t_start_DT</th>\n",
       "      <th>time_since_note</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10248673</td>\n",
       "      <td>33680639</td>\n",
       "      <td>24.000</td>\n",
       "      <td>24.388</td>\n",
       "      <td>10248673-DS-5</td>\n",
       "      <td>\\nName:  ___                   Unit No:   ___...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>69.516622</td>\n",
       "      <td>168.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18.250</td>\n",
       "      <td>11.167</td>\n",
       "      <td>2177-06-20 13:36:43</td>\n",
       "      <td>2177-06-21 13:36:43.000000000</td>\n",
       "      <td>901.611944</td>\n",
       "      <td>[0.08366300067315613, -0.09525524751737473, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10248673</td>\n",
       "      <td>33680639</td>\n",
       "      <td>24.388</td>\n",
       "      <td>25.038</td>\n",
       "      <td>10248673-DS-5</td>\n",
       "      <td>\\nName:  ___                   Unit No:   ___...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>69.516622</td>\n",
       "      <td>168.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19.250</td>\n",
       "      <td>12.167</td>\n",
       "      <td>2177-06-20 13:36:43</td>\n",
       "      <td>2177-06-21 13:59:59.800000000</td>\n",
       "      <td>901.999944</td>\n",
       "      <td>[0.09037796095801452, -0.1034607368093436, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10248673</td>\n",
       "      <td>33680639</td>\n",
       "      <td>25.038</td>\n",
       "      <td>25.388</td>\n",
       "      <td>10248673-DS-5</td>\n",
       "      <td>\\nName:  ___                   Unit No:   ___...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>69.516622</td>\n",
       "      <td>168.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19.900</td>\n",
       "      <td>12.817</td>\n",
       "      <td>2177-06-20 13:36:43</td>\n",
       "      <td>2177-06-21 14:38:59.800000000</td>\n",
       "      <td>902.649944</td>\n",
       "      <td>[0.10461136858709813, -0.08170781207205698, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10248673</td>\n",
       "      <td>33680639</td>\n",
       "      <td>25.388</td>\n",
       "      <td>25.421</td>\n",
       "      <td>10248673-DS-5</td>\n",
       "      <td>\\nName:  ___                   Unit No:   ___...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>69.516622</td>\n",
       "      <td>168.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.250</td>\n",
       "      <td>13.167</td>\n",
       "      <td>2177-06-20 13:36:43</td>\n",
       "      <td>2177-06-21 14:59:59.800000000</td>\n",
       "      <td>902.999944</td>\n",
       "      <td>[0.09262458924580079, -0.1298178264939714, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10248673</td>\n",
       "      <td>33680639</td>\n",
       "      <td>25.421</td>\n",
       "      <td>26.388</td>\n",
       "      <td>10248673-DS-5</td>\n",
       "      <td>\\nName:  ___                   Unit No:   ___...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>69.516622</td>\n",
       "      <td>168.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.283</td>\n",
       "      <td>13.200</td>\n",
       "      <td>2177-06-20 13:36:43</td>\n",
       "      <td>2177-06-21 15:01:58.600000000</td>\n",
       "      <td>903.032944</td>\n",
       "      <td>[0.09782734270128297, -0.11514283711793492, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT_ID  ICUSTAY_ID  t_start   t_end        NOTE_ID  \\\n",
       "0    10248673    33680639   24.000  24.388  10248673-DS-5   \n",
       "1    10248673    33680639   24.388  25.038  10248673-DS-5   \n",
       "2    10248673    33680639   25.038  25.388  10248673-DS-5   \n",
       "3    10248673    33680639   25.388  25.421  10248673-DS-5   \n",
       "4    10248673    33680639   25.421  26.388  10248673-DS-5   \n",
       "\n",
       "                                                text  delta  \\\n",
       "0   \\nName:  ___                   Unit No:   ___...      0   \n",
       "1   \\nName:  ___                   Unit No:   ___...      0   \n",
       "2   \\nName:  ___                   Unit No:   ___...      0   \n",
       "3   \\nName:  ___                   Unit No:   ___...      0   \n",
       "4   \\nName:  ___                   Unit No:   ___...      0   \n",
       "\n",
       "   Capillary refill rate_1.0  Capillary refill rate_nan  Ethnicity_1.0  ...  \\\n",
       "0                          0                          1              0  ...   \n",
       "1                          0                          1              0  ...   \n",
       "2                          0                          1              0  ...   \n",
       "3                          0                          1              0  ...   \n",
       "4                          0                          1              0  ...   \n",
       "\n",
       "         Age  Height  Weight  #past_IVs  t_from_last_IV_t_start  \\\n",
       "0  69.516622   168.0    66.0          1                  18.250   \n",
       "1  69.516622   168.0    66.0          1                  19.250   \n",
       "2  69.516622   168.0    66.0          1                  19.900   \n",
       "3  69.516622   168.0    66.0          1                  20.250   \n",
       "4  69.516622   168.0    66.0          1                  20.283   \n",
       "\n",
       "   t_from_last_IV_t_end               INTIME                     t_start_DT  \\\n",
       "0                11.167  2177-06-20 13:36:43  2177-06-21 13:36:43.000000000   \n",
       "1                12.167  2177-06-20 13:36:43  2177-06-21 13:59:59.800000000   \n",
       "2                12.817  2177-06-20 13:36:43  2177-06-21 14:38:59.800000000   \n",
       "3                13.167  2177-06-20 13:36:43  2177-06-21 14:59:59.800000000   \n",
       "4                13.200  2177-06-20 13:36:43  2177-06-21 15:01:58.600000000   \n",
       "\n",
       "   time_since_note                                          embedding  \n",
       "0       901.611944  [0.08366300067315613, -0.09525524751737473, 0....  \n",
       "1       901.999944  [0.09037796095801452, -0.1034607368093436, 0.2...  \n",
       "2       902.649944  [0.10461136858709813, -0.08170781207205698, 0....  \n",
       "3       902.999944  [0.09262458924580079, -0.1298178264939714, 0.2...  \n",
       "4       903.032944  [0.09782734270128297, -0.11514283711793492, 0....  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0552], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outputs['classifier_last_hidden_state'][0].shape\n",
    "# outputs['logits'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_with_dense = T5EncoderWithDense(encoder = encoder, num_classes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense_layer_outputs = encoder_with_dense(inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0930]])]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoder_with_dense.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0930]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dense_layer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0374]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dense(pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Freeze the weights of the encoder layers\n",
    "# for param in model_encoder_only.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# # dataset = load_dataset('csv', data_files=mimic_iv_train_NOTE_path, split='train') # split = 'train\n",
    "# df = pd.read_csv(mimic_iv_train_NOTE_path)[['NOTE_ID', 'text']]\n",
    "# df.drop_duplicates(inplace=True)\n",
    "# df.dropna(inplace=True) \n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# inputs = tokenized_test_note\n",
    "# outputs = tokenized_test_note\n",
    "# outputs = model(input_ids = inputs.input_ids, attention_mask = inputs.attention_mask, decoder_input_ids = inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import torch\n",
    "# inputs = tokenized_test_note\n",
    "# labels = torch.tensor([1]).unsqueeze(0)\n",
    "# outputs = model_encoder_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_note_to_encode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[139], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m encoder\u001b[39m.\u001b[39mforward(test_note_to_encode)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_note_to_encode' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m generated_outputs \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m i, row \u001b[39min\u001b[39;00m notes_to_extract\u001b[39m.\u001b[39miterrows():\n\u001b[1;32m      5\u001b[0m     input_text \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# generated_outputs = []\n",
    "# labels = torch.tensor([1]).unsqueeze(0)\n",
    "\n",
    "# for i, row in notes_to_extract.iterrows():\n",
    "#     input_text = row['text']\n",
    "#     input_ids = tokenizer.encode(input_text, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "#     outputs = model(**inputs, labels=labels)\n",
    "#     generated_outputs.append(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Generate a random target dataset with n = 1000\n",
    "# n = 1000\n",
    "# target_df = np.random.rand(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clinical_t5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
