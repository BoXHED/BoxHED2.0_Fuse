{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train, test, dataloaders generated\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--test', action='store_true', help='enable testing mode')\n",
    "# parser.add_argument('--gpu-no', dest = 'GPU_NO', help='use GPU_NO specified')\n",
    "# parser.add_argument('--ckpt-dir', dest = 'ckpt_dir', help='FULL PATH of directory where model checkpoint is stored')\n",
    "# parser.add_argument('--ckpt-model-name', dest = 'ckpt_model_name', help='directory where model checkpoint is stored')\n",
    "# parser.add_argument('--note-type', dest = 'note_type', help='which notes, radiology or discharge?')\n",
    "# parser.add_argument('--model-name', dest = 'model_name')\n",
    "# parser.add_argument('--model-type', dest = 'model_type', help = 'T5 or Longformer')\n",
    "# parser.add_argument('--run-cntr', dest = 'run_cntr', help = 'appended to dirname for storing additional runs')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# args_list = [arg for arg in vars(args) if getattr(args, arg) is not None]\n",
    "# print(args_list)\n",
    "# required_arguments = ['GPU_NO', 'ckpt_model_name', 'ckpt_dir', 'note_type', 'run_cntr', 'model_name', 'model_type']  # List of required arguments\n",
    "# # Check if any of the required arguments are missing\n",
    "# missing_args = [arg for arg in required_arguments if arg not in args_list]\n",
    "# if missing_args != []:\n",
    "#     raise ValueError(f\"Missing required arguments: {missing_args}\")\n",
    "\n",
    "testing = True\n",
    "GPU_NO = 0\n",
    "ckpt_dir = '/home/ugrads/a/aa_ron_su/physionet.org/files/clinical-t5/1.0.0/Clinical-T5-Base_out/2/results'\n",
    "ckpt_model_name = 'model_checkpoint__epoch1.pt'\n",
    "note_type = 'radiology'\n",
    "model_name = 'Clinical-T5-Base'\n",
    "model_type = 'T5'\n",
    "run_cntr = '1'\n",
    "\n",
    "print(f'joining {ckpt_dir} and {ckpt_model_name}')\n",
    "finetuned_model_path = os.path.join(ckpt_dir, ckpt_model_name)\n",
    "# print(\"extract_embeddings.py args:\")\n",
    "# for arg in vars(args):\n",
    "#     print(f\"\\t{arg}: {getattr(args, arg)}\")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPU_NO)  # use the correct gpu\n",
    "device = torch.device(f'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "# import torch.nn as nn\n",
    "# import sys\n",
    "\n",
    "root = '/home/ugrads/a/aa_ron_su/physionet.org/files/clinical-t5/1.0.0'\n",
    "# finetuned_model_path = root + '/model_from_ckpt1/meta_ft_classify.pt' # modify this line!\n",
    "temivef_train_NOTE_TARGET1_FT_path = f'/home/ugrads/a/aa_ron_su/JSS_SUBMISSION/data/till_end_mimic_iv_extra_features_train_NOTE_TARGET1_FT_{\"rad\" if note_type == \"radiology\" else \"\"}.csv'\n",
    "temivef_test_NOTE_TARGET1_FT_path = f'/home/ugrads/a/aa_ron_su/JSS_SUBMISSION/data/till_end_mimic_iv_extra_features_test_NOTE_TARGET1_FT_{\"rad\" if note_type == \"radiology\" else \"\"}.csv'\n",
    "temivef_train_NOTE_path = f'/home/ugrads/a/aa_ron_su/JSS_SUBMISSION/data/till_end_mimic_iv_extra_features_train_NOTE_{\"rad\" if note_type == \"radiology\" else \"\"}.csv'\n",
    "temivef_test_NOTE_path = f'/home/ugrads/a/aa_ron_su/JSS_SUBMISSION/data/till_end_mimic_iv_extra_features_test_NOTE_{\"rad\" if note_type == \"radiology\" else \"\"}.csv'\n",
    "outfolder = f\"{model_name}_{'rad' if note_type == 'radiology' else 'dis'}_{'test_' if testing else ''}out/{run_cntr}\"\n",
    "out_dir = f'/home/ugrads/a/aa_ron_su/JSS_SUBMISSION/data/final/{outfolder}/' # modify this line!\n",
    "\n",
    "train_outpath = os.path.join(out_dir, 'till_end_mimic_iv_extra_features_train.csv')\n",
    "test_outpath = os.path.join(out_dir, 'till_end_mimic_iv_extra_features_test.csv') \n",
    "\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "from helpers import tokenization\n",
    "from functools import partial\n",
    "from transformers import LongformerTokenizerFast, AutoTokenizer\n",
    "\n",
    "tokenizer = None\n",
    "if model_type == 'T5':\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "elif model_type == 'Longformer':\n",
    "    model_path = \"yikuan8/Clinical-Longformer\"\n",
    "    tokenizer = LongformerTokenizerFast.from_pretrained(model_path)\n",
    "else:\n",
    "    print(\"incorrect model_type specified. Should be T5 or Longformer\")\n",
    "    exit(1)\n",
    "assert(tokenizer != None)\n",
    "\n",
    "def df_to_tokens_ds(data):\n",
    "    data = Dataset.from_pandas(data).select_columns(['text', 'label'])\n",
    "    data = data.map(partial(tokenization, tokenizer, note_type, model_type), batched = True, batch_size = len(train_data) // 10)\n",
    "    data = data.remove_columns('text')\n",
    "    return data\n",
    "target = 'delta_in_2_days'\n",
    "train_data = pd.read_csv(temivef_train_NOTE_TARGET1_FT_path).rename(columns = {target:'label'})\n",
    "test_data = pd.read_csv(temivef_test_NOTE_TARGET1_FT_path).rename(columns = {target:'label'})\n",
    "\n",
    "tokenized_train_notes = df_to_tokens_ds(train_data)\n",
    "tokenized_test_notes = df_to_tokens_ds(test_data)\n",
    "print(f'tokenized_train_notes dataset:, {tokenized_train_notes}')\n",
    "print(f'tokenized_test_notes dataset:, {tokenized_test_notes}')\n",
    "\n",
    "tokenized_train_notes = pd.DataFrame(tokenized_train_notes)\n",
    "tokenized_test_notes = pd.DataFrame(tokenized_test_notes)\n",
    "print('tokenized notes converted back to dataframes for extraction...')\n",
    "\n",
    "\n",
    "classifier = torch.load(finetuned_model_path) \n",
    "print(f'classifier loaded from {finetuned_model_path}, class is {classifier.__class__}')\n",
    "\n",
    "if testing:\n",
    "    tokenized_train_notes = tokenized_train_notes[:12]\n",
    "    tokenized_test_notes = tokenized_test_notes[:12]\n",
    "    print(f\"testing mode truncated tokenized_notes to length {len(tokenized_train_notes)}\")\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from time import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# train_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label']) # FIXME this is more elegant...\n",
    "# val_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "def generate_dataloader(tokenized_notes, batch_size, device):\n",
    "    input_ids = torch.tensor(tokenized_notes.input_ids, dtype=torch.long)\n",
    "    attention_mask = torch.tensor(tokenized_notes.attention_mask, dtype=torch.long)\n",
    "\n",
    "    tdataset = TensorDataset(input_ids.to(device),\n",
    "                            attention_mask.to(device),\n",
    "                            )\n",
    "    dataloader = DataLoader(tdataset, batch_size=batch_size, shuffle=False, drop_last=False) \n",
    "    # shuffle is false so that notes retain their order for concat with df\n",
    "    return dataloader\n",
    "\n",
    "batch_size = 6\n",
    "train_dataloader = generate_dataloader(tokenized_train_notes, batch_size, device)\n",
    "test_dataloader = generate_dataloader(tokenized_test_notes, batch_size, device)\n",
    "print(\"train, test, dataloaders generated\")\n",
    "\n",
    "# classifier = torch.load(finetuned_model_path)\n",
    "# classifier.encoder.eval() # makes sure dropout does not occur\n",
    "# classifier.classifier.eval()\n",
    "# print(f\"loaded classifier from {finetuned_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/2 | Time  0.45 seconds\n",
      "Step 1/2 | Time  0.47 seconds\n",
      "Extracted 12 note embeddings. Shape: (12, 64)\n",
      "finished train embedding extraction\n",
      "Step 0/2 | Time  0.02 seconds\n",
      "Step 1/2 | Time  0.03 seconds\n",
      "Extracted 12 note embeddings. Shape: (12, 64)\n",
      "finished test embedding extraction\n"
     ]
    }
   ],
   "source": [
    "classifier.encoder.eval()\n",
    "classifier.classifier.eval()\n",
    "classifier.eval() # here's some redundancy, but just in case...\n",
    "\n",
    "def extract_embeddings(dataloader, classifier):\n",
    "    with(torch.no_grad()):\n",
    "        start_time = time()\n",
    "        embeddings = []\n",
    "        for step, batch in enumerate(dataloader):\n",
    "                input_ids, attention_mask = batch\n",
    "                emb = classifier.forward(input_ids = input_ids, attention_mask = attention_mask, return_embeddings=True)\n",
    "                embeddings.append(emb)\n",
    "                print(f\"Step {step}/{len(dataloader)} | Time {time() - start_time : .2f} seconds\")\n",
    "\n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "    embeddings = embeddings.cpu()\n",
    "    embeddings_df = pd.DataFrame(embeddings.detach().numpy()).add_prefix('emb')\n",
    "    print(f\"Extracted {len(embeddings_df)} note embeddings. Shape: {embeddings_df.shape}\") # should be size 64\n",
    "    return embeddings_df\n",
    "\n",
    "train_embeddings_df = extract_embeddings(train_dataloader, classifier)\n",
    "print(\"finished train embedding extraction\") \n",
    "test_embeddings_df = extract_embeddings(test_dataloader, classifier)\n",
    "print(\"finished test embedding extraction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    # Assuming each batch consists of input_ids, attention_mask, and labels\n",
    "    input_ids, attention_mask = batch\n",
    "\n",
    "    # Print or examine the data in each batch\n",
    "    print('step:', step)\n",
    "    print('Input IDs:', input_ids)\n",
    "    print('Attention Mask:', attention_mask)\n",
    "\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from temivef_train_NOTE_TARGET1_FT_path\n",
      "reading from temivef_test_NOTE_TARGET1_FT_path\n",
      "loaded train notes to extract from /home/ugrads/a/aa_ron_su/JSS_SUBMISSION/data/till_end_mimic_iv_extra_features_train_NOTE_TARGET1_FT_rad.csv\n",
      "loaded test notes to extract from /home/ugrads/a/aa_ron_su/JSS_SUBMISSION/data/till_end_mimic_iv_extra_features_test_NOTE_TARGET1_FT_rad.csv\n",
      "concatenating train and train_embeddings_df with shape (76148, 4) and (12, 64) respectively\n",
      "concatenating test and test_embeddings_df with shape (18470, 4) and (12, 64) respectively\n",
      "reading from temivef_train_NOTE_path\n",
      "reading from temivef_test_NOTE_path\n"
     ]
    }
   ],
   "source": [
    "for mode in [\"train\", \"test\"]:\n",
    "    print(f\"reading from temivef_{mode}_NOTE_TARGET1_FT_path\")\n",
    "train = pd.read_csv(temivef_train_NOTE_TARGET1_FT_path)\n",
    "test = pd.read_csv(temivef_test_NOTE_TARGET1_FT_path)\n",
    "print(f\"loaded train notes to extract from {temivef_train_NOTE_TARGET1_FT_path}\")\n",
    "print(f\"loaded test notes to extract from {temivef_test_NOTE_TARGET1_FT_path}\")\n",
    "\n",
    "#concat notes with ICUSTAY column for merging later\n",
    "train_df_small = pd.concat([train[['ICUSTAY_ID', 'NOTE_ID']], train_embeddings_df], axis = 1)\n",
    "test_df_small = pd.concat([test[['ICUSTAY_ID', 'NOTE_ID']], test_embeddings_df], axis = 1)\n",
    "\n",
    "print(f\"concatenating train and train_embeddings_df with shape {train.shape} and {train_embeddings_df.shape} respectively\")\n",
    "print(f\"concatenating test and test_embeddings_df with shape {test.shape} and {test_embeddings_df.shape} respectively\")\n",
    "\n",
    "\n",
    "for mode in [\"train\", \"test\"]:\n",
    "    print(f\"reading from temivef_{mode}_NOTE_path\")\n",
    "train_df_big = pd.read_csv(temivef_train_NOTE_path)\n",
    "test_df_big = pd.read_csv(temivef_test_NOTE_path)\n",
    "print(f\"loaded train df to merge with data from {temivef_train_NOTE_path}\")\n",
    "print(f\"loaded test notes to merge with data from {temivef_test_NOTE_path}\")\n",
    "\n",
    "\n",
    "def merge_and_fill_embeddings(df_small, df_big):\n",
    "    print(f\"BEFORE merge: len = {len(df_big)}\")\n",
    "    out_df = df_big.merge(df_small, on = ['ICUSTAY_ID','NOTE_ID'], how = 'left')\n",
    "    print(f\"AFTER merge: len = {len(out_df)}\")\n",
    "    def fill_embedding_na(note_id_group):\n",
    "        note_id_group = note_id_group.fillna(method='ffill').fillna(method='bfill')\n",
    "        return note_id_group\n",
    "\n",
    "    emb_cols = ['emb' + str(i) for i in range(64)]\n",
    "\n",
    "    print(f\"BEFORE fill na: len = {len(out_df)}\")\n",
    "    out_df[emb_cols] = out_df.groupby('NOTE_ID')[emb_cols].transform(fill_embedding_na) # transform preserves the shape of the original\n",
    "    print(f\"AFTER fill na: len = {len(out_df)}\")\n",
    "\n",
    "\n",
    "    print(f\"copied {len(df_small)} embeddings into rows with the correct NOTE_ID\")\n",
    "    print(f\"len(out_df): {len(out_df)}\")\n",
    "    print(f\"No. nonnull embedding rows in out_df: {len(out_df[pd.notna(out_df['emb0'])])}\")\n",
    "    return out_df\n",
    "\n",
    "train_out_df = merge_and_fill_embeddings(train_df_small, train_df_big)\n",
    "print(\"merged and filled embeddings for train_out_df\")\n",
    "test_out_df = merge_and_fill_embeddings(test_df_small, test_df_big)\n",
    "print(\"merged and filled embeddings for test_out_df\")\n",
    "\n",
    "\n",
    "# format like mimic_iv_train\n",
    "def format_cols(df):\n",
    "    df.drop(['text', 'NOTE_ID', 't_start_DT','INTIME'], axis=1, inplace=True)\n",
    "    df.rename(columns = {\n",
    "        'SUBJECT_ID':'subject',\n",
    "        'ICUSTAY_ID':'Icustay'\n",
    "        }, inplace = True)\n",
    "    return df\n",
    "\n",
    "train_out_df = format_cols(train_out_df)\n",
    "test_out_df = format_cols(test_out_df)\n",
    "\n",
    "train_out_df.to_csv(train_outpath, index = False)\n",
    "print(\"wrote to\", train_outpath)\n",
    "test_out_df.to_csv(test_outpath, index = False)\n",
    "print(\"wrote to\", test_outpath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clinical1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
