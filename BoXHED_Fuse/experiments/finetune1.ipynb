{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from transformers import LongformerTokenizerFast, LongformerForSequenceClassification, Trainer, TrainingArguments, LongformerConfig, T5Config\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = True\n",
    "run_cntr = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/ugrads/a/aa_ron_su/physionet.org/files/clinical-t5/1.0.0/'\n",
    "data_path = '/data/datasets/mimiciv_notes/physionet.org/files/mimic-iv-note/2.2/note/discharge.csv'\n",
    "temivef_train_NOTE_TARGET1_FT_path = '/home/ugrads/a/aa_ron_su/data/till_end_mimic_iv_extra_features_train_NOTE_TARGET1_FT_rad.csv'\n",
    "\n",
    "model_path = \"yikuan8/Clinical-Longformer\"\n",
    "model_name = 'Clinical-Longformer' # 'Clinical-T5-Base'\n",
    "\n",
    "# model_name = 'Clinical-T5-Base'\n",
    "out_dir = f\"{model_name}_{'test_' if testing else ''}out/{run_cntr}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /home/ugrads/a/aa_ron_su/.cache/huggingface/hub/models--yikuan8--Clinical-Longformer/snapshots/5e2ebe8f6e98d4751eaf71a8514e0edbe73989a2/vocab.json\n",
      "loading file merges.txt from cache at /home/ugrads/a/aa_ron_su/.cache/huggingface/hub/models--yikuan8--Clinical-Longformer/snapshots/5e2ebe8f6e98d4751eaf71a8514e0edbe73989a2/merges.txt\n",
      "loading file tokenizer.json from cache at /home/ugrads/a/aa_ron_su/.cache/huggingface/hub/models--yikuan8--Clinical-Longformer/snapshots/5e2ebe8f6e98d4751eaf71a8514e0edbe73989a2/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/ugrads/a/aa_ron_su/.cache/huggingface/hub/models--yikuan8--Clinical-Longformer/snapshots/5e2ebe8f6e98d4751eaf71a8514e0edbe73989a2/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/ugrads/a/aa_ron_su/.cache/huggingface/hub/models--yikuan8--Clinical-Longformer/snapshots/5e2ebe8f6e98d4751eaf71a8514e0edbe73989a2/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ugrads/a/aa_ron_su/.cache/huggingface/hub/models--yikuan8--Clinical-Longformer/snapshots/5e2ebe8f6e98d4751eaf71a8514e0edbe73989a2/config.json\n",
      "Model config LongformerConfig {\n",
      "  \"_name_or_path\": \"yikuan8/Clinical-Longformer\",\n",
      "  \"architectures\": [\n",
      "    \"LongformerForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_mode\": \"longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"onnx_export\": false,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ugrads/a/aa_ron_su/.cache/huggingface/hub/models--yikuan8--Clinical-Longformer/snapshots/5e2ebe8f6e98d4751eaf71a8514e0edbe73989a2/pytorch_model.bin\n",
      "Some weights of the model checkpoint at yikuan8/Clinical-Longformer were not used when initializing LongformerForSequenceClassification: ['longformer.embeddings.position_ids', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at yikuan8/Clinical-Longformer and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Config, AutoConfig, LongformerTokenizerFast, AutoModelForSequenceClassification, AutoModel, LongformerForSequenceClassification\n",
    "from T5EncoderForSequenceClassification import T5EncoderForSequenceClassification\n",
    "from ClinicalLongformerForSequenceClassification import ClinicalLongformerForSequenceClassification\n",
    "\n",
    "#clinLF:\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, \n",
    "                                                    num_labels = 2, gradient_checkpointing = True)\n",
    "longformer = model.get_submodule('longformer')\n",
    "\n",
    "config_new = longformer.config\n",
    "config_new.num_labels=2\n",
    "config_new.last_hidden_size=64\n",
    "\n",
    "classifier = ClinicalLongformerForSequenceClassification(longformer, config_new)\n",
    "\n",
    "\n",
    "# #T5:\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModel.from_pretrained(model_name)\n",
    "# encoder = model.get_encoder() # we only need the clinical-t5 encoder for our purposes\n",
    "\n",
    "# config_new = encoder.config\n",
    "# config_new.num_labels=2\n",
    "# config_new.last_hidden_size=64\n",
    "\n",
    "# classifier = T5EncoderForSequenceClassification(encoder, config_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label, input_ids, attention_mask = train_data[0:5].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder(    input_ids=input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             # inputs_embeds=inputs_embeds,\n",
    "#             # head_mask=head_mask,\n",
    "#             # output_attentions=output_attentions,\n",
    "#             # output_hidden_states=output_hidden_states,\n",
    "#             # return_dict=return_dict,\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder.config.save_pretrained(f\"{out_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading notes and target from /home/ugrads/a/aa_ron_su/data/till_end_mimic_iv_extra_features_train_NOTE_TARGET1_FT_rad.csv\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(temivef_train_NOTE_TARGET1_FT_path)\n",
    "print(f\"reading notes and target from {temivef_train_NOTE_TARGET1_FT_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_train_test(ID):\n",
    "    ID             = ID.astype(int)\n",
    "    ID_unique_srtd = np.unique(ID)\n",
    "    np.random.shuffle(ID_unique_srtd)    \n",
    "\n",
    "    num_train_ids = int(.80 * len(ID_unique_srtd))\n",
    "    train_ids = ID_unique_srtd[:num_train_ids]\n",
    "    val_ids = ID_unique_srtd[num_train_ids:]\n",
    "\n",
    "    train = ID[ID.isin(train_ids)]\n",
    "    val = ID[ID.isin(val_ids)]\n",
    "\n",
    "    assert(len(train) + len(val) == len(ID))\n",
    "    assert(len(train_ids) + len(val_ids) == len(ID_unique_srtd))\n",
    "    assert(len(train_ids) + len(val_ids) == len(ID_unique_srtd))\n",
    "\n",
    "    return list(train.index), list(val.index)\n",
    "\n",
    "train_idxs, val_idxs = group_train_test(train['ICUSTAY_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7510568f842c48838ff290488872c77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1548ab5bf8d47daa9228c4a039f2caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e4b5cb5f8e4ee7996d5859300ab222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6408d69f22384b3a802d55d3f842ddba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "testing = True\n",
    "\n",
    "target = 'delta_in_2_days'\n",
    "train = train.rename(columns = {target:'label'})\n",
    "\n",
    "train_data = train.iloc[train_idxs]\n",
    "val_data = train.iloc[val_idxs]\n",
    "\n",
    "if testing:\n",
    "    train_data = train_data.iloc[:500]\n",
    "    val_data = val_data.iloc[:500]\n",
    "\n",
    "train_data = Dataset.from_pandas(train_data).select_columns(['text', 'label'])\n",
    "val_data = Dataset.from_pandas(val_data).select_columns(['text', 'label'])\n",
    "\n",
    "if not os.path.exists(f'{out_dir}/data_cache'):\n",
    "    # define a function that will tokenize the model, and will return the relevant inputs for the model\n",
    "    def tokenization(batched_text):\n",
    "        return tokenizer(batched_text['text'], padding = 'max_length', truncation=True, max_length = 512)\n",
    "\n",
    "    train_data = train_data.map(tokenization, batched = True, batch_size = len(train_data) // 10)\n",
    "    val_data = val_data.map(tokenization, batched = True, batch_size = len(val_data) // 10)\n",
    "\n",
    "    train_data.save_to_disk(f'{out_dir}/data_cache/tokenized_train_data')\n",
    "    val_data.save_to_disk(f'{out_dir}/data_cache/tokenized_val_data')\n",
    "\n",
    "else: \n",
    "    print(f'loading train, val from', f'{out_dir}/data_cache/')\n",
    "    train_data = train_data.load_from_disk(f'{out_dir}/data_cache/tokenized_train_data')\n",
    "    val_data = val_data.load_from_disk(f'{out_dir}/data_cache/tokenized_val_data')\n",
    "\n",
    "train_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "train_data = train_data.remove_columns('text')\n",
    "val_data = val_data.remove_columns('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define accuracy metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # argmax(pred.predictions, axis=1)\n",
    "    #pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = f'{out_dir}/results',\n",
    "    num_train_epochs = 1,\n",
    "    per_device_train_batch_size = 3,\n",
    "    gradient_accumulation_steps = 8,    \n",
    "    per_device_eval_batch_size= 6,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate = 2e-5, # FIXME only for Longformer...\n",
    "    save_strategy=\"epoch\",\n",
    "    disable_tqdm = False, \n",
    "    load_best_model_at_end=True,\n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps = 8,\n",
    "    fp16 = True,\n",
    "    logging_dir=f'{out_dir}/logs',\n",
    "    dataloader_num_workers = 0,\n",
    "    run_name = f'{model_name}_{note_type}_run{run_cntr}',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MyTrainer.MyTrainer object at 0x7fe38b9ee320>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the trainer class and check for available devices\n",
    "from MyTrainer import MyTrainer\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    model=classifier,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    # callbacks = [save_model_callback]\n",
    ")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 3\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 20\n",
      "  Number of trainable parameters = 148708802\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maa_ron_su\u001b[0m (\u001b[33maaron_team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ugrads/a/aa_ron_su/physionet.org/files/clinical-t5/1.0.0/wandb/run-20230526_122526-j221m6ii</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aaron_team/huggingface/runs/j221m6ii' target=\"_blank\">lf_radiology_run1</a></strong> to <a href='https://wandb.ai/aaron_team/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aaron_team/huggingface' target=\"_blank\">https://wandb.ai/aaron_team/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aaron_team/huggingface/runs/j221m6ii' target=\"_blank\">https://wandb.ai/aaron_team/huggingface/runs/j221m6ii</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/20 06:26 < 03:13, 0.03 it/s, Epoch 0.62/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/clinical1/lib/python3.10/site-packages/transformers/trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1498\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1499\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1500\u001b[0m )\n\u001b[0;32m-> 1501\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1502\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1503\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1504\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1505\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1506\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/clinical1/lib/python3.10/site-packages/transformers/trainer.py:1749\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1748\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1749\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1751\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1752\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1753\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1754\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1755\u001b[0m ):\n\u001b[1;32m   1756\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/clinical1/lib/python3.10/site-packages/transformers/trainer.py:2518\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2515\u001b[0m     loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n\u001b[1;32m   2517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_grad_scaling:\n\u001b[0;32m-> 2518\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m   2519\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_apex:\n\u001b[1;32m   2520\u001b[0m     \u001b[39mwith\u001b[39;00m amp\u001b[39m.\u001b[39mscale_loss(loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer) \u001b[39mas\u001b[39;00m scaled_loss:\n",
      "File \u001b[0;32m~/miniconda3/envs/clinical1/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/clinical1/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer2 = Trainer(\n",
    "#     model=classifier,\n",
    "#     args=training_args,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     train_dataset=train_data,\n",
    "#     eval_dataset=val_data,\n",
    "#     # callbacks = [save_model_callback]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trainer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer2.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['input_ids'].size(), train_data['label'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# wandb.init()\n",
    "# print(wandb.run.get_url())\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DefaultFlowCallback\n",
    "\n",
    "# class SaveModelCallback(TrainerCallback):\n",
    "#     def __init__(self, output_dir):\n",
    "#         self.output_dir = output_dir\n",
    "\n",
    "#     def on_epoch_end(self, args, state, control, **kwargs):\n",
    "#         model = state.model\n",
    "#         epoch = state.epoch\n",
    "#         ckpt_dir = os.path.join(self.output_dir, f\"epoch_{epoch}\")\n",
    "#         os.makedirs(ckpt_dir, exist_ok=True)\n",
    "#         model.save_pretrained(ckpt_dir)\n",
    "\n",
    "# save_model_callback = SaveModelCallback(output_dir=out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model = trainer._load_best_model() # doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(best_model, f'{out_dir}/best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "\n",
    "# train_inputs = train_tensor\n",
    "# train_labels = torch.tensor(train['delta_in_2_days'].to_numpy())\n",
    "# train_dataset = TensorDataset(train_inputs.to(device), train_labels.to(device)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Subset\n",
    "# batch_size = 4\n",
    "\n",
    "\n",
    "# for fold, (train_idx, val_idx) in enumerate(kfold):\n",
    "#     print(train_idx.shape, val_idx.shape)\n",
    "#     val_set = Subset(train_dataset, val_idx)\n",
    "#     val_dataloader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "#     train_set = Subset(train_dataset, train_idx)\n",
    "#     train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "#     print(len(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes_to_extract = df['text']\n",
    "# texts = notes_to_extract.tolist()\n",
    "# tokenized_notes_to_extract = tokenizer(texts, truncation=True, padding=True, return_tensors = \"pt\")\n",
    "\n",
    "# # test_note_to_extract = notes_to_extract.iloc[0]\n",
    "# # tokenized_test_note = tokenize_function(test_note_to_extract)\n",
    "# # tokenized_test_note.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from T5EncoderForSequenceClassification import T5EncoderForSequenceClassification, T5EncoderClassificationHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.nn.utils import parameters_to_vector\n",
    "\n",
    "# num_params = len(parameters_to_vector(encoder.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(parameters_to_vector(classifier.classifier.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.encoder.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "# from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# # Define your training data\n",
    "# train_inputs = tokenized_notes_to_extract.input_ids\n",
    "# # train_labels = torch.tensor(np.random.rand(len(train_inputs)))\n",
    "# train_labels = torch.tensor(df['delta_in_2_days'].to_numpy())\n",
    "# train_dataset = TensorDataset(train_inputs.to(device), train_labels.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset[0][0].device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "# train_sampler = RandomSampler(train_dataset)\n",
    "# train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 16\n",
    "# num_epochs = 1\n",
    "# learning_rate = 5e-5\n",
    "# adam_epsilon = 1e-8\n",
    "# max_grad_norm = 1.0\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.AdamW(classifier.classifier.parameters(), lr=learning_rate, eps=adam_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_steps = len(train_dataloader) * num_epochs\n",
    "# scheduler = get_linear_schedule_with_warmup(\n",
    "#     optimizer,\n",
    "#     num_warmup_steps=0,\n",
    "#     num_training_steps=total_steps\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # freeze encoder weights:\n",
    "# for param in classifier.encoder.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# classifier.classifier.train()\n",
    "# for epoch in tqdm(range(num_epochs)):\n",
    "#     for step, batch in enumerate(train_dataloader):\n",
    "#         inputs, labels = batch\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = classifier.forward(inputs, labels=labels)\n",
    "#         loss = outputs.loss\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(classifier.classifier.parameters(), max_grad_norm)\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "\n",
    "#         # Print progress every 10 steps\n",
    "#         if step % 10 == 0:\n",
    "#             print(f\"Epoch {epoch + 1}/{num_epochs} | Step {step}/{len(train_dataloader)} | Loss {loss.item():.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT NOTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_embedding = classifier.forward(train_inputs[0:2], labels = train_labels[0:2], return_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = classifier.forward(train_inputs, labels = train_labels, return_embeddings=True)\n",
    "# embeddings_df = pd.DataFrame({'embedding': list(embeddings.detach().numpy())})\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df = pd.concat([df, embeddings_df], axis = 1)\n",
    "# df.to_csv(mimic_iv_train_NOTE_EMBEDDINGS_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_df = pd.DataFrame({'embedding': list(embeddings.detach().numpy())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_small = df.iloc[0:5].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_small = pd.concat([df_small, embeddings_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge embeddings with dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs['classifier_last_hidden_state'][0].shape\n",
    "# outputs['logits'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_with_dense = T5EncoderWithDense(encoder = encoder, num_classes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense_layer_outputs = encoder_with_dense(inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_with_dense.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense_layer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense(pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Freeze the weights of the encoder layers\n",
    "# for param in model_encoder_only.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# # dataset = load_dataset('csv', data_files=mimic_iv_train_NOTE_path, split='train') # split = 'train\n",
    "# df = pd.read_csv(mimic_iv_train_NOTE_path)[['NOTE_ID', 'text']]\n",
    "# df.drop_duplicates(inplace=True)\n",
    "# df.dropna(inplace=True) \n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# inputs = tokenized_test_note\n",
    "# outputs = tokenized_test_note\n",
    "# outputs = model(input_ids = inputs.input_ids, attention_mask = inputs.attention_mask, decoder_input_ids = inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import torch\n",
    "# inputs = tokenized_test_note\n",
    "# labels = torch.tensor([1]).unsqueeze(0)\n",
    "# outputs = model_encoder_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_outputs = []\n",
    "# labels = torch.tensor([1]).unsqueeze(0)\n",
    "\n",
    "# for i, row in notes_to_extract.iterrows():\n",
    "#     input_text = row['text']\n",
    "#     input_ids = tokenizer.encode(input_text, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "#     outputs = model(**inputs, labels=labels)\n",
    "#     generated_outputs.append(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Generate a random target dataset with n = 1000\n",
    "# n = 1000\n",
    "# target_df = np.random.rand(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clinical_t5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
