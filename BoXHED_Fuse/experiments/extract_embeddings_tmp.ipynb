{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "testing = False\n",
    "GPU_NO = 3\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(3)  # use the correct gpu\n",
    "device = torch.device(f'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "\n",
    "root = '/home/ugrads/a/aa_ron_su/physionet.org/files/clinical-t5/1.0.0/'\n",
    "finetuned_model_path = root + '/model/meta_ft_classify.pt' # modify this line!\n",
    "temivef_train_NOTE_TARGET1_FT_path = '/home/ugrads/a/aa_ron_su/data/till_end_mimic_iv_extra_features_train_NOTE_TARGET1_FT.csv'\n",
    "temivef_test_NOTE_TARGET1_FT_path = '/home/ugrads/a/aa_ron_su/data/till_end_mimic_iv_extra_features_test_NOTE_TARGET1_FT.csv'\n",
    "temivef_train_NOTE_path = '/home/ugrads/a/aa_ron_su/data/till_end_mimic_iv_extra_features_train_NOTE.csv'\n",
    "temivef_test_NOTE_path = '/home/ugrads/a/aa_ron_su/data/till_end_mimic_iv_extra_features_test_NOTE.csv'\n",
    "outdir = '/home/ugrads/a/aa_ron_su/data/final2/' # modify this line!\n",
    "train_outpath = os.path.join(outdir, 'till_end_mimic_iv_extra_features_train.csv')\n",
    "test_outpath = os.path.join(outdir, 'till_end_mimic_iv_extra_features_test.csv') \n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "model_name = \"Clinical-T5-Base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Clinical-T5-Base\")\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "tensor_dir = \"./tokenized_notes\"\n",
    "train_tensor_path = os.path.join(tensor_dir, \"train_tensor.pt\")\n",
    "test_tensor_path = os.path.join(tensor_dir, \"test_tensor.pt\")\n",
    "\n",
    "tokenized_train_notes = torch.load(train_tensor_path)\n",
    "tokenized_test_notes = torch.load(test_tensor_path)\n",
    "\n",
    "if testing:\n",
    "    tokenized_train_notes = tokenized_train_notes[:12]\n",
    "    tokenized_test_notes = tokenized_test_notes[:12]\n",
    "    print(f\"testing mode truncated tokenized_notes to length {len(tokenized_train_notes)}\")\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from time import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def generate_dataloader(tokenized_notes, batch_size, device):\n",
    "    inputs = tokenized_notes\n",
    "    labels = torch.tensor([-1] * len(inputs)) # not actually used in this case, since we are not evaluating loss\n",
    "    dataset = TensorDataset(inputs.to(device), labels.to(device))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=False) \n",
    "    # shuffle is false so that notes retain their order for concat with df\n",
    "    return dataloader\n",
    "\n",
    "batch_size = 3\n",
    "train_dataloader = generate_dataloader(tokenized_train_notes, batch_size, device)\n",
    "test_dataloader = generate_dataloader(tokenized_test_notes, batch_size, device)\n",
    "print(\"dataloaders generated\")\n",
    "\n",
    "classifier = torch.load(finetuned_model_path)\n",
    "classifier.encoder.eval() # makes sure dropout does not occur\n",
    "classifier.classifier.eval()\n",
    "print(f\"loaded classifier from {finetuned_model_path}\")\n",
    "\n",
    "def extract_embeddings(dataloader, classifier):\n",
    "    with(torch.no_grad()):\n",
    "        start_time = time()\n",
    "        embeddings = []\n",
    "        for step, batch in enumerate(dataloader):\n",
    "                inputs, _ = batch\n",
    "                emb = classifier.forward(inputs, return_embeddings=True)\n",
    "                embeddings.append(emb)\n",
    "                print(f\"Step {step}/{len(dataloader)} | Time {time() - start_time : .2f} seconds\")\n",
    "\n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "    embeddings = embeddings.cpu()\n",
    "    embeddings_df = pd.DataFrame(embeddings.detach().numpy()).add_prefix('emb')\n",
    "    print(f\"Extracted {len(embeddings_df)} note embeddings. Shape: {embeddings_df.shape}\") # should be size 64\n",
    "    return embeddings_df\n",
    "\n",
    "train_embeddings_df = extract_embeddings(train_dataloader, classifier)\n",
    "print(\"finished train embedding extraction\") \n",
    "test_embeddings_df = extract_embeddings(test_dataloader, classifier)\n",
    "print(\"finished test embedding extraction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from temivef_train_NOTE_TARGET1_FT_path\n",
      "reading from temivef_test_NOTE_TARGET1_FT_path\n",
      "concatenating train and train_embeddings_df with shape (14573, 4) and (14573, 64) respectively\n",
      "concatenating test and test_embeddings_df with shape (2582, 4) and (2582, 64) respectively\n"
     ]
    }
   ],
   "source": [
    "for mode in [\"train\", \"test\"]:\n",
    "    print(f\"reading from temivef_{mode}_NOTE_TARGET1_FT_path\")\n",
    "train = pd.read_csv(temivef_train_NOTE_TARGET1_FT_path)\n",
    "test = pd.read_csv(temivef_test_NOTE_TARGET1_FT_path)\n",
    "\n",
    "#concat notes with ICUSTAY column for merging later\n",
    "train_df_small = pd.concat([train[['ICUSTAY_ID', 'NOTE_ID']], train_embeddings_df], axis = 1)\n",
    "test_df_small = pd.concat([test[['ICUSTAY_ID', 'NOTE_ID']], test_embeddings_df], axis = 1)\n",
    "\n",
    "print(f\"concatenating train and train_embeddings_df with shape {train.shape} and {train_embeddings_df.shape} respectively\")\n",
    "print(f\"concatenating test and test_embeddings_df with shape {test.shape} and {test_embeddings_df.shape} respectively\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from temivef_train_NOTE_path\n",
      "reading from temivef_test_NOTE_path\n"
     ]
    }
   ],
   "source": [
    "for mode in [\"train\", \"test\"]:\n",
    "    print(f\"reading from temivef_{mode}_NOTE_path\")\n",
    "train_df_big = pd.read_csv(temivef_train_NOTE_path)\n",
    "test_df_big = pd.read_csv(temivef_test_NOTE_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE merge: len = 3710672\n",
      "AFTER merge: len = 3710672\n",
      "BEFORE fill na: len = 3710672\n",
      "AFTER fill na: len = 3710672\n",
      "copied 14573 embeddings into rows with the correct NOTE_ID\n",
      "len(out_df): 3710672\n",
      "No. nonnull embedding rows in out_df: 1627917\n",
      "merged and filled embeddings for train_out_df\n",
      "BEFORE merge: len = 1018912\n",
      "AFTER merge: len = 1018912\n",
      "BEFORE fill na: len = 1018912\n",
      "AFTER fill na: len = 1018912\n",
      "copied 2582 embeddings into rows with the correct NOTE_ID\n",
      "len(out_df): 1018912\n",
      "No. nonnull embedding rows in out_df: 413281\n",
      "merged and filled embeddings for test_out_df\n"
     ]
    }
   ],
   "source": [
    "def merge_and_fill_embeddings(df_small, df_big):\n",
    "    print(f\"BEFORE merge: len = {len(df_big)}\")\n",
    "    out_df = df_big.merge(df_small, on = ['ICUSTAY_ID','NOTE_ID'], how = 'left')\n",
    "    print(f\"AFTER merge: len = {len(out_df)}\")\n",
    "    def fill_embedding_na(note_id_group):\n",
    "        note_id_group = note_id_group.fillna(method='ffill').fillna(method='bfill')\n",
    "        return note_id_group\n",
    "\n",
    "    emb_cols = ['emb' + str(i) for i in range(64)]\n",
    "\n",
    "    print(f\"BEFORE fill na: len = {len(out_df)}\")\n",
    "    out_df[emb_cols] = out_df.groupby('NOTE_ID')[emb_cols].transform(fill_embedding_na) # transform preserves the shape of the original\n",
    "    print(f\"AFTER fill na: len = {len(out_df)}\")\n",
    "\n",
    "\n",
    "    print(f\"copied {len(df_small)} embeddings into rows with the correct NOTE_ID\")\n",
    "    print(f\"len(out_df): {len(out_df)}\")\n",
    "    print(f\"No. nonnull embedding rows in out_df: {len(out_df[pd.notna(out_df['emb0'])])}\")\n",
    "    return out_df\n",
    "\n",
    "train_out_df = merge_and_fill_embeddings(train_df_small, train_df_big)\n",
    "print(\"merged and filled embeddings for train_out_df\")\n",
    "test_out_df = merge_and_fill_embeddings(test_df_small, test_df_big)\n",
    "print(\"merged and filled embeddings for test_out_df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format like mimic_iv_train\n",
    "def format_cols(df):\n",
    "    df.drop(['text', 'NOTE_ID', 't_start_DT','INTIME'], axis=1, inplace=True)\n",
    "    df.rename(columns = {\n",
    "        'SUBJECT_ID':'subject',\n",
    "        'ICUSTAY_ID':'Icustay'\n",
    "        }, inplace = True)\n",
    "    return df\n",
    "\n",
    "train_out_df = format_cols(train_out_df)\n",
    "test_out_df = format_cols(test_out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote to /home/ugrads/a/aa_ron_su/data/final2/till_end_mimic_iv_extra_features_train.csv\n",
      "wrote to /home/ugrads/a/aa_ron_su/data/final2/till_end_mimic_iv_extra_features_test.csv\n"
     ]
    }
   ],
   "source": [
    "train_out_df.to_csv(train_outpath, index = False)\n",
    "print(\"wrote to\", train_outpath)\n",
    "test_out_df.to_csv(test_outpath, index = False)\n",
    "print(\"wrote to\", test_outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do I prevent duplicate merges? merge on Icustay AND NOTE_ID?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading from temivef_train_NOTE_TARGET1_FT_path\n",
    "# reading from temivef_test_NOTE_TARGET1_FT_path\n",
    "# concatenating test and test_embeddings_df with shape (2582, 3) and (2580, 64) respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        30000484.0\n",
       "1        30001947.0\n",
       "2        30002521.0\n",
       "3        30003202.0\n",
       "4        30003226.0\n",
       "            ...    \n",
       "14568    39996870.0\n",
       "14569    39998012.0\n",
       "14570    39999230.0\n",
       "14571    39999301.0\n",
       "14572    39999301.0\n",
       "Name: ICUSTAY_ID, Length: 14573, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_small.ICUSTAY_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        30000484.0\n",
       "1        30001947.0\n",
       "2        30002521.0\n",
       "3        30003202.0\n",
       "4        30003226.0\n",
       "            ...    \n",
       "14567    39996783.0\n",
       "14568    39996870.0\n",
       "14569    39998012.0\n",
       "14570    39999230.0\n",
       "14571    39999301.0\n",
       "Name: ICUSTAY_ID, Length: 13632, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_small.ICUSTAY_ID.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "941"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14573 - 13632"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_fill_embeddings(df_small, df_big):\n",
    "    print(f\"BEFORE merge: len = {len(df_big)}\")\n",
    "    out_df = df_big.merge(df_small, on = 'ICUSTAY_ID', how = 'left')\n",
    "\n",
    "    print(f\"AFTER merge: len = {len(out_df)}\")\n",
    "    def fill_embedding_na(note_id_group):\n",
    "        note_id_group = note_id_group.fillna(method='ffill').fillna(method='bfill')\n",
    "        return note_id_group\n",
    "\n",
    "    emb_cols = ['emb' + str(i) for i in range(64)]\n",
    "\n",
    "    print(f\"BEFORE fill na: len = {len(out_df)}\")\n",
    "    out_df[emb_cols] = out_df.groupby('NOTE_ID')[emb_cols].transform(fill_embedding_na) # transform preserves the shape of the original\n",
    "    print(f\"AFTER fill na: len = {len(out_df)}\")\n",
    "\n",
    "\n",
    "    print(f\"copied {len(df_small)} embeddings into rows with the correct NOTE_ID\")\n",
    "    print(f\"len(out_df): {len(out_df)}\")\n",
    "    print(f\"No. nonnull embedding rows in out_df: {len(out_df[pd.notna(out_df['emb0'])])}\")\n",
    "    return out_df\n",
    "\n",
    "train_out_df = merge_and_fill_embeddings(train_df_small, train_df_big)\n",
    "print(\"merged and filled embeddings for train_out_df\")\n",
    "test_out_df = merge_and_fill_embeddings(test_df_small, test_df_big)\n",
    "print(\"merged and filled embeddings for test_out_df\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # format like mimic_iv_train\n",
    "# def format_cols(df):\n",
    "#     df.drop(['text', 'NOTE_ID', 't_start_DT','INTIME'], axis=1, inplace=True)\n",
    "#     df.rename(columns = {\n",
    "#         'SUBJECT_ID':'subject',\n",
    "#         'ICUSTAY_ID':'Icustay'\n",
    "#         }, inplace = True)\n",
    "#     return df\n",
    "\n",
    "# train_out_df = format_cols(train_out_df)\n",
    "# test_out_df = format_cols(test_out_df)\n",
    "\n",
    "# train_out_df.to_csv(train_outpath, index = False)\n",
    "# print(\"wrote to\", train_outpath)\n",
    "# test_out_df.to_csv(test_outpath, index = False)\n",
    "# print(\"wrote to\", test_outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 12 note embeddings of shape (12, 64)\n",
      "concatenating train and embeddings_df w shapes (12, 3), (12, 64)\n",
      "out_df shape: (12, 67)\n",
      "No. nonnull embedding rows: 12\n",
      "Concatenated note embeddings to mimic_iv_train_NOTE\n",
      "len(out_df): 12\n"
     ]
    }
   ],
   "source": [
    "embeddings_df = pd.DataFrame(embeddings.detach().numpy()).add_prefix('emb')\n",
    "\n",
    "# embeddings = classifier.forward(train_inputs, return_embeddings=True)\n",
    "# embeddings_df = pd.DataFrame({'embedding': list(embeddings.detach().numpy())})\n",
    "print(f\"Extracted {len(embeddings_df)} note embeddings of shape {embeddings_df.shape}\") # should be size 64\n",
    "\n",
    "# # df.reset_index(drop=True, inplace=True)\n",
    "# #concat notes with the train df\n",
    "print(f\"concatenating train and embeddings_df w shapes {train.shape}, {embeddings_df.shape}\")\n",
    "out_df_small = pd.concat([train, embeddings_df], axis = 1)\n",
    "print(f\"out_df shape: {out_df_small.shape}\")\n",
    "print(f\"No. nonnull embedding rows: {len(out_df_small[pd.notna(out_df_small['emb1'])])}\") # should be 13\n",
    "print(f\"Concatenated note embeddings to mimic_iv_train_NOTE\")\n",
    "print(f\"len(out_df): {len(out_df_small)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_to_merge = pd.read_csv(temivef_train_NOTE_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = out_df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = train_df_to_merge.merge(out_df_small.drop(['text', 'delta_in_2_days'], axis = 1), on = 'ICUSTAY_ID', how = 'left')\n",
    "# out_df = train_df_to_merge.merge(out_df_small, on = 'ICUSTAY_ID', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_embedding_na(note_id_group):\n",
    "    note_id_group = note_id_group.fillna(method='ffill').fillna(method='bfill')\n",
    "    return note_id_group\n",
    "\n",
    "emb_cols = ['emb' + str(i) for i in range(64)]\n",
    "\n",
    "out_df[emb_cols] = out_df.groupby('NOTE_ID')[emb_cols].transform(fill_embedding_na) # transform preserves the shape of the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copied 12 notes into rows with the correct NOTE_ID\n",
      "len(out_df): 3710957\n",
      "No. nonnull embedding rows: 2951\n"
     ]
    }
   ],
   "source": [
    "print(f\"copied {len(out_df_small)} notes into rows with the correct NOTE_ID\")\n",
    "print(f\"len(out_df): {len(out_df)}\")\n",
    "print(f\"No. nonnull embedding rows: {len(out_df[pd.notna(out_df['emb0'])])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpt = out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.drop(['text', 'NOTE_ID', 't_start_DT','INTIME'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.rename(columns = {\n",
    "    'SUBJECT_ID':'subject',\n",
    "    'ICUSTAY_ID':'Icustay'\n",
    "    }, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_to_merge[train_df_to_merge.ICUSTAY_ID.isin(out_df_small.ICUSTAY_ID)].NOTE_ID.unique()\n",
    "# out_df[pd.notna(out_df.emb0)].NOTE_ID.unique()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clinical_t5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
