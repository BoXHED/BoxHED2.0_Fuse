{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, T5Config, AutoModel, AutoConfig\n",
    "import torch\n",
    "from T5EncoderForSequenceClassification import T5EncoderForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, LongformerForSequenceClassification\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"jpwahle/longformer-base-plagiarism-detection\")\n",
    "# model = LongformerForSequenceClassification.from_pretrained(\"jpwahle/longformer-base-plagiarism-detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_dir = \"./Clinical-T5-Base_out/2/results/checkpoint-3810\"\n",
    "model_path = \"./Clinical-T5-Base_out/2/results/model_checkpoint__epoch3.pt\"\n",
    "#load model\n",
    "model = torch.load(model_path)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './tmpout' #f'{out_dir}/results',\n",
    "    num_train_epochs = num_epochs,\n",
    "    per_device_train_batch_size = 5,\n",
    "    gradient_accumulation_steps = 3,    \n",
    "    per_device_eval_batch_size= 10,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    disable_tqdm = False, \n",
    "    load_best_model_at_end=True,\n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps = 15, # make it 15\n",
    "    fp16 = True,\n",
    "    logging_dir=f'{out_dir}/logs',\n",
    "    dataloader_num_workers = 0,\n",
    "    run_name = 'examplerun' #f'{model_name}_{note_type}_run{run_cntr}',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"./Clinical-T5-Base_test_out/4/results/checkpoint-15\"\n",
    "out_dir = \"./Clinical-T5-Base_test_out/4\"\n",
    "model = torch.load('model_checkpoint_epoch_0.9523809523809523.pt')\n",
    "\n",
    "checkpoint_dir2 = \"./Clinical-Longformer_test_out/5/results/checkpoint-31\"\n",
    "out_dir2 = \"./Clinical-Longformer_test_out/5/results\"\n",
    "model2 = torch.load(os.path.join(out_dir2, 'model_checkpoint__epoch1.pt'))\n",
    "\n",
    "# trainer = Trainer.from_pretrained()\n",
    "\n",
    "# clin_t5_config = AutoConfig.from_pretrained(checkpoint_dir)\n",
    "\n",
    "# # Load the model's state dictionary\n",
    "# model_state_dict = torch.load(os.path.join(checkpoint_dir, \"pytorch_model.bin\"))\n",
    "# config = AutoConfig.from_pretrained(config_dir)\n",
    "# classifier = AutoModel.from_pretrained(checkpoint_dir, config=config)\n",
    "# Instantiate the model with the appropriate architecture\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained('Clinical-T5-Base')\n",
    "# encoder = model.get_encoder() # we only need the clinical-t5 encoder for our purposes\n",
    "\n",
    "# config = T5Config(\n",
    "#     hidden_size=768,\n",
    "#     classifier_dropout=None,\n",
    "#     num_labels=2,\n",
    "#     hidden_dropout_prob=0.01,\n",
    "#     last_hidden_size=64,\n",
    "#     gradient_checkpointing=True,\n",
    "# )\n",
    "# classifier = T5EncoderForSequenceClassification(encoder, config)\n",
    "\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "# classifier.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "num_params2 = sum(p.numel() for p in model2.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.348728712049025"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(num_params2 - num_params)/num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Config {\n",
       "  \"_name_or_path\": \"Clinical-T5-Base\",\n",
       "  \"architectures\": [\n",
       "    \"T5ForConditionalGeneration\"\n",
       "  ],\n",
       "  \"d_ff\": 3072,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_start_token_id\": 0,\n",
       "  \"dense_act_fn\": \"relu\",\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"relu\",\n",
       "  \"gradient_checkpointing\": true,\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": false,\n",
       "  \"is_gated_act\": false,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"model_type\": \"t5\",\n",
       "  \"n_positions\": 512,\n",
       "  \"num_decoder_layers\": 12,\n",
       "  \"num_heads\": 12,\n",
       "  \"num_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"problem_type\": \"single_label_classification\",\n",
       "  \"relative_attention_max_distance\": 128,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"task_specific_params\": {\n",
       "    \"summarization\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"length_penalty\": 2.0,\n",
       "      \"max_length\": 200,\n",
       "      \"min_length\": 30,\n",
       "      \"no_repeat_ngram_size\": 3,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"summarize: \"\n",
       "    },\n",
       "    \"translation_en_to_de\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to German: \"\n",
       "    },\n",
       "    \"translation_en_to_fr\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to French: \"\n",
       "    },\n",
       "    \"translation_en_to_ro\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to Romanian: \"\n",
       "    }\n",
       "  },\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.24.0\",\n",
       "  \"use_cache\": false,\n",
       "  \"vocab_size\": 32115\n",
       "}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LongformerConfig {\n",
       "  \"_name_or_path\": \"yikuan8/Clinical-Longformer\",\n",
       "  \"architectures\": [\n",
       "    \"LongformerForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_mode\": \"longformer\",\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"attention_window\": [\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512,\n",
       "    512\n",
       "  ],\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"ignore_attention_mask\": false,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"last_hidden_size\": 64,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 4098,\n",
       "  \"model_type\": \"longformer\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"onnx_export\": false,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"problem_type\": \"single_label_classification\",\n",
       "  \"sep_token_id\": 2,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.24.0\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5EncoderForSequenceClassification.T5EncoderForSequenceClassification"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define accuracy metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    print(f\"labels: {pred.label_ids}, pred: {pred.predictions}\")\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # argmax(pred.predictions, axis=1)\n",
    "    #pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_data = Dataset.load_from_disk(f'{out_dir}/data_cache/tokenized_train_data')\n",
    "val_data = Dataset.load_from_disk(f'{out_dir}/data_cache/tokenized_val_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, TrainerState\n",
    "\n",
    "# Load the model and optimizer states\n",
    "# model = torch.load(\"pytorch_model.bin\")\n",
    "optimizer = torch.load(f\"{checkpoint_dir}/optimizer.pt\")\n",
    "\n",
    "# Load the other state files\n",
    "rng_state = torch.load(f\"{checkpoint_dir}/rng_state.pth\")\n",
    "scaler = torch.load(f\"{checkpoint_dir}/scaler.pt\")\n",
    "scheduler = torch.load(f\"{checkpoint_dir}/scheduler.pt\")\n",
    "trainer_state = TrainerState.load_from_json(f\"{checkpoint_dir}/trainer_state.json\")\n",
    "training_args = torch.load(f\"{checkpoint_dir}/training_args.bin\")\n",
    "\n",
    "# Create a new Trainer object with the loaded states\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset= train_data,  # Replace with your train dataset\n",
    "    eval_dataset= val_data,\n",
    ")\n",
    "# trainer = MyTrainer(\n",
    "#     model=classifier,\n",
    "#     args=training_args,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     train_dataset=train_data,\n",
    "#     eval_dataset=val_data,\n",
    "#     # callbacks = [save_model_callback]\n",
    "# )\n",
    "\n",
    "# Continue using the trainer object for further training or evaluation\n",
    "# Make sure you have the necessary dependencies installed, especially the transformers package if you're using the Hugging Face library.\n",
    "\n",
    "# Note that you may need to adjust the code according to your specific use case, such as replacing the train_dataset and data_collator with your actual training dataset and data collator functions.\n",
    "\n",
    "# Additionally, please note that loading a trainer from a saved state doesn't guarantee the exact same training state as it was during the previous training. It is recommended to use the same code and training configuration that was used during the initial training for accurate results.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `T5EncoderForSequenceClassification.forward` and have been ignored: text. If text are not expected by `T5EncoderForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "/home/ugrads/a/aa_ron_su/miniconda3/envs/clinical1/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: [0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0\n",
      " 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0\n",
      " 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0], pred: [[      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [-0.0184    0.057   ]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [-0.000632  0.04315 ]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]\n",
      " [      nan       nan]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maa_ron_su\u001b[0m (\u001b[33maaron_team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ugrads/a/aa_ron_su/physionet.org/files/clinical-t5/1.0.0/wandb/run-20230524_223319-x2g0802k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aaron_team/huggingface/runs/x2g0802k' target=\"_blank\">t5_radiology_run1</a></strong> to <a href='https://wandb.ai/aaron_team/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aaron_team/huggingface' target=\"_blank\">https://wandb.ai/aaron_team/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aaron_team/huggingface/runs/x2g0802k' target=\"_blank\">https://wandb.ai/aaron_team/huggingface/runs/x2g0802k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': nan,\n",
       " 'eval_accuracy': 0.824,\n",
       " 'eval_f1': 0.022222222222222223,\n",
       " 'eval_precision': 0.5,\n",
       " 'eval_recall': 0.011363636363636364,\n",
       " 'eval_runtime': 22.9244,\n",
       " 'eval_samples_per_second': 21.811,\n",
       " 'eval_steps_per_second': 2.748}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=4,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_steps=None,\n",
       "evaluation_strategy=epoch,\n",
       "fp16=True,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=8,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=False,\n",
       "group_by_length=False,\n",
       "half_precision_backend=cuda_amp,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=True,\n",
       "local_rank=-1,\n",
       "log_level=passive,\n",
       "log_level_replica=passive,\n",
       "log_on_each_node=True,\n",
       "logging_dir=Clinical-T5-Base_test_out/4/logs,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=8,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_type=linear,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=loss,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=1,\n",
       "optim=adamw_hf,\n",
       "output_dir=Clinical-T5-Base_test_out/4/results,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=2,\n",
       "per_device_train_batch_size=1,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=['wandb'],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=t5_radiology_run1,\n",
       "save_on_each_node=False,\n",
       "save_steps=500,\n",
       "save_strategy=epoch,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=200,\n",
       "weight_decay=0.01,\n",
       "xpu_backend=None,\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5EncoderForSequenceClassification(\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32115, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (classifier): T5EncoderClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (last_dense): Linear(in_features=768, out_features=64, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('encoder.embed_tokens.weight',\n",
       "              tensor([[  3.5675,  -3.3486,  -8.2628,  ...,  -8.7707,  -4.3911,   9.9809],\n",
       "                      [ 78.7873, -35.5742,  34.9051,  ...,   6.7170,  59.8352, -41.4054],\n",
       "                      [-62.4314, -36.1461, -69.0395,  ..., -43.4215,  71.9762, 104.9035],\n",
       "                      ...,\n",
       "                      [-61.8046, -48.3203,  63.5298,  ...,  25.3621,  80.7012, -31.0647],\n",
       "                      [ 35.2618, -40.6307,  64.0298,  ...,   4.9481,  50.1437,  -2.8818],\n",
       "                      [ 22.5664,  23.6029,   2.1540,  ...,  -4.6252,  25.9078,  32.2544]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.0.layer.0.SelfAttention.q.weight',\n",
       "              tensor([[ 0.1184, -0.0894, -0.0152,  ...,  0.0307, -0.0746, -0.0531],\n",
       "                      [ 0.0816, -0.0600, -0.0104,  ..., -0.0172,  0.0025, -0.0327],\n",
       "                      [-0.0027,  0.0133, -0.0160,  ..., -0.0134,  0.0376, -0.1138],\n",
       "                      ...,\n",
       "                      [-0.0449, -0.0250,  0.0240,  ...,  0.0109,  0.0790, -0.0159],\n",
       "                      [-0.0165, -0.0119, -0.0460,  ...,  0.0756, -0.0609, -0.0684],\n",
       "                      [-0.0688,  0.0740, -0.0182,  ..., -0.0805,  0.0855,  0.0095]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.0.layer.0.SelfAttention.k.weight',\n",
       "              tensor([[ 0.6525, -0.3780, -0.1383,  ...,  0.6094, -0.6481, -0.0117],\n",
       "                      [ 0.9002, -0.2680, -0.0288,  ..., -0.4818,  0.3815, -0.4772],\n",
       "                      [-0.1286, -0.0049,  0.0329,  ...,  0.0417,  0.7717, -0.9533],\n",
       "                      ...,\n",
       "                      [ 0.9687,  0.0555,  0.2883,  ...,  0.3569, -0.4366,  0.6170],\n",
       "                      [-0.6827,  0.4813,  0.6300,  ..., -0.7419,  0.4471, -0.0751],\n",
       "                      [-0.5516,  0.5082,  0.4251,  ...,  0.1188, -0.0664,  0.0075]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.0.layer.0.SelfAttention.v.weight',\n",
       "              tensor([[-0.6652,  1.0451, -0.3168,  ...,  1.2129, -0.2908, -0.1614],\n",
       "                      [ 0.5344,  0.2544, -0.2917,  ..., -0.7421,  0.2677,  0.1315],\n",
       "                      [ 0.8971, -1.5326,  0.1859,  ...,  0.5312,  0.7891,  0.6787],\n",
       "                      ...,\n",
       "                      [-0.2889, -0.0221, -0.2799,  ..., -0.3513,  0.1080, -0.0497],\n",
       "                      [-0.4069,  0.4729,  0.1562,  ...,  0.1442,  0.8312, -0.3408],\n",
       "                      [-0.5650, -0.2963, -0.0985,  ..., -0.7134, -0.5905,  0.4258]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.0.layer.0.SelfAttention.o.weight',\n",
       "              tensor([[ 0.8140, -1.0246, -0.0379,  ...,  0.3980,  1.1175, -0.5885],\n",
       "                      [-1.2435, -0.2423,  1.3507,  ...,  0.3944, -0.2551, -0.7956],\n",
       "                      [ 0.7142, -0.1119,  0.1029,  ...,  0.2467,  1.2522,  0.4841],\n",
       "                      ...,\n",
       "                      [-0.7650,  1.0264, -0.6021,  ..., -0.4904, -0.4936,  0.1944],\n",
       "                      [-0.0255, -0.6821, -0.7088,  ...,  0.4819,  0.0245, -0.8116],\n",
       "                      [ 0.3367, -0.2493, -1.3099,  ..., -0.3333, -0.1239,  0.4970]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight',\n",
       "              tensor([[ 2.5254e+00, -3.4629e+00,  4.2406e-01, -5.0689e-01, -1.0154e+01,\n",
       "                        4.1836e+00, -2.5533e+01, -9.7332e+00, -1.8710e-01, -8.7446e-01,\n",
       "                       -8.6039e-01, -2.6487e+01],\n",
       "                      [-1.2790e+01,  7.1287e+00,  4.3685e+00,  1.8022e+00,  3.4063e+00,\n",
       "                        2.2266e+00, -8.7948e+00,  3.1166e+00,  6.1425e+00, -3.6668e+01,\n",
       "                       -1.6461e+01, -3.0109e+00],\n",
       "                      [-3.3990e+00,  3.9839e+00,  3.7315e+00,  2.9166e+00,  3.7550e+00,\n",
       "                        7.9639e-01, -2.4261e+00,  3.7182e+00,  5.4454e+00, -2.2648e+01,\n",
       "                       -1.6030e+01, -8.4733e-01],\n",
       "                      [-2.9151e+00,  2.4845e+00,  3.4236e+00,  2.9936e+00,  3.7078e+00,\n",
       "                        6.4165e-01, -3.1170e-02,  3.9099e+00,  4.6963e+00, -2.7093e+01,\n",
       "                       -1.6745e+01,  1.2999e-01],\n",
       "                      [-2.5295e+00,  1.0587e+00,  2.8181e+00,  2.7013e+00,  3.6440e+00,\n",
       "                        2.0392e-01,  5.5144e-01,  3.8010e+00,  4.0741e+00, -1.8291e+01,\n",
       "                       -1.6033e+01,  6.0908e-01],\n",
       "                      [-1.9513e+00,  9.4090e-02,  2.1358e+00,  2.9519e+00,  3.5563e+00,\n",
       "                       -1.6241e-01,  1.2075e+00,  3.6009e+00,  3.3512e+00, -1.9660e+01,\n",
       "                       -1.5642e+01,  8.2391e-01],\n",
       "                      [-2.6046e+00, -6.9453e-01,  1.6858e+00,  2.4923e+00,  3.4160e+00,\n",
       "                        9.0116e-02,  1.3185e+00,  3.6858e+00,  3.2201e+00, -1.9906e+01,\n",
       "                       -1.5516e+01,  1.1524e+00],\n",
       "                      [-1.6476e+00, -1.2561e+00,  1.0788e+00,  2.6524e+00,  3.3730e+00,\n",
       "                       -1.4408e-01,  1.5140e+00,  3.5139e+00,  2.6579e+00, -1.7613e+01,\n",
       "                       -1.4831e+01,  1.5817e+00],\n",
       "                      [-1.0105e+00, -1.8121e+00, -1.0245e+00,  2.4220e+00,  3.2183e+00,\n",
       "                       -3.8390e-01,  1.6114e+00,  3.1958e+00,  1.8415e+00, -2.2918e+01,\n",
       "                       -1.5138e+01,  1.1415e+00],\n",
       "                      [-1.2134e+00, -2.5340e+00, -3.5449e+00,  2.0871e+00,  2.7739e+00,\n",
       "                        4.8366e-02,  1.9643e+00,  2.8057e+00,  5.0177e-01, -2.1185e+01,\n",
       "                       -1.2978e+01,  1.6319e+00],\n",
       "                      [-1.2177e+00, -2.2853e+00, -6.2620e+00,  1.7398e+00,  2.2478e+00,\n",
       "                       -2.9921e-01,  2.1333e+00,  2.3274e+00, -3.5057e-01, -2.1256e+01,\n",
       "                       -1.1170e+01,  1.7714e+00],\n",
       "                      [-9.9284e-01, -1.8190e+00, -8.3137e+00,  1.3778e+00,  1.7812e+00,\n",
       "                       -2.1596e-01,  1.9586e+00,  1.7741e+00, -1.9020e+00, -2.0754e+01,\n",
       "                       -5.0801e+00,  1.7679e+00],\n",
       "                      [-8.4947e-01, -2.2921e+00, -1.1106e+01,  8.1071e-01,  1.3941e+00,\n",
       "                       -2.5801e-01,  1.9910e+00,  1.0967e+00, -3.6968e+00, -2.1856e+01,\n",
       "                       -2.8659e+00,  1.5943e+00],\n",
       "                      [-5.0694e-01, -1.6706e+00, -1.2287e+01,  3.8606e-01,  7.8255e-01,\n",
       "                       -5.4304e-01,  1.9478e+00,  3.9245e-01, -5.3011e+00, -2.4647e+01,\n",
       "                       -1.9316e+00,  1.5498e+00],\n",
       "                      [-8.3609e-01, -1.7741e+00, -1.3713e+01, -9.4748e-02,  3.8314e-01,\n",
       "                       -5.9400e-01,  1.9182e+00, -2.5149e-01, -6.7391e+00, -2.4074e+01,\n",
       "                       -2.8634e+00,  1.6004e+00],\n",
       "                      [-5.7979e-01, -2.1086e+00, -3.3929e+01, -8.5943e+00, -1.3902e+00,\n",
       "                       -1.0380e+00,  1.8043e+00, -1.7051e+00, -1.0334e+01, -2.7173e+01,\n",
       "                       -1.6918e+00,  1.4561e+00],\n",
       "                      [ 3.5742e-01, -2.5977e-01,  3.5352e-01,  9.7656e-02, -3.6328e-01,\n",
       "                        2.5195e-01,  2.6367e-01, -2.6562e-01, -1.2695e-01, -1.2109e-01,\n",
       "                       -3.5742e-01, -5.2734e-02],\n",
       "                      [ 7.4185e-01, -2.3550e+00,  6.0262e+00, -6.9809e-01,  3.7050e+00,\n",
       "                        4.5255e+00, -7.4556e+00, -1.2341e+01, -3.6614e+00,  5.8319e+00,\n",
       "                        7.7252e+00, -3.2491e+00],\n",
       "                      [ 1.8508e+00, -2.3497e+00,  5.5235e+00,  6.2371e-01,  3.9665e+00,\n",
       "                        2.2036e+00, -1.4017e+00, -7.5049e+00, -3.8138e+00,  5.8824e+00,\n",
       "                        5.5017e+00, -7.1525e-01],\n",
       "                      [ 1.9877e+00, -2.1694e+00,  5.2942e+00,  8.4029e-01,  3.8620e+00,\n",
       "                        1.1905e+00,  6.3492e-02, -7.1482e+00, -3.9999e+00,  5.5338e+00,\n",
       "                        4.3285e+00, -2.4964e-01],\n",
       "                      [ 2.1141e+00, -2.1862e+00,  4.9898e+00,  9.3992e-01,  3.7201e+00,\n",
       "                        1.3779e-01,  1.0516e+00, -7.9787e+00, -4.1773e+00,  5.1568e+00,\n",
       "                        3.5180e+00,  2.9248e-01],\n",
       "                      [ 2.1071e+00, -2.1959e+00,  4.4593e+00,  1.1732e+00,  3.5347e+00,\n",
       "                       -3.6630e-02,  1.1348e+00, -7.1578e+00, -3.5064e+00,  4.7249e+00,\n",
       "                        2.6128e+00,  3.1219e-01],\n",
       "                      [ 2.1424e+00, -1.9496e+00,  4.3906e+00,  9.8050e-01,  3.4375e+00,\n",
       "                       -4.0873e-01,  1.6449e+00, -7.5349e+00, -3.9044e+00,  4.5448e+00,\n",
       "                        2.2691e+00,  9.1526e-01],\n",
       "                      [ 1.9008e+00, -2.1820e+00,  4.3160e+00,  9.4367e-01,  3.2629e+00,\n",
       "                       -6.8061e-01,  1.4163e+00, -6.9730e+00, -4.2036e+00,  4.2303e+00,\n",
       "                        1.9206e+00,  9.2253e-01],\n",
       "                      [ 2.0069e+00, -2.2750e+00,  4.0944e+00,  9.5146e-01,  3.3317e+00,\n",
       "                       -8.1066e-01,  2.1435e+00, -6.5869e+00, -4.0114e+00,  3.6766e+00,\n",
       "                        1.1342e+00,  1.2121e+00],\n",
       "                      [ 1.8140e+00, -2.1143e+00,  3.5924e+00,  9.1633e-01,  3.0054e+00,\n",
       "                       -1.1679e+00,  2.1753e+00, -6.2008e+00, -3.8827e+00,  2.7356e+00,\n",
       "                       -1.7090e-01,  1.2471e+00],\n",
       "                      [ 1.7879e+00, -2.2365e+00,  3.2370e+00,  2.3985e-01,  2.3962e+00,\n",
       "                       -1.0319e+00,  2.3705e+00, -6.9093e+00, -3.8072e+00,  1.9993e+00,\n",
       "                       -8.9185e-01,  1.4703e+00],\n",
       "                      [ 1.5066e+00, -1.8049e+00,  2.5839e+00,  1.2561e-01,  2.2540e+00,\n",
       "                       -8.9751e-01,  2.5163e+00, -6.5783e+00, -3.7788e+00,  1.2672e+00,\n",
       "                       -1.1731e+00,  1.4664e+00],\n",
       "                      [ 1.4537e+00, -2.3004e+00,  2.0725e+00, -1.7385e-01,  1.5450e+00,\n",
       "                       -1.5085e+00,  2.4006e+00, -6.2372e+00, -3.7400e+00,  3.3822e-01,\n",
       "                       -1.3762e+00,  1.3167e+00],\n",
       "                      [ 1.0745e+00, -1.8406e+00,  1.4917e+00, -7.3954e-01,  1.1410e+00,\n",
       "                       -1.2404e+00,  2.4170e+00, -5.8408e+00, -3.4542e+00, -3.5645e-01,\n",
       "                       -1.1606e+00,  1.5674e+00],\n",
       "                      [ 9.3780e-01, -1.8793e+00,  1.1095e+00, -1.0402e+00,  6.2464e-01,\n",
       "                       -1.0285e+00,  2.4627e+00, -5.9955e+00, -3.4845e+00, -1.0355e+00,\n",
       "                       -1.6621e+00,  1.4430e+00],\n",
       "                      [ 3.1748e-01, -1.9337e+00, -3.0237e-02, -2.1588e+00, -1.2568e+00,\n",
       "                       -8.7523e-01,  2.1239e+00, -5.3259e+00, -3.4326e+00, -1.9148e+00,\n",
       "                       -1.5339e+00,  1.3337e+00]], device='cuda:0')),\n",
       "             ('encoder.block.0.layer.0.layer_norm.weight',\n",
       "              tensor([ 0.0637,  0.0490,  0.0422,  0.0140,  0.0626,  0.0507,  0.0878,  0.0655,\n",
       "                       0.0644,  0.0630,  0.0426,  0.0754,  0.0563,  0.0611,  0.0676,  0.0581,\n",
       "                       0.0691,  0.0387,  0.0595,  0.0599,  0.0665,  0.0548,  0.0653,  0.0519,\n",
       "                       0.0598,  0.0566,  0.0636,  0.0556,  0.0709,  0.0560,  0.0627,  0.0483,\n",
       "                       0.0616,  0.0616,  0.0577,  0.0591,  0.0813,  0.0379,  0.0580,  0.0602,\n",
       "                       0.0541,  0.0740,  0.0485,  0.0789,  0.0623,  0.0560,  0.0674,  0.0524,\n",
       "                       0.0609,  0.0676,  0.0596,  0.0709,  0.0569,  0.0557,  0.0388,  0.0580,\n",
       "                       0.0681,  0.0517,  0.0619,  0.0649,  0.0600,  0.0764,  0.0688,  0.0670,\n",
       "                       0.0747,  0.0474,  0.0678,  0.0621,  0.0548,  0.0488,  0.0599,  0.0808,\n",
       "                       0.0667,  0.0606,  0.0584,  0.0737,  0.0479,  0.0622,  0.0573,  0.0615,\n",
       "                       0.0717,  0.0531,  0.0488,  0.0642,  0.0539,  0.0608,  0.0443,  0.0557,\n",
       "                       0.0692,  0.0576,  0.0477,  0.0525,  0.0534,  0.0574,  0.0717,  0.0776,\n",
       "                       0.0751,  0.0594,  0.0507,  0.0476,  0.0621,  0.0568,  0.0561,  0.0609,\n",
       "                       0.0544,  0.0666,  0.0562,  0.0608,  0.0630,  0.0739,  0.0640,  0.0513,\n",
       "                       0.0558,  0.0631,  0.0570,  0.0574,  0.0521,  0.0449,  0.1235,  0.0598,\n",
       "                       0.0591,  0.0627,  0.0659,  0.0626,  0.0768,  0.0538,  0.0700,  0.0543,\n",
       "                       0.0539,  0.0521,  0.0699,  0.0510,  0.0688,  0.0617,  0.0415,  0.0663,\n",
       "                       0.0511,  0.0382,  0.0629,  0.0416,  0.0329,  0.0638,  0.0553,  0.0765,\n",
       "                       0.0511,  0.0513,  0.0541,  0.0530,  0.0624,  0.0559,  0.0704,  0.0458,\n",
       "                       0.0456,  0.0606,  0.0709,  0.0672,  0.0580,  0.0643,  0.0549,  0.0521,\n",
       "                       0.0601,  0.0556,  0.0704,  0.0840,  0.0404,  0.0336,  0.0657,  0.0475,\n",
       "                       0.0298,  0.0686,  0.0535,  0.0664,  0.0692,  0.0713,  0.0737,  0.0702,\n",
       "                       0.0511,  0.0676,  0.0550,  0.0718,  0.0697,  0.0534,  0.0603,  0.0688,\n",
       "                       0.0539,  0.0453,  0.0668,  0.0513,  0.0548,  0.0500,  0.0486,  0.0696,\n",
       "                       0.0648,  0.0651,  0.0487,  0.0685,  0.0484,  0.0640,  0.0520,  0.0546,\n",
       "                       0.0685,  0.0572,  0.0699,  0.0595,  0.0623, -0.0626,  0.0523,  0.0592,\n",
       "                       0.0550,  0.0653,  0.0477,  0.0513,  0.0546,  0.0726,  0.0605,  0.0630,\n",
       "                       0.0554,  0.0549,  0.0580,  0.0550,  0.0651,  0.0499,  0.0458,  0.0555,\n",
       "                       0.0535,  0.0557,  0.0129,  0.0763,  0.0567,  0.0552,  0.0566,  0.0651,\n",
       "                       0.0663,  0.0719,  0.0640,  0.0541,  0.0451,  0.0591,  0.0311,  0.0669,\n",
       "                       0.0745,  0.0645,  0.0494,  0.0632,  0.0525,  0.0570,  0.0608,  0.0656,\n",
       "                       0.0507,  0.0373,  0.0706,  0.0618,  0.0659,  0.0588,  0.0605,  0.0458,\n",
       "                       0.0676,  0.0207,  0.0769,  0.0599,  0.0493,  0.0406,  0.0576,  0.0656,\n",
       "                       0.0507,  0.0505,  0.0490,  0.0468,  0.0476,  0.0627,  0.0545,  0.0327,\n",
       "                       0.0756,  0.0777,  0.0643,  0.0558,  0.0701,  0.0656,  0.0697,  0.0476,\n",
       "                      -0.0773,  0.0597,  0.0723,  0.0631,  0.0642,  0.0819,  0.0657,  0.0618,\n",
       "                       0.0650,  0.0678,  0.0489,  0.0548,  0.0645,  0.0607,  0.0779,  0.0559,\n",
       "                       0.0307,  0.0690,  0.0592,  0.0464,  0.0746,  0.0709,  0.0640,  0.0653,\n",
       "                       0.0682,  0.0617,  0.0505,  0.0525,  0.0546,  0.0509,  0.0538,  0.0493,\n",
       "                       0.0510,  0.0589,  0.0390,  0.0540,  0.0766,  0.0647,  0.0664,  0.0512,\n",
       "                       0.0563,  0.0796,  0.0562,  0.0558,  0.0479,  0.0457,  0.0564,  0.0584,\n",
       "                       0.0615,  0.0576,  0.0719,  0.0360,  0.0714,  0.0425,  0.0628,  0.0542,\n",
       "                       0.0618,  0.0656,  0.0547,  0.0792,  0.0593,  0.0595,  0.0705,  0.0547,\n",
       "                       0.0746,  0.0540,  0.0651,  0.0565,  0.0628,  0.0556,  0.0557,  0.0396,\n",
       "                       0.0599,  0.0503,  0.0473,  0.0696,  0.0552,  0.0839,  0.0773,  0.0626,\n",
       "                       0.0703,  0.0654,  0.0396,  0.0450,  0.0665,  0.0615,  0.0609,  0.0535,\n",
       "                       0.0613,  0.0415,  0.0766,  0.0645,  0.0807,  0.0591,  0.0651,  0.0644,\n",
       "                       0.0598,  0.0448,  0.0564,  0.0586,  0.0471,  0.0563,  0.0641,  0.0641,\n",
       "                       0.0658,  0.0810,  0.0556,  0.0695,  0.0628,  0.0624,  0.0624,  0.0613,\n",
       "                       0.0534,  0.0655,  0.0641,  0.0634,  0.0609,  0.0668,  0.0553,  0.0573,\n",
       "                       0.0418,  0.0781,  0.0750,  0.0738,  0.0509,  0.0585,  0.0593,  0.0595,\n",
       "                       0.0567,  0.0624,  0.0542,  0.0652,  0.0587,  0.0602,  0.0600,  0.0518,\n",
       "                       0.0623,  0.0667,  0.0270,  0.0602,  0.0430,  0.0593,  0.0716,  0.0570,\n",
       "                       0.0584,  0.0604,  0.0472,  0.0665,  0.0506,  0.0503,  0.0507,  0.0537,\n",
       "                       0.0814,  0.0536,  0.0669,  0.0615,  0.0605,  0.0651,  0.0519,  0.0636,\n",
       "                       0.0645,  0.0413,  0.0556,  0.0746,  0.0587,  0.0569,  0.0557,  0.0628,\n",
       "                       0.0490,  0.0706, -0.0476,  0.0673,  0.0672,  0.0655,  0.0566,  0.0530,\n",
       "                       0.0893,  0.0102,  0.0568,  0.0550,  0.0586,  0.0652,  0.0601,  0.0560,\n",
       "                       0.0501,  0.0654,  0.0588,  0.0586,  0.0532,  0.0618,  0.0568,  0.0557,\n",
       "                       0.0294,  0.0600,  0.0596,  0.0675,  0.0724,  0.0662,  0.0631,  0.0596,\n",
       "                       0.0604,  0.0771,  0.0634,  0.0480,  0.0047,  0.0619,  0.0465,  0.0436,\n",
       "                       0.0724,  0.0489,  0.0700,  0.0529,  0.0719,  0.0712,  0.0559,  0.0864,\n",
       "                       0.0527,  0.0538,  0.0580,  0.0698,  0.0597,  0.0444,  0.0535,  0.0879,\n",
       "                       0.0556,  0.0408,  0.0508,  0.0484,  0.0669,  0.0600,  0.0567,  0.0696,\n",
       "                       0.0517,  0.0624,  0.0538,  0.0642,  0.0542,  0.0558,  0.0748,  0.0774,\n",
       "                       0.0628,  0.0533,  0.0607,  0.0686,  0.0645,  0.0669,  0.0594,  0.0602,\n",
       "                       0.0671,  0.0561,  0.0679,  0.0586,  0.0592,  0.0659,  0.0546,  0.0551,\n",
       "                       0.0595,  0.0643,  0.0558,  0.0628,  0.0489,  0.0568,  0.0616,  0.0672,\n",
       "                       0.0530,  0.0620,  0.0415,  0.0501,  0.0501,  0.0757,  0.0422,  0.0528,\n",
       "                       0.0537,  0.1057,  0.0492,  0.0723,  0.0416,  0.0575,  0.0463,  0.0593,\n",
       "                       0.0650,  0.0501,  0.0571,  0.0638,  0.0589,  0.0631,  0.0829,  0.0583,\n",
       "                       0.0489,  0.0525,  0.0623,  0.0630,  0.0573,  0.0502,  0.0542,  0.0519,\n",
       "                       0.0739,  0.0506,  0.0610,  0.0465,  0.0626,  0.0687,  0.0662,  0.0584,\n",
       "                       0.0514,  0.0725,  0.0632,  0.0468,  0.0635,  0.0652,  0.0626,  0.0559,\n",
       "                       0.0659,  0.0592,  0.0488,  0.0654,  0.0521,  0.0815,  0.0821,  0.0587,\n",
       "                       0.0469,  0.0683,  0.0543,  0.0707,  0.0628,  0.0621,  0.0652, -0.0450,\n",
       "                       0.0716,  0.0606,  0.0600,  0.0516,  0.0687,  0.0660,  0.0572,  0.0745,\n",
       "                       0.0459,  0.0386,  0.0581,  0.0632,  0.0560,  0.0462,  0.0648,  0.0434,\n",
       "                       0.0669,  0.0548,  0.0462,  0.0562,  0.0439,  0.0779,  0.0511,  0.0641,\n",
       "                       0.0419,  0.0960,  0.0701,  0.0739,  0.0455,  0.0673,  0.0733,  0.0684,\n",
       "                       0.0421,  0.0598,  0.0627,  0.0578,  0.0406,  0.0511,  0.0489,  0.0558,\n",
       "                       0.0592,  0.0819,  0.0424,  0.0590,  0.0546,  0.0677,  0.0769,  0.0643,\n",
       "                       0.0626,  0.0536,  0.0605,  0.0672,  0.0705,  0.0543,  0.0555,  0.0607,\n",
       "                       0.0528,  0.0605,  0.0546,  0.0619,  0.0630,  0.0616,  0.0616,  0.0661,\n",
       "                       0.0596,  0.0544,  0.0585,  0.0604,  0.0459,  0.0575,  0.0572,  0.0757,\n",
       "                       0.0620,  0.0567,  0.0438,  0.0511,  0.0465,  0.0549,  0.0600,  0.0512,\n",
       "                       0.0531,  0.0628,  0.0574,  0.0650,  0.0524,  0.0567,  0.0684,  0.0440,\n",
       "                       0.0766,  0.0741,  0.0710,  0.0540,  0.0627,  0.0641,  0.0573,  0.0461,\n",
       "                       0.0515,  0.0706,  0.0673,  0.0736,  0.0488,  0.0616,  0.0468,  0.0631,\n",
       "                       0.0499,  0.0588,  0.0716,  0.0608,  0.0632,  0.0539,  0.0563,  0.0643,\n",
       "                       0.0574,  0.0677,  0.0697,  0.0622,  0.0674,  0.0345,  0.0641,  0.0533,\n",
       "                       0.0611,  0.0627,  0.0661,  0.0621,  0.0618,  0.0660,  0.0614,  0.0620,\n",
       "                       0.0713,  0.0603,  0.0616,  0.0443,  0.0526,  0.0488,  0.0547,  0.0669,\n",
       "                       0.0554,  0.0247,  0.0407,  0.0609,  0.0465,  0.0446,  0.0620,  0.0430,\n",
       "                       0.0436,  0.0635,  0.0664,  0.0731,  0.0636,  0.0450,  0.0563,  0.0514,\n",
       "                       0.0453,  0.0580,  0.0601,  0.0578,  0.0563,  0.0597,  0.0561,  0.0514],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.0.layer.1.DenseReluDense.wi.weight',\n",
       "              tensor([[ 0.0690, -0.6003, -0.3884,  ...,  0.4314,  0.6258, -0.6826],\n",
       "                      [ 1.1520, -1.0763, -0.4021,  ...,  0.0159,  0.3083,  0.0382],\n",
       "                      [-1.5952,  0.2516, -0.1143,  ..., -0.3718, -0.8273, -0.2195],\n",
       "                      ...,\n",
       "                      [ 1.6450, -0.5401,  0.2957,  ..., -1.1833,  1.8722,  0.0264],\n",
       "                      [-0.7442,  0.4229, -0.3450,  ...,  1.0017, -0.6792, -0.5489],\n",
       "                      [-0.6130,  0.3762,  0.5212,  ...,  1.1622, -0.6729, -1.3414]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.0.layer.1.DenseReluDense.wo.weight',\n",
       "              tensor([[-0.1479,  0.1197,  0.0952,  ..., -0.1661, -0.4331, -0.8787],\n",
       "                      [ 0.0036, -0.2319,  0.6775,  ...,  0.0269, -0.0970,  0.8095],\n",
       "                      [ 0.0509,  0.0396,  0.6090,  ...,  0.3206, -0.2480, -0.1623],\n",
       "                      ...,\n",
       "                      [-0.2118, -0.8540, -0.3982,  ...,  0.4184, -0.1493, -0.7657],\n",
       "                      [-0.6443, -0.4457,  0.3875,  ...,  0.5358,  0.1882, -0.2404],\n",
       "                      [-0.3697, -0.4716, -0.1072,  ..., -0.4957,  0.4086, -0.2483]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.0.layer.1.layer_norm.weight',\n",
       "              tensor([ 0.1750,  0.1849,  0.1578,  0.2555,  0.1247,  0.1621,  0.2085,  0.1638,\n",
       "                       0.2900,  0.1557,  0.1586,  0.1468,  0.2247,  0.1824,  0.1317,  0.1596,\n",
       "                       0.1680,  0.1673, -0.1711,  0.1599,  0.1528,  0.1395,  0.1828,  0.1325,\n",
       "                       0.1184,  0.1654,  0.1428,  0.1973,  0.1233,  0.1320,  0.1386,  0.2650,\n",
       "                       0.1673,  0.1479,  0.1470,  0.1188,  0.1895,  0.2442,  0.1647,  0.1750,\n",
       "                       0.1710,  0.1873,  0.1693,  0.1382, -0.5456,  0.1524,  0.2433,  0.1243,\n",
       "                       0.1316,  0.1838,  0.1957,  0.1346,  0.1291,  0.1481,  0.0694,  0.1552,\n",
       "                       0.3699,  0.1540,  0.1383,  0.1258,  0.1634, -0.1584,  0.1564,  0.1411,\n",
       "                       0.1374,  0.2967,  0.1471,  0.1457,  0.2685,  0.1607,  0.1480,  0.1705,\n",
       "                       0.1511,  0.1454,  0.1285,  0.1482,  0.1886,  0.1464,  0.1270,  0.1156,\n",
       "                       0.1441,  0.1304,  0.1455,  0.2143,  0.1232,  0.1414,  0.4343,  0.1655,\n",
       "                       0.1630,  0.1459,  0.1492,  0.1386,  0.1557,  0.1712,  0.1447,  0.1414,\n",
       "                       0.1444,  0.1439,  0.1531,  0.1282,  0.1549,  0.1533,  0.1915,  0.1393,\n",
       "                       0.1343,  0.2125,  0.1490,  0.1269,  0.1200,  0.1848,  0.1668,  0.1599,\n",
       "                       0.1358,  0.1401,  0.1756,  0.1358,  0.1328,  0.1461,  0.3433,  0.1469,\n",
       "                       0.1327,  0.1304,  0.2094,  0.1500,  0.1792,  0.1506,  0.1352,  0.1598,\n",
       "                       0.1669,  0.2139,  0.2573,  0.1457,  0.1370,  0.1263,  0.1633,  0.1829,\n",
       "                       0.1454,  0.1448,  0.1577,  0.1804,  0.1513,  0.1516,  0.1648,  0.1927,\n",
       "                       0.1744,  0.1997,  0.1446,  0.1474,  0.1297,  0.1458,  0.1424,  0.1415,\n",
       "                       0.1538,  0.1871,  0.1423,  0.1300,  0.1695,  0.1457,  0.1880,  0.1316,\n",
       "                       0.1310,  0.1479,  0.1863,  0.2769,  0.1346,  0.1080,  0.1390,  0.2039,\n",
       "                       0.1299,  0.1547,  0.2106, -0.1549,  0.1393,  0.1937,  0.1272,  0.1503,\n",
       "                       0.1338,  0.2008,  0.1872,  0.0689,  0.1449,  0.1523,  0.1505,  0.1355,\n",
       "                       0.1569,  0.1831,  0.1435,  0.1602,  0.1492,  0.1505,  0.1583,  0.1411,\n",
       "                       0.1405,  0.1308,  0.1544,  0.3085,  0.1643,  0.1601,  0.1554,  0.1353,\n",
       "                       0.1162,  0.1601,  0.1465,  0.1538,  0.1462,  0.1722,  0.1631,  0.1519,\n",
       "                       0.1254,  0.1528,  0.1899,  0.1645,  0.1283,  0.1708,  0.1718,  0.1428,\n",
       "                       0.1392,  0.1507, -0.1375,  0.5661,  0.2286,  0.1476,  0.1300,  0.1456,\n",
       "                       0.1520,  0.1457,  0.1916,  0.1556,  0.1500,  0.1678,  0.2024,  0.1670,\n",
       "                       0.2457,  0.1312,  0.1380, -0.1569,  0.0174,  0.1540,  0.2744,  0.1724,\n",
       "                       0.5246,  0.1758,  0.1416,  0.1645,  0.1255,  0.1653,  0.1388,  0.1500,\n",
       "                       0.1474,  0.1474,  0.2407,  0.1466,  0.1343,  0.1387,  0.1489,  0.1472,\n",
       "                       0.1504,  0.1574,  0.2268,  0.1415,  0.1912,  0.1549,  0.1489,  0.1600,\n",
       "                       0.1547,  0.1463,  0.1458,  0.1620,  0.1534,  0.1414,  0.4132,  0.1897,\n",
       "                       0.1676,  0.2628,  0.2078,  0.1674,  0.1495,  0.1534,  0.1367,  0.1689,\n",
       "                      -0.3945,  0.2076,  0.1384,  0.1550,  0.1572,  0.1799,  0.1246,  0.1321,\n",
       "                       0.1638,  0.2642,  0.1457,  0.1537,  0.1374,  0.1204,  0.1370,  0.1325,\n",
       "                       0.1570,  0.1269,  0.1652,  0.1479,  0.1750,  0.1363,  0.2071,  0.1521,\n",
       "                       0.1456,  0.1363,  0.1484,  0.1431,  0.1404,  0.1560,  0.1449,  0.1442,\n",
       "                       0.1434,  0.2248,  0.1404,  0.1427,  0.1973,  0.3099,  0.1629,  0.1485,\n",
       "                       0.1285,  0.1436,  0.1558,  0.1356,  0.1586,  0.1333,  0.1561,  0.1563,\n",
       "                       0.1720,  0.1318,  0.1347,  0.1747,  0.1564,  0.1435,  0.1240,  0.1539,\n",
       "                       0.1352,  0.1379,  0.1528,  0.2434,  0.1550,  0.1542,  0.1487,  0.1490,\n",
       "                       0.1509,  0.1488,  0.1613,  0.1820,  0.2226,  0.2117,  0.2004,  0.1441,\n",
       "                       0.1538,  0.1406,  0.1603,  0.1413,  0.1503,  0.2234,  0.1360,  0.1660,\n",
       "                       0.1312,  0.1747,  0.2560,  0.1435,  0.1416,  0.1614,  0.2282,  0.1483,\n",
       "                       0.1554,  0.1509,  0.1495,  0.1549,  0.0811,  0.1292,  0.1568,  0.1514,\n",
       "                       0.1900,  0.1614,  0.1375,  0.2521,  0.1253,  0.1585,  0.2109,  0.1501,\n",
       "                       0.1387,  0.2358,  0.1494,  0.1986,  0.1469,  0.1606,  0.1332,  0.1789,\n",
       "                       0.1517,  0.2460,  0.1796,  0.1498,  0.1753,  0.1346,  0.1268,  0.1373,\n",
       "                       0.1359,  0.0638,  0.1184,  0.1479,  0.1682,  0.1466,  0.1785,  0.1452,\n",
       "                       0.1309,  0.1615,  0.1329,  0.1491,  0.1594,  0.1432,  0.1176,  0.1444,\n",
       "                       0.1313,  0.1312,  0.0753,  0.1713,  0.4913,  0.1372,  0.1219,  0.2423,\n",
       "                       0.1623,  0.1531,  0.1587,  0.1359,  0.1662,  0.1739,  0.1561,  0.1203,\n",
       "                       0.2054,  0.1453,  0.1917,  0.1515,  0.1561,  0.1425,  0.1452,  0.1648,\n",
       "                       0.1341,  0.1974,  0.1896,  0.1640,  0.1505,  0.1472,  0.1315,  0.1424,\n",
       "                       0.4386,  0.1308,  0.1528,  0.1343,  0.1313,  0.1519,  0.1453,  0.1811,\n",
       "                       0.2343,  0.2214,  0.1857,  0.1528,  0.1190,  0.1194,  0.1401,  0.1805,\n",
       "                       0.1637,  0.1422,  0.1434,  0.1374,  0.1467,  0.1333,  0.1430,  0.1287,\n",
       "                       0.1023,  0.1266,  0.1633,  0.1396,  0.1302,  0.1279,  0.1695,  0.1256,\n",
       "                       0.1506,  0.2367,  0.1611,  0.1365,  0.2105,  0.1559,  0.1654,  0.1425,\n",
       "                       0.1361,  0.1532,  0.1234,  0.1445,  0.1873,  0.1176,  0.1545,  0.4485,\n",
       "                       0.1538,  0.1800,  0.1417,  0.1453,  0.1300,  0.1979,  0.1625,  0.2150,\n",
       "                       0.1389,  0.1674,  0.1643,  0.1562,  0.1236,  0.1487,  0.1248,  0.1381,\n",
       "                       0.3451,  0.1390,  0.1723,  0.1399,  0.1443,  0.1534,  0.2541,  0.1484,\n",
       "                       0.1488,  0.1520,  0.1292,  0.1696,  0.1370,  0.1955,  0.1568,  0.1779,\n",
       "                       0.1384,  0.1496,  0.1726,  0.1303,  0.1117,  0.1491,  0.1636,  0.1391,\n",
       "                       0.1389,  0.1350,  0.1684,  0.1408,  0.1554,  0.1637,  0.1617,  0.1504,\n",
       "                       0.1551,  0.1527,  0.1500,  0.1339,  0.1607,  0.1753,  0.1808,  0.1402,\n",
       "                       0.1502,  0.4150,  0.1413,  0.1460,  0.1561,  0.1368,  0.1554,  0.1685,\n",
       "                       0.1454,  0.1528,  0.2665,  0.4651,  0.1221,  0.1422,  0.0536,  0.1685,\n",
       "                       0.1514,  0.1299,  0.1375,  0.0926,  0.1259,  0.1258,  0.1475,  0.1345,\n",
       "                       0.1615,  0.1674,  0.1202,  0.2621,  0.1759,  0.1249,  0.1360,  0.1540,\n",
       "                       0.1257,  0.1232,  0.1440,  0.1566,  0.1665,  0.1408,  0.1443,  0.1397,\n",
       "                       0.1274,  0.1634,  0.1425,  0.1242,  0.1762,  0.1576,  0.2501,  0.1352,\n",
       "                       0.1507,  0.1452,  0.1782,  0.2064,  0.1550,  0.1232,  0.1483,  0.2246,\n",
       "                       0.1792,  0.1777, -0.1599,  0.1458,  0.1482,  0.1413,  0.1405,  0.1493,\n",
       "                       0.1422,  0.2501,  0.1562,  0.1456,  0.1833,  0.1480,  0.1423,  0.1764,\n",
       "                       0.1535,  0.1494,  0.1676,  0.1728,  0.1592,  0.1939,  0.1979,  0.1531,\n",
       "                       0.1690,  0.5389,  0.1507,  0.2183,  0.1656,  0.1623,  0.1579,  0.1300,\n",
       "                      -0.1854,  0.1448,  0.1486,  0.1593,  0.1773,  0.1538,  0.2270,  0.1386,\n",
       "                       0.1455,  0.1434,  0.1576,  0.1371,  0.1397,  0.1604,  0.1504,  0.2979,\n",
       "                       0.1622,  0.1398,  0.1704,  0.1715,  0.1676,  0.0804,  0.1621,  0.1485,\n",
       "                       0.1605,  0.1502,  0.1455,  0.1199,  0.1570,  0.1482,  0.1412,  0.1606,\n",
       "                       0.1595,  0.1397,  0.1531,  0.1214,  0.1684,  0.1647,  0.1532,  0.1582,\n",
       "                       0.1639,  0.1430,  0.1581,  0.1519, -0.1556,  0.1798, -0.1484,  0.1702,\n",
       "                       0.1369,  0.1470, -0.1388,  0.1612,  0.2441,  0.1311,  0.1507,  0.1376,\n",
       "                       0.1373,  0.1741,  0.1178,  0.1557,  0.1502, -0.1640,  0.1513,  0.1335,\n",
       "                       0.1450,  0.1364,  0.1475,  0.1957,  0.2087, -0.1361,  0.3748,  0.1474,\n",
       "                       0.1389,  0.1488,  0.2895,  0.1150,  0.1760,  0.1407,  0.1845,  0.1435,\n",
       "                       0.1580,  0.1906,  0.1765,  0.1384,  0.1596,  0.2082,  0.1408,  0.1621,\n",
       "                       0.1164,  0.1654,  0.1594,  0.1428,  0.1361,  0.1998,  0.1537,  0.1068,\n",
       "                       0.1362,  0.1805,  0.1420,  0.1455,  0.1304,  0.1594,  0.1720,  0.1952,\n",
       "                       0.1323,  0.1584,  0.1582,  0.1241,  0.1560,  0.1374,  0.1543,  0.1457,\n",
       "                       0.2059,  0.1329,  0.1782,  0.1398,  0.1715,  0.1527,  0.1913,  0.1613,\n",
       "                       0.1694,  0.1612,  0.1580,  0.1258,  0.1676,  0.1542,  0.1390,  0.1896],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.1.layer.0.SelfAttention.q.weight',\n",
       "              tensor([[ 0.0717, -0.0057, -0.0285,  ..., -0.0193,  0.0066,  0.0317],\n",
       "                      [ 0.0428,  0.0053,  0.0599,  ...,  0.0002,  0.0335, -0.0191],\n",
       "                      [-0.1301, -0.0746, -0.1070,  ..., -0.0748,  0.0012, -0.0599],\n",
       "                      ...,\n",
       "                      [ 0.0495,  0.0051,  0.0166,  ..., -0.0310, -0.0903,  0.0046],\n",
       "                      [ 0.0096, -0.0545, -0.0585,  ..., -0.0907,  0.0433,  0.0357],\n",
       "                      [ 0.0108, -0.0196,  0.0215,  ...,  0.0208, -0.1321,  0.0088]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.1.layer.0.SelfAttention.k.weight',\n",
       "              tensor([[ 0.2190,  0.8442,  0.0803,  ..., -0.3290,  0.1990,  0.3695],\n",
       "                      [ 0.3291,  0.6975,  0.1210,  ..., -0.1172, -0.3221, -0.4531],\n",
       "                      [-1.0127,  0.6828, -0.4938,  ..., -0.3236, -0.5781, -1.1825],\n",
       "                      ...,\n",
       "                      [ 0.1779, -0.4866, -0.5781,  ..., -0.0731, -0.7198,  0.2849],\n",
       "                      [ 0.3486, -0.3723, -0.9844,  ..., -0.5110,  0.9921,  1.0443],\n",
       "                      [ 0.1851,  0.1932, -0.1891,  ...,  0.0937, -0.4634, -0.0035]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.1.layer.0.SelfAttention.v.weight',\n",
       "              tensor([[ 0.5012, -0.0143,  0.0725,  ...,  0.4006, -0.5266, -0.7450],\n",
       "                      [-0.6777,  0.0431, -0.0411,  ..., -1.2082,  0.1818,  0.0248],\n",
       "                      [-2.0849,  0.1739,  0.2751,  ..., -0.9336, -0.1375, -0.6550],\n",
       "                      ...,\n",
       "                      [-0.4486, -0.4732,  0.4100,  ..., -0.6535,  0.5202,  1.3244],\n",
       "                      [ 0.7255, -0.6624,  1.0020,  ..., -0.2536,  0.3663, -0.6754],\n",
       "                      [-0.5922,  0.0752, -0.5581,  ..., -0.5923, -0.6134, -0.3733]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.1.layer.0.SelfAttention.o.weight',\n",
       "              tensor([[ 0.0791, -0.0591,  0.2017,  ...,  0.6466,  0.2340,  0.7692],\n",
       "                      [ 0.2948, -0.5316, -0.2024,  ...,  0.7983, -0.5261,  0.0238],\n",
       "                      [-0.0466,  0.3818,  0.3103,  ..., -0.8599,  0.1062,  1.6105],\n",
       "                      ...,\n",
       "                      [ 0.2836,  0.6296,  0.8190,  ..., -0.2864, -0.2991,  0.2868],\n",
       "                      [ 0.8634, -0.0290,  0.7454,  ..., -0.5603, -0.4963, -1.7717],\n",
       "                      [-0.1094, -0.6400,  0.0659,  ..., -0.9275,  0.0246, -1.5337]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.1.layer.0.layer_norm.weight',\n",
       "              tensor([ 0.0824,  0.0788,  0.0856,  0.0591,  0.0731,  0.0541,  0.0375,  0.0947,\n",
       "                       0.0419,  0.0827,  0.0834,  0.0860,  0.0553,  0.0815,  0.0689,  0.0734,\n",
       "                       0.0898,  0.0715,  0.0857,  0.0722,  0.0801,  0.0876,  0.0135,  0.0613,\n",
       "                       0.0685,  0.0772,  0.0733,  0.0752,  0.0909,  0.0631,  0.0643,  0.0521,\n",
       "                       0.0855,  0.0679,  0.0943,  0.0849,  0.0839,  0.0923,  0.0580,  0.1100,\n",
       "                       0.0841,  0.0898,  0.0881,  0.0956,  0.0358,  0.0666,  0.0821,  0.0737,\n",
       "                       0.0722,  0.0884,  0.1044,  0.0741,  0.0850,  0.0749,  0.0122,  0.0697,\n",
       "                       0.0223,  0.0755,  0.0844,  0.0686,  0.0857,  0.0653,  0.0808,  0.0749,\n",
       "                       0.0759,  0.0340,  0.0685,  0.0757,  0.0989,  0.0814,  0.0848,  0.1031,\n",
       "                       0.0596,  0.0820,  0.0779,  0.0721,  0.0776,  0.0777,  0.0649,  0.0765,\n",
       "                       0.0721,  0.0868,  0.0602,  0.0869,  0.0784,  0.0784,  0.0461,  0.0864,\n",
       "                       0.0958,  0.0636,  0.0731,  0.0594,  0.0792,  0.0790,  0.0716,  0.0721,\n",
       "                       0.0769,  0.0845,  0.0626,  0.0572,  0.0926,  0.0892,  0.0828,  0.0681,\n",
       "                       0.0859,  0.0954,  0.0698,  0.0793,  0.0619,  0.0873,  0.0773,  0.0817,\n",
       "                       0.0633,  0.0724,  0.0815,  0.0742,  0.0665,  0.0504,  0.0789,  0.0684,\n",
       "                       0.0618,  0.0790,  0.0829,  0.0710,  0.0936,  0.0707,  0.0775,  0.0743,\n",
       "                       0.0718,  0.0703,  0.0673,  0.0860,  0.0672, -0.0636,  0.0658,  0.0829,\n",
       "                       0.0870,  0.0870,  0.0752,  0.0701,  0.0707,  0.1022,  0.0895,  0.0868,\n",
       "                       0.0786,  0.0754,  0.0582,  0.0821,  0.0643,  0.0743,  0.0706,  0.0710,\n",
       "                       0.0733,  0.0924,  0.0767,  0.0640,  0.0843,  0.0711,  0.0730,  0.0655,\n",
       "                       0.0581, -0.0719,  0.0593,  0.0457,  0.0507,  0.0635,  0.0830,  0.0698,\n",
       "                       0.0573,  0.0916,  0.0988,  0.0810,  0.0814,  0.0904,  0.0795,  0.1012,\n",
       "                       0.0783,  0.1145,  0.0925,  0.0002,  0.0736,  0.0703,  0.0798,  0.0711,\n",
       "                       0.0787,  0.0934,  0.0577,  0.1019,  0.0789,  0.0633,  0.0594,  0.0778,\n",
       "                       0.0570,  0.0819,  0.0627,  0.0334,  0.0765,  0.0990,  0.0828,  0.0713,\n",
       "                       0.0633,  0.0824,  0.0816,  0.0763,  0.0718,  0.0815,  0.0630,  0.0757,\n",
       "                       0.0679,  0.0813,  0.0662,  0.0754,  0.0827,  0.0948,  0.0667,  0.0900,\n",
       "                       0.0840,  0.0736,  0.0816,  0.0417,  0.0647,  0.0756,  0.0493,  0.0798,\n",
       "                       0.0707,  0.0712,  0.0602,  0.1164,  0.0709,  0.0878,  0.0579,  0.0851,\n",
       "                       0.0468,  0.0588,  0.0673,  0.0339,  0.0353,  0.0984,  0.0484,  0.1052,\n",
       "                       0.0623,  0.0996,  0.0685,  0.0727,  0.0554,  0.0719,  0.0733,  0.0706,\n",
       "                       0.0620,  0.0813,  0.0568,  0.0630,  0.0688,  0.0658,  0.0673,  0.0614,\n",
       "                       0.0728,  0.0504,  0.0833,  0.0678,  0.0941,  0.0992,  0.0772,  0.0812,\n",
       "                       0.0676,  0.0672,  0.0749,  0.0688,  0.0672,  0.0782,  0.0266,  0.0837,\n",
       "                       0.0962,  0.0218,  0.0981,  0.0833,  0.0740,  0.0850,  0.0703,  0.0663,\n",
       "                       0.0900,  0.0939,  0.0773,  0.0781,  0.0953,  0.0699,  0.0591,  0.0746,\n",
       "                       0.0728,  0.0816,  0.0711,  0.0741,  0.0727,  0.0771,  0.0745,  0.0640,\n",
       "                       0.0804,  0.0730,  0.0706,  0.0833,  0.0887,  0.0715,  0.0944,  0.1039,\n",
       "                       0.0697,  0.0609,  0.0853,  0.0648,  0.0783,  0.0860,  0.0731,  0.1018,\n",
       "                       0.0689,  0.0618,  0.0864,  0.0780,  0.0712,  0.0709,  0.0653,  0.0685,\n",
       "                       0.0718,  0.1058,  0.0751,  0.0639,  0.0879,  0.0662,  0.0931,  0.1077,\n",
       "                       0.0719,  0.0763,  0.0791,  0.0838,  0.0937,  0.0685,  0.0689,  0.0751,\n",
       "                       0.0634,  0.0741,  0.0708,  0.1024,  0.0814,  0.0872,  0.0769,  0.0702,\n",
       "                       0.0779,  0.0765,  0.0763,  0.0662,  0.0979,  0.0628,  0.0793,  0.0605,\n",
       "                       0.0836,  0.0701,  0.0872,  0.0822,  0.0850,  0.1091,  0.0759,  0.0967,\n",
       "                       0.0747,  0.1055,  0.0612,  0.0514,  0.0689,  0.0874,  0.0747,  0.0872,\n",
       "                       0.0632,  0.0731,  0.0799,  0.0820,  0.0033,  0.0617,  0.0879,  0.0634,\n",
       "                       0.0958,  0.0677,  0.1123,  0.0671,  0.0719,  0.0750,  0.0836,  0.0762,\n",
       "                       0.0788,  0.0793,  0.0780,  0.1054,  0.0512,  0.0791,  0.0648,  0.0948,\n",
       "                       0.0749,  0.0961,  0.0865,  0.0819,  0.0855,  0.0675,  0.0825,  0.0727,\n",
       "                       0.0586,  0.0106,  0.0651,  0.0836,  0.0687,  0.0698,  0.0915,  0.0659,\n",
       "                       0.0659,  0.0669,  0.0743,  0.0659,  0.0707,  0.0736,  0.0662,  0.0844,\n",
       "                       0.0628,  0.0692,  0.0302,  0.0753,  0.0494,  0.0801,  0.0641,  0.0488,\n",
       "                       0.0754,  0.0680,  0.0811,  0.0679,  0.0819,  0.0814,  0.0807,  0.0568,\n",
       "                       0.1002,  0.0581,  0.1041,  0.0708,  0.0762,  0.0683,  0.0697,  0.0868,\n",
       "                       0.0824,  0.0940,  0.0926,  0.0997,  0.0635,  0.0807,  0.0727,  0.0772,\n",
       "                       0.0507,  0.0815,  0.0844,  0.0759,  0.0657,  0.0668,  0.0830,  0.0729,\n",
       "                       0.0987,  0.0498,  0.0700,  0.0718,  0.0635,  0.0651,  0.0682,  0.0752,\n",
       "                       0.0726,  0.0892,  0.0628,  0.0700,  0.0642,  0.0680,  0.0582,  0.0611,\n",
       "                       0.0384,  0.0648,  0.0719,  0.0764,  0.0752,  0.0725,  0.0948,  0.0765,\n",
       "                       0.0843,  0.0907,  0.0915,  0.0618,  0.0834,  0.0696,  0.0654,  0.0728,\n",
       "                       0.0644,  0.0873,  0.0816,  0.0916,  0.0864,  0.0687,  0.0764,  0.0666,\n",
       "                       0.0903,  0.0690,  0.0714,  0.0642,  0.0693,  0.0831,  0.0845,  0.0280,\n",
       "                       0.0765,  0.0750,  0.0771, -0.0847,  0.0600,  0.0883,  0.0839,  0.0692,\n",
       "                       0.0392,  0.0814,  0.1004,  0.0725,  0.0730,  0.0657,  0.0785,  0.0917,\n",
       "                       0.0661,  0.0762,  0.0624,  0.0771,  0.0735,  0.0803,  0.0829,  0.1009,\n",
       "                       0.0704,  0.0710,  0.0860,  0.0755,  0.0884,  0.0824,  0.0873,  0.0696,\n",
       "                       0.0867,  0.0742,  0.0917,  0.0671,  0.0732,  0.0765,  0.0696,  0.0985,\n",
       "                       0.0746,  0.0754,  0.0872,  0.0725,  0.0845,  0.0928,  0.0939,  0.0941,\n",
       "                       0.0571,  0.0691,  0.0708,  0.0918,  0.0594,  0.0724,  0.0784,  0.0922,\n",
       "                       0.0963,  0.0738,  0.0790,  0.0473,  0.0781,  0.0928,  0.0088,  0.0746,\n",
       "                       0.0830,  0.0739,  0.0897,  0.0276,  0.0643,  0.0753,  0.0774,  0.0921,\n",
       "                       0.0761,  0.0964,  0.0725,  0.0855,  0.0613,  0.0702,  0.0822,  0.0834,\n",
       "                       0.0610,  0.0712,  0.0719,  0.0632,  0.0506,  0.0695,  0.0759,  0.0821,\n",
       "                       0.0733,  0.0775,  0.0778,  0.0753,  0.0723,  0.0827,  0.0855,  0.0703,\n",
       "                       0.0812,  0.0771,  0.0847,  0.0651,  0.0809,  0.0770,  0.0646,  0.0732,\n",
       "                       0.0986,  0.0824,  0.0863,  0.0754,  0.0784,  0.0645,  0.0809,  0.0842,\n",
       "                       0.0570,  0.0656,  0.0652,  0.0794,  0.0782,  0.0576,  0.0750,  0.0762,\n",
       "                       0.0768,  0.0751,  0.0718,  0.0852,  0.0629,  0.0767,  0.0761,  0.0728,\n",
       "                       0.0798,  0.0573,  0.0728,  0.0969,  0.0753,  0.0918,  0.1083,  0.0697,\n",
       "                       0.0934,  0.0817,  0.0644,  0.0813,  0.0700,  0.0848,  0.0742,  0.0720,\n",
       "                       0.0620,  0.0725,  0.0608,  0.0752,  0.0707,  0.0799,  0.0841,  0.0706,\n",
       "                       0.0789,  0.0691,  0.0914,  0.0793,  0.0894,  0.0051,  0.0594,  0.1085,\n",
       "                       0.0722,  0.0915,  0.0778,  0.0680,  0.0820,  0.0749,  0.0771,  0.0803,\n",
       "                       0.0728,  0.0704,  0.0822,  0.0692,  0.0713,  0.0712,  0.0812,  0.0787,\n",
       "                       0.0729,  0.0697,  0.0780,  0.0577,  0.0849,  0.0705,  0.0716,  0.0739,\n",
       "                       0.0679,  0.0915,  0.0670,  0.0877,  0.1212,  0.0666,  0.0460,  0.0618,\n",
       "                       0.0870,  0.0621,  0.0750,  0.0753,  0.0808,  0.0551,  0.0845,  0.0773,\n",
       "                       0.0789,  0.0753,  0.0693,  0.0767,  0.0867,  0.0768,  0.0236,  0.0760,\n",
       "                       0.0794,  0.0707,  0.0284,  0.0796,  0.0968,  0.0877,  0.0834,  0.1007,\n",
       "                       0.0636,  0.0800,  0.1049,  0.0684,  0.0958,  0.0746,  0.0854,  0.0669,\n",
       "                       0.0691,  0.0784,  0.0814,  0.0695,  0.0730,  0.0573,  0.0717,  0.0746,\n",
       "                       0.0806,  0.0755,  0.0662,  0.0850,  0.0739,  0.0887,  0.0844,  0.0788,\n",
       "                       0.0667,  0.0936,  0.0687,  0.0642,  0.0806,  0.0783,  0.0728,  0.0706,\n",
       "                       0.0978,  0.0896,  0.0926,  0.0801,  0.0783,  0.0683,  0.0967,  0.0849,\n",
       "                       0.0644,  0.0572,  0.0791,  0.0643,  0.0875,  0.0900,  0.0813,  0.0813],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.1.layer.1.DenseReluDense.wi.weight',\n",
       "              tensor([[-0.0705,  0.3047,  1.6485,  ..., -1.1141,  0.7510, -2.1403],\n",
       "                      [-0.1971, -0.5156,  0.6082,  ...,  0.5601,  0.0060, -0.8706],\n",
       "                      [-0.1802,  1.0763, -0.2925,  ..., -0.6207,  0.3353,  0.3988],\n",
       "                      ...,\n",
       "                      [-0.4677, -0.7662, -0.1463,  ..., -0.6095, -0.2811, -0.6909],\n",
       "                      [-2.6019, -0.3645, -1.2776,  ..., -0.1146,  1.2995, -1.1591],\n",
       "                      [-0.4612,  0.4263,  0.7559,  ..., -0.7972, -0.2148,  0.5624]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.1.layer.1.DenseReluDense.wo.weight',\n",
       "              tensor([[-0.0070, -0.6055,  0.1155,  ..., -0.4987,  0.0967, -0.2510],\n",
       "                      [ 0.0936,  0.2665,  0.0540,  ...,  0.2978, -0.1244,  0.0765],\n",
       "                      [-0.1465,  0.6228,  0.1627,  ..., -0.5305, -0.2165, -0.3085],\n",
       "                      ...,\n",
       "                      [-0.2203,  0.8102,  0.4256,  ..., -0.4092, -0.4495,  0.2840],\n",
       "                      [-0.2170, -0.1217,  0.5674,  ...,  0.2192,  0.3797, -0.2635],\n",
       "                      [-0.8988, -0.3613,  0.7661,  ..., -0.3744,  0.8173, -0.2080]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.1.layer.1.layer_norm.weight',\n",
       "              tensor([ 3.2646e-01,  3.8674e-01,  4.2285e-01,  3.7865e-01,  2.0669e-01,\n",
       "                       2.8136e-01,  1.4825e-01,  3.1983e-01,  7.3803e-01,  2.6534e-01,\n",
       "                       3.1457e-01,  3.2420e-01,  3.7270e-01,  4.1645e-01,  2.0754e-01,\n",
       "                       2.7255e-01,  3.1972e-01,  3.0664e-01,  3.9382e-01,  2.1871e-01,\n",
       "                       2.8716e-01,  3.1642e-01,  4.0403e-01,  2.5246e-01,  2.0849e-01,\n",
       "                       2.8210e-01,  1.6764e-01,  4.3498e-01,  2.5880e-01,  2.5302e-01,\n",
       "                      -2.7955e-01,  3.3697e-01,  3.0219e-01,  2.3178e-01,  3.3532e-01,\n",
       "                       2.6627e-01,  4.8343e-01,  5.0681e-01,  1.7230e-01,  3.9012e-01,\n",
       "                       3.8166e-01,  4.6522e-01,  3.5271e-01,  3.0289e-01,  1.0273e+00,\n",
       "                       2.4837e-01,  5.8497e-01,  3.1498e-01,  2.9525e-01,  4.9823e-01,\n",
       "                       4.0492e-01,  3.3904e-01,  4.0604e-01,  2.3912e-01,  2.3897e-01,\n",
       "                       3.4587e-01,  5.1564e-01,  3.3648e-01,  2.1418e-01,  2.8482e-01,\n",
       "                       2.7409e-01,  2.0199e-01,  3.0598e-01,  2.3882e-01,  2.1891e-01,\n",
       "                       5.7749e-01,  2.4069e-01,  2.4331e-01,  5.3699e-01,  2.9112e-01,\n",
       "                       2.8810e-01,  3.3966e-01,  2.1487e-01,  3.0583e-01,  2.6062e-01,\n",
       "                       2.0436e-01,  4.8313e-01,  3.2715e-01,  2.2815e-01,  3.1482e-01,\n",
       "                       2.3394e-01,  2.7927e-01,  2.8164e-01,  5.4114e-01,  2.7934e-01,\n",
       "                       2.9696e-01,  5.1046e-01,  2.9978e-01,  2.7808e-01,  2.1603e-01,\n",
       "                       3.1292e-01,  1.8144e-01,  2.7591e-01,  3.6706e-01,  2.3628e-01,\n",
       "                       2.5457e-01,  2.2184e-01,  2.6822e-01,  1.9809e-01,  2.7135e-01,\n",
       "                       3.4084e-01,  2.6462e-01,  4.8460e-01,  2.3269e-01,  2.7300e-01,\n",
       "                       5.9427e-01,  2.4485e-01,  2.2817e-01,  1.8692e-01,  4.5286e-01,\n",
       "                       3.0163e-01,  2.6568e-01,  2.9806e-01,  3.1708e-01,  2.3171e-01,\n",
       "                       1.7145e-01,  2.7576e-01,  2.2292e-01,  6.5603e-01,  2.7783e-01,\n",
       "                       2.1115e-01,  2.6791e-01,  4.9666e-01,  2.4639e-01,  3.7723e-01,\n",
       "                       3.1427e-01,  2.9201e-01,  2.6943e-01,  3.0774e-01,  5.1009e-01,\n",
       "                       6.0342e-01,  3.2278e-01,  2.0290e-01,  3.0320e-01,  2.7274e-01,\n",
       "                       5.3557e-01,  2.9726e-01,  2.5435e-01,  2.7228e-01,  3.3109e-01,\n",
       "                       2.8082e-01,  3.6143e-01,  4.1548e-01,  5.5254e-01,  3.6319e-01,\n",
       "                       4.5170e-01,  2.3664e-01,  3.0757e-01,  2.5090e-01,  2.9371e-01,\n",
       "                       2.2657e-01,  3.1554e-01,  3.8145e-01,  4.8197e-01,  1.9922e-01,\n",
       "                       2.3869e-01,  3.8310e-01,  2.2301e-01,  4.2507e-01,  2.9042e-01,\n",
       "                       2.3452e-01, -2.7203e-01,  3.5372e-01,  7.4209e-01,  2.1171e-01,\n",
       "                       1.9857e-01,  2.3819e-01,  4.2193e-01,  2.8349e-01,  3.2515e-01,\n",
       "                       5.3242e-01,  2.0299e-01,  2.4017e-01,  5.1301e-01,  2.9736e-01,\n",
       "                       3.4319e-01,  3.0899e-01,  4.6157e-01,  5.1964e-01,  1.4176e-01,\n",
       "                       2.6402e-01,  2.6396e-01,  3.1872e-01,  1.6930e-01,  3.3886e-01,\n",
       "                       4.3319e-01,  2.4869e-01,  4.3375e-01,  3.3508e-01,  2.7384e-01,\n",
       "                       2.3937e-01,  1.8103e-01,  1.8023e-01,  2.6985e-01,  2.1665e-01,\n",
       "                       1.7245e-01,  2.4429e-01,  4.1705e-01,  3.2544e-01,  1.8772e-01,\n",
       "                       2.2217e-01,  3.1715e-01,  3.0053e-01,  2.6474e-01,  2.7568e-01,\n",
       "                       4.4184e-01,  3.5729e-01,  2.5851e-01,  2.0044e-01,  2.4514e-01,\n",
       "                       3.9764e-01,  3.3746e-01,  3.3768e-01,  3.5708e-01,  2.7148e-01,\n",
       "                       3.2130e-01,  2.9240e-01,  2.8246e-01,  3.5531e-01,  7.1727e-01,\n",
       "                       4.5283e-01,  3.3021e-01,  1.9420e-01,  3.1166e-01,  3.3423e-01,\n",
       "                       2.7385e-01,  3.7215e-01,  3.3287e-01,  2.7003e-01,  3.8231e-01,\n",
       "                       3.1321e-01,  2.7626e-01,  4.0080e-01,  2.5943e-01,  2.4923e-01,\n",
       "                       1.2569e-01, -1.9978e-04,  3.1127e-01,  3.5753e-01,  4.0819e-01,\n",
       "                       5.3671e-01,  3.2733e-01,  2.5316e-01,  2.6448e-01,  1.9075e-01,\n",
       "                       2.4869e-01,  2.4504e-01,  2.9346e-01,  1.9826e-01,  3.6954e-01,\n",
       "                       6.8303e-01,  1.8519e-01,  2.7070e-01,  2.3282e-01,  2.4052e-01,\n",
       "                       2.3028e-01,  2.3303e-01,  2.8327e-01,  5.2279e-01,  2.4771e-01,\n",
       "                       4.5562e-01,  1.1279e-01,  2.5142e-01,  2.7878e-01,  2.7136e-01,\n",
       "                       2.8722e-01,  2.5752e-01,  2.7744e-01,  2.4904e-01,  2.4285e-01,\n",
       "                       4.9442e-01,  4.9759e-01,  4.4800e-01,  6.9259e-01,  5.7073e-01,\n",
       "                       3.0924e-01,  2.3925e-01,  4.8547e-01,  2.5942e-01,  2.7789e-01,\n",
       "                       4.8323e-01,  5.1313e-01,  2.5167e-01,  3.8080e-01,  3.7765e-01,\n",
       "                       4.0972e-01,  2.2847e-01,  2.1621e-01,  3.3777e-01,  6.0130e-01,\n",
       "                       2.5962e-01,  2.9423e-01,  2.2711e-01,  2.0186e-01,  2.5364e-01,\n",
       "                       2.0927e-01,  4.0061e-01,  2.3595e-01,  2.5337e-01,  2.7516e-01,\n",
       "                       3.2154e-01,  2.8560e-01,  4.9318e-01,  4.0833e-01,  2.5587e-01,\n",
       "                       2.2956e-01,  3.1881e-01,  2.7461e-01,  3.2811e-01,  3.3415e-01,\n",
       "                       2.2765e-01,  3.7841e-01,  2.4872e-01,  4.2670e-01,  3.5404e-01,\n",
       "                       2.8340e-01,  3.7921e-01,  4.7498e-01,  3.6700e-01,  2.2215e-01,\n",
       "                       2.2740e-01,  3.5836e-01,  2.9459e-01,  2.2589e-01,  4.1700e-01,\n",
       "                       2.3164e-01,  3.7373e-01,  4.2355e-01,  3.5130e-01,  2.3264e-01,\n",
       "                       2.2945e-01,  4.4319e-01,  4.0424e-01,  2.8937e-01,  2.5385e-01,\n",
       "                       2.5348e-01,  2.1823e-01,  2.7262e-01,  3.1371e-01,  5.8882e-01,\n",
       "                       3.1694e-01,  3.1075e-01,  3.2651e-01,  3.0774e-01,  2.4749e-01,\n",
       "                       3.2654e-01,  3.7453e-01,  3.0765e-01,  4.9843e-01,  5.2234e-01,\n",
       "                       3.3116e-01,  1.9681e-01,  2.7478e-01,  1.9821e-01,  3.5664e-01,\n",
       "                       3.2013e-01,  2.9724e-01,  5.1587e-01,  2.1948e-01,  3.4820e-01,\n",
       "                       2.1754e-01,  4.1049e-01,  6.2789e-01, -1.5151e-01,  2.0865e-01,\n",
       "                       2.4730e-01,  4.9433e-01,  3.1130e-01,  2.5644e-01,  2.8119e-01,\n",
       "                       3.2502e-01,  3.5084e-01,  5.4794e-03,  2.4391e-01,  3.6091e-01,\n",
       "                       2.3571e-01,  4.1529e-01,  2.9360e-01,  3.9214e-01,  6.0178e-01,\n",
       "                       2.9859e-01,  3.0990e-01,  4.8288e-01,  2.8331e-01,  2.6795e-01,\n",
       "                       5.4350e-01,  2.3359e-01,  4.9136e-01,  1.3698e-01,  3.7537e-01,\n",
       "                       2.1829e-01,  4.2039e-01,  2.9081e-01,  5.1952e-01,  3.6690e-01,\n",
       "                       2.7642e-01,  3.9245e-01,  2.0401e-01,  2.7730e-01,  2.6704e-01,\n",
       "                       2.1887e-01,  2.0374e-01,  1.8540e-01,  2.7675e-01,  2.2247e-01,\n",
       "                       2.3874e-01,  5.0513e-01,  2.1769e-01,  2.0869e-01,  2.4552e-01,\n",
       "                       2.4976e-01,  1.9570e-01,  2.9283e-01,  3.1616e-01,  2.0789e-01,\n",
       "                       3.3065e-01,  2.4261e-01,  2.3206e-01,  8.1808e-02,  3.9375e-01,\n",
       "                       6.5600e-01,  2.7095e-01,  1.7649e-01,  3.5933e-01,  2.6028e-01,\n",
       "                       2.7622e-01,  3.3874e-01,  2.5765e-01,  3.4761e-01,  3.9659e-01,\n",
       "                       4.2949e-01,  2.3976e-01,  4.4334e-01,  2.0974e-01,  3.9463e-01,\n",
       "                       2.6009e-01,  3.0044e-01,  2.4662e-01,  3.5242e-01,  4.4945e-01,\n",
       "                       3.1412e-01,  4.5655e-01,  3.7642e-01,  4.0458e-01,  2.3054e-01,\n",
       "                       3.3162e-01,  1.9439e-01,  3.4088e-01,  5.5561e-01,  2.9785e-01,\n",
       "                       2.7346e-01,  1.9829e-01,  2.9601e-01,  2.0675e-01,  2.8755e-01,\n",
       "                       3.7358e-01,  4.8865e-01,  3.2799e-01,  4.0102e-01,  2.5130e-01,\n",
       "                       2.3941e-01, -1.9594e-01,  2.4142e-01,  3.2032e-01,  3.5118e-01,\n",
       "                       2.8793e-01, -3.1482e-01,  2.9402e-01,  2.3456e-01,  2.0822e-01,\n",
       "                       2.2452e-01,  2.1377e-01,  1.7100e-01,  1.9453e-01,  3.4306e-01,\n",
       "                       2.3921e-01,  2.1858e-01,  2.5515e-01,  3.9136e-01,  2.5753e-01,\n",
       "                       3.7895e-01,  5.4013e-01,  2.5939e-01,  2.4731e-01,  6.4976e-01,\n",
       "                       2.9569e-01,  2.5579e-01,  3.5147e-01,  2.0910e-01,  3.2004e-01,\n",
       "                       2.5178e-01,  3.2273e-01,  4.2461e-01,  2.1087e-01,  3.3040e-01,\n",
       "                       6.8894e-01,  2.8743e-01,  3.1989e-01,  2.3096e-01,  2.2340e-01,\n",
       "                       2.2977e-01,  4.3202e-01,  3.4330e-01,  4.3735e-01,  2.7484e-01,\n",
       "                       2.9662e-01,  3.8918e-01,  4.2514e-01,  2.3972e-01,  3.6972e-01,\n",
       "                       3.4135e-01,  1.9138e-01,  6.9337e-01,  2.1593e-01,  4.2617e-01,\n",
       "                       2.2069e-01,  2.9495e-01,  3.2958e-01,  6.0863e-01,  3.2955e-01,\n",
       "                       2.1662e-01,  3.0907e-01,  2.4164e-01,  2.9846e-01,  2.4859e-01,\n",
       "                       5.5668e-01,  3.7022e-01,  4.3579e-01,  2.5705e-01,  3.0904e-01,\n",
       "                       4.1385e-01,  2.5729e-01,  2.8975e-01,  3.0129e-01,  3.2336e-01,\n",
       "                       2.9239e-01,  3.4388e-01,  2.4911e-01,  4.0250e-01,  3.1256e-01,\n",
       "                       3.4723e-01,  2.2739e-01,  3.8351e-01,  3.7352e-01,  2.5817e-01,\n",
       "                       2.5303e-01,  3.5434e-01,  2.6593e-01,  2.5162e-01,  4.5267e-01,\n",
       "                       5.1510e-01,  4.7107e-01,  2.5833e-01,  7.1206e-01,  2.2384e-01,\n",
       "                       2.6525e-01,  3.9425e-01,  2.7828e-01,  3.2753e-01,  4.0310e-01,\n",
       "                       3.7026e-01,  2.5940e-01,  4.6358e-01,  4.0729e-01,  2.8745e-01,\n",
       "                       3.8733e-01,  1.0705e-01,  3.2499e-01,  3.4348e-01,  2.3843e-01,\n",
       "                       2.9346e-01,  1.0865e-01,  2.3109e-01,  2.9863e-01,  2.5620e-01,\n",
       "                       3.1489e-01,  2.5868e-01,  3.3216e-01,  2.5874e-01,  6.7012e-01,\n",
       "                       2.9294e-01,  2.6743e-01,  2.6410e-01,  2.8985e-01,  2.0936e-01,\n",
       "                       1.5736e-01,  2.2384e-01,  2.4300e-01,  2.2554e-01,  2.9802e-01,\n",
       "                       2.6632e-01,  3.5146e-01,  2.0106e-01,  3.1420e-01,  3.2789e-01,\n",
       "                       2.6080e-01,  3.8707e-01,  2.9540e-01,  5.9065e-01,  2.4815e-01,\n",
       "                       3.2698e-01,  2.5639e-01,  2.9558e-01,  3.6731e-01,  3.5567e-01,\n",
       "                       2.0705e-01,  2.5076e-01,  3.9011e-01,  3.3813e-01,  2.3844e-01,\n",
       "                       2.9209e-01,  2.1171e-01,  3.3467e-01,  2.3283e-01,  2.7821e-01,\n",
       "                       3.7085e-01, -2.3019e-01,  4.3039e-01,  2.6557e-01,  2.6644e-01,\n",
       "                       3.4491e-01, -2.9252e-01,  2.7576e-01,  4.7235e-01,  2.9344e-01,\n",
       "                       2.6242e-01,  3.2900e-01,  4.1428e-01,  2.7287e-01,  4.2075e-01,\n",
       "                       3.5439e-01,  2.6782e-01,  3.7831e-01,  5.9099e-01,  2.0958e-01,\n",
       "                       4.9675e-01,  3.0923e-01,  2.9024e-01,  3.7678e-01,  2.5313e-01,\n",
       "                       4.5333e-01,  3.2765e-01,  2.5480e-01,  2.5150e-01,  3.1373e-01,\n",
       "                       3.3639e-01,  4.3961e-01,  2.6665e-01,  2.7653e-01,  2.8288e-01,\n",
       "                       2.1579e-01,  2.2762e-01,  2.6344e-01,  3.1237e-01,  3.8286e-01,\n",
       "                       5.7623e-01,  2.7520e-01,  3.2171e-01,  3.4000e-01,  4.0682e-01,\n",
       "                       3.6099e-01,  7.4197e-02,  2.6086e-01,  4.8963e-01,  2.5277e-01,\n",
       "                       3.8186e-01,  2.7680e-01,  2.1774e-01,  2.6981e-01,  2.8164e-01,\n",
       "                       2.7841e-01,  3.6236e-01,  2.6145e-01,  2.3896e-01,  2.5130e-01,\n",
       "                       2.0356e-01,  2.8031e-01,  2.5132e-01,  3.0481e-01,  2.2977e-01,\n",
       "                       2.7933e-01,  2.0204e-01,  3.0476e-01,  2.2003e-01,  2.5905e-01,\n",
       "                       5.1784e-01,  2.4312e-01,  3.3759e-01,  2.2442e-01,  2.7600e-01,\n",
       "                       1.7882e-01,  3.1425e-01,  6.2041e-01,  2.1458e-01,  3.0084e-01,\n",
       "                       2.9094e-01,  3.3548e-01,  2.6869e-01,  2.5560e-01,  3.1066e-01,\n",
       "                       3.3147e-01,  1.9338e-01,  2.9862e-01,  2.7771e-01,  3.5040e-01,\n",
       "                       2.1704e-01,  2.3963e-01,  5.5516e-01,  4.6320e-01,  2.3144e-01,\n",
       "                       8.2364e-01,  2.7570e-01,  3.0562e-01,  2.2918e-01,  3.0032e-01,\n",
       "                       3.5518e-01,  2.9280e-01,  3.7328e-01,  2.5053e-01,  4.4110e-01,\n",
       "                       2.3634e-01, -3.8449e-01,  4.9998e-01,  2.6423e-01,  2.8192e-01,\n",
       "                       4.9701e-01,  2.9305e-01,  2.0656e-01,  2.3076e-01,  3.3409e-01,\n",
       "                       3.3365e-01,  1.9596e-01,  2.7846e-01, -2.1628e-01,  3.1717e-01,\n",
       "                       3.0045e-01,  3.0323e-01,  2.5880e-01,  2.9610e-01,  3.2760e-01,\n",
       "                       3.2668e-01,  5.1732e-01,  3.2914e-01,  3.7976e-01,  1.9954e-01,\n",
       "                       4.3402e-01,  3.1164e-01,  2.1028e-01,  3.2919e-01,  4.5510e-01,\n",
       "                       2.7897e-01,  2.6003e-01,  4.9897e-01,  3.2103e-01,  3.6814e-01,\n",
       "                       2.6632e-01,  3.6323e-01,  2.6598e-01,  3.3615e-01,  3.4405e-01,\n",
       "                       3.3637e-01,  2.0853e-01,  3.0913e-01,  1.7334e-01,  3.3126e-01,\n",
       "                       3.2890e-01,  2.3897e-01,  3.8499e-01], device='cuda:0')),\n",
       "             ('encoder.block.2.layer.0.SelfAttention.q.weight',\n",
       "              tensor([[-0.0385,  0.0228, -0.0921,  ..., -0.0968,  0.0837,  0.0647],\n",
       "                      [-0.1300,  0.0292, -0.0104,  ..., -0.0152, -0.0076,  0.1469],\n",
       "                      [ 0.0123, -0.0678,  0.0904,  ...,  0.0647, -0.0528,  0.0216],\n",
       "                      ...,\n",
       "                      [ 0.0257, -0.0325, -0.0894,  ..., -0.0250, -0.0353,  0.0716],\n",
       "                      [ 0.0685, -0.0202,  0.0442,  ..., -0.0205, -0.0767,  0.0061],\n",
       "                      [-0.1323, -0.0682, -0.0991,  ..., -0.1609,  0.0399,  0.0700]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.2.layer.0.SelfAttention.k.weight',\n",
       "              tensor([[ 0.0630,  0.9948, -1.3439,  ...,  0.1494,  1.7999,  0.0438],\n",
       "                      [-0.9037,  0.8614,  0.7457,  ...,  0.3972, -0.2145,  1.1656],\n",
       "                      [-0.4125, -0.9006, -0.0986,  ...,  0.4313, -0.0098,  0.1414],\n",
       "                      ...,\n",
       "                      [-0.2116,  0.2695,  0.4744,  ...,  0.3844,  0.5021,  0.0782],\n",
       "                      [-0.2501, -0.6808,  0.5082,  ...,  0.0080, -0.2193, -0.2888],\n",
       "                      [-0.4644, -0.5700, -0.0747,  ...,  0.8021, -0.1671,  0.3788]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.2.layer.0.SelfAttention.v.weight',\n",
       "              tensor([[-0.3182, -1.0483, -0.8639,  ...,  0.1614, -0.5490,  0.1581],\n",
       "                      [ 0.5030,  0.1299, -0.1190,  ..., -0.6724,  0.4136, -0.2786],\n",
       "                      [-0.6464, -0.7274,  0.1456,  ..., -0.0764, -0.1027, -0.9864],\n",
       "                      ...,\n",
       "                      [-0.4863,  1.2117,  0.3903,  ...,  1.3048,  0.1170, -1.9328],\n",
       "                      [ 1.0771, -0.0453, -0.3987,  ..., -1.2992, -1.7295,  0.6012],\n",
       "                      [ 2.2603, -0.8518,  1.2628,  ...,  0.7941, -0.8288, -0.8105]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.2.layer.0.SelfAttention.o.weight',\n",
       "              tensor([[ 0.1600, -0.8170, -0.1085,  ...,  0.4994, -0.0988, -1.2165],\n",
       "                      [-0.8688,  1.1293, -0.0684,  ..., -1.0937,  0.0766, -0.2785],\n",
       "                      [-0.2852,  0.3741,  0.4557,  ..., -0.5273,  0.9811, -3.1092],\n",
       "                      ...,\n",
       "                      [ 0.4380,  0.1482, -0.1254,  ...,  0.5154,  1.2570, -0.6960],\n",
       "                      [-0.4080,  0.0903, -0.0437,  ..., -0.0141,  1.0381,  1.4326],\n",
       "                      [ 0.7583, -0.2285, -0.2120,  ...,  2.0184, -2.4350,  2.2288]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.2.layer.0.layer_norm.weight',\n",
       "              tensor([ 0.0945,  0.0906,  0.1001,  0.1016,  0.0779,  0.0781,  0.0333,  0.1098,\n",
       "                       0.0464,  0.0857,  0.0954,  0.1044,  0.0701,  0.0960,  0.0741,  0.0929,\n",
       "                       0.0939,  0.1046,  0.0931,  0.0712,  0.0760,  0.0840,  0.0003,  0.0711,\n",
       "                       0.0683,  0.0855,  0.0727,  0.0794,  0.0964,  0.0777,  0.0755,  0.0722,\n",
       "                       0.0842,  0.0783,  0.0982,  0.0834,  0.0575,  0.1225,  0.0705,  0.1263,\n",
       "                       0.0883,  0.0976,  0.1137,  0.0986,  0.0215,  0.0716,  0.0700,  0.0728,\n",
       "                       0.0949,  0.0734,  0.1169,  0.0806,  0.0756,  0.0776,  0.0093,  0.0749,\n",
       "                       0.0249,  0.0832,  0.0742,  0.0830,  0.0869,  0.0591,  0.0903,  0.0701,\n",
       "                       0.0661,  0.0358,  0.0847,  0.0836,  0.1020,  0.0888,  0.0916,  0.1053,\n",
       "                       0.0737,  0.0790,  0.0676,  0.0691,  0.0799,  0.0945,  0.0773,  0.0923,\n",
       "                       0.0825,  0.1055,  0.0809,  0.0993,  0.0804,  0.0842,  0.0792,  0.0965,\n",
       "                       0.0917,  0.0730,  0.0906,  0.0679,  0.0721,  0.0868,  0.0600,  0.0777,\n",
       "                       0.0884,  0.0721,  0.0819,  0.0686,  0.1088,  0.0795,  0.0860,  0.0673,\n",
       "                       0.0867,  0.0982,  0.0682,  0.0831,  0.0739,  0.0881,  0.0922,  0.0799,\n",
       "                       0.0799,  0.0809,  0.0910,  0.0753,  0.0981,  0.0791,  0.0952,  0.0627,\n",
       "                       0.0641,  0.0886,  0.0959,  0.0819,  0.1125,  0.0787,  0.0921,  0.0935,\n",
       "                       0.0700,  0.0973,  0.0496,  0.0951,  0.0703,  0.0756,  0.0752,  0.0868,\n",
       "                       0.0980,  0.0724,  0.0831,  0.0845,  0.0714,  0.0980,  0.0983,  0.0866,\n",
       "                       0.0893,  0.0924,  0.0861,  0.0838,  0.0746,  0.0822,  0.0803,  0.0977,\n",
       "                       0.0962,  0.1133,  0.0851,  0.0767,  0.0984,  0.0791,  0.0617,  0.0651,\n",
       "                       0.0646,  0.0753,  0.0546,  0.0334,  0.0602,  0.0697,  0.0763,  0.0924,\n",
       "                       0.0743,  0.1045,  0.0998,  0.0852,  0.0865,  0.0802,  0.0868,  0.1040,\n",
       "                       0.0875,  0.1390,  0.0804, -0.0012,  0.0769,  0.0820,  0.0854,  0.0722,\n",
       "                       0.0949,  0.0927,  0.0817,  0.1146,  0.0906,  0.0802,  0.0609,  0.0798,\n",
       "                       0.0421,  0.0841,  0.0712,  0.0484,  0.0737,  0.0917,  0.0810,  0.0649,\n",
       "                       0.0663,  0.0861,  0.0955,  0.0795,  0.0801,  0.0981,  0.0803,  0.0864,\n",
       "                       0.0801,  0.0831,  0.0850,  0.0750,  0.0837,  0.0906,  0.0767,  0.0852,\n",
       "                       0.1020,  0.0950,  0.0909,  0.0437,  0.0601,  0.0837,  0.0600,  0.0915,\n",
       "                       0.0827,  0.0823,  0.0869,  0.1098,  0.0776,  0.1070,  0.0617,  0.0754,\n",
       "                       0.0515,  0.0665,  0.0619,  0.0293,  0.0237,  0.0973,  0.0726,  0.1206,\n",
       "                       0.0848,  0.0923,  0.0774,  0.0812,  0.0561,  0.0832,  0.0873,  0.0868,\n",
       "                       0.0606,  0.0872,  0.0498,  0.0689,  0.0731,  0.0614,  0.0674,  0.0740,\n",
       "                       0.0817,  0.0673,  0.0604,  0.0731,  0.0806,  0.1201,  0.0802,  0.0867,\n",
       "                       0.0767,  0.0683,  0.0958,  0.0794,  0.0819,  0.0812,  0.0333,  0.0973,\n",
       "                       0.0932,  0.0127,  0.1036,  0.0936,  0.0768,  0.0961,  0.0805,  0.0682,\n",
       "                       0.1025,  0.0944,  0.0822,  0.0726,  0.1035,  0.0835,  0.0624,  0.0764,\n",
       "                       0.0776,  0.0588,  0.0675,  0.0848,  0.0657,  0.0846,  0.0762,  0.0656,\n",
       "                       0.0947,  0.0904,  0.0696,  0.1040,  0.1116,  0.0858,  0.1212,  0.0898,\n",
       "                       0.0927,  0.0744,  0.0896,  0.0719,  0.0854,  0.0948,  0.0748,  0.1060,\n",
       "                       0.0767,  0.0617,  0.0928,  0.0834,  0.0625,  0.1158,  0.0631,  0.0694,\n",
       "                       0.0772,  0.1074,  0.0880,  0.0718,  0.0984,  0.0730,  0.0780,  0.1209,\n",
       "                       0.0826,  0.0876,  0.0830,  0.0800,  0.0871,  0.0780,  0.0670,  0.0831,\n",
       "                       0.0689,  0.0816,  0.0891,  0.1174,  0.0840,  0.0995,  0.0946,  0.0869,\n",
       "                       0.0883,  0.0761,  0.0917,  0.0779,  0.1010,  0.0544,  0.1101,  0.0736,\n",
       "                       0.0931,  0.0798,  0.1008,  0.0822,  0.0826,  0.1090,  0.0775,  0.1113,\n",
       "                       0.0674,  0.1101,  0.0647,  0.0546,  0.0650,  0.0900,  0.0887,  0.0835,\n",
       "                       0.0927,  0.0693,  0.0909,  0.1085,  0.0013,  0.0639,  0.0996,  0.0794,\n",
       "                       0.1152,  0.0746,  0.1245,  0.0534,  0.0782,  0.0962,  0.1242,  0.0789,\n",
       "                       0.0786,  0.0406,  0.0698,  0.1083,  0.0473,  0.0965,  0.0819,  0.0798,\n",
       "                       0.0921,  0.1047,  0.1022,  0.0944,  0.0930,  0.0811,  0.0907,  0.0773,\n",
       "                       0.0729,  0.0116,  0.0634,  0.0905,  0.0848,  0.0832,  0.1173,  0.0784,\n",
       "                       0.0753,  0.0763,  0.0794,  0.0768,  0.0887,  0.0812,  0.0841,  0.0818,\n",
       "                       0.0775,  0.0760,  0.0309,  0.0905,  0.0630,  0.0790,  0.0622,  0.0519,\n",
       "                       0.0912,  0.0800,  0.0968,  0.0824,  0.0885,  0.0847,  0.1105,  0.0895,\n",
       "                       0.0799,  0.0770,  0.0933,  0.0768,  0.0720,  0.0781,  0.1189,  0.1290,\n",
       "                       0.0999,  0.0893,  0.1084,  0.1087,  0.0798,  0.0793,  0.0708,  0.0926,\n",
       "                       0.0935,  0.0946,  0.0860,  0.0766,  0.1064,  0.0827,  0.1053,  0.0942,\n",
       "                       0.0818,  0.0706,  0.0910,  0.0754,  0.0784,  0.0767,  0.0858,  0.0965,\n",
       "                       0.0717,  0.0868,  0.0578,  0.0701,  0.0644,  0.0757,  0.0677,  0.0583,\n",
       "                       0.0583,  0.0792,  0.0806,  0.0891,  0.0707,  0.0686,  0.0961,  0.0863,\n",
       "                       0.1044,  0.0742,  0.0927,  0.0793,  0.1077,  0.0941,  0.0781,  0.0825,\n",
       "                       0.0549,  0.0937,  0.0759,  0.0892,  0.0549,  0.0858,  0.0796,  0.0681,\n",
       "                       0.0993,  0.0790,  0.0851,  0.0788,  0.0709,  0.0965,  0.0892,  0.0227,\n",
       "                       0.0781,  0.0866,  0.0952,  0.1032,  0.0676,  0.0976,  0.1007,  0.0772,\n",
       "                       0.0446,  0.0609,  0.0872,  0.0866,  0.0715,  0.0665,  0.0732,  0.1023,\n",
       "                       0.0679,  0.0735,  0.0632,  0.0870,  0.0956,  0.0630,  0.0969,  0.1195,\n",
       "                       0.0780,  0.0906,  0.1088,  0.0821,  0.0891,  0.0723,  0.0834,  0.0891,\n",
       "                       0.0889,  0.0828,  0.1038,  0.0938,  0.0911,  0.0805,  0.0904,  0.0996,\n",
       "                       0.0823,  0.0799,  0.0960,  0.0975,  0.0797,  0.0874,  0.0884,  0.1088,\n",
       "                       0.0607,  0.0508,  0.0720,  0.0964,  0.0843,  0.0826,  0.0733,  0.1002,\n",
       "                       0.0917,  0.0595,  0.1010,  0.0971,  0.1056,  0.1032,  0.0110,  0.0794,\n",
       "                       0.0802,  0.0827,  0.0919,  0.0308,  0.0662,  0.0772,  0.0771,  0.0863,\n",
       "                       0.0847,  0.0983,  0.0876,  0.0816,  0.0794,  0.0701,  0.0846,  0.0878,\n",
       "                       0.0642,  0.0819,  0.0774,  0.0666,  0.0643,  0.0780,  0.0785,  0.0906,\n",
       "                       0.0797,  0.0932,  0.0811,  0.0826,  0.0873,  0.0824,  0.1018,  0.0854,\n",
       "                       0.1010,  0.0807,  0.0909,  0.0574,  0.0892,  0.0620,  0.0656,  0.0856,\n",
       "                       0.1118,  0.0975,  0.0977,  0.0820,  0.0904,  0.0727,  0.0795,  0.0945,\n",
       "                       0.0712,  0.0797,  0.0836,  0.0785,  0.0884,  0.0680,  0.0811,  0.1029,\n",
       "                       0.0765,  0.0891,  0.0722,  0.0765,  0.0749,  0.0889,  0.0987,  0.0849,\n",
       "                       0.0800,  0.0926,  0.0702,  0.0897,  0.0845,  0.0922,  0.1027,  0.0859,\n",
       "                       0.0723,  0.0959,  0.0757,  0.0925,  0.0914,  0.0972,  0.0770,  0.0748,\n",
       "                       0.0748,  0.0773,  0.0741,  0.0747,  0.0786,  0.0775,  0.0957,  0.0842,\n",
       "                       0.0931,  0.0707,  0.0868,  0.1091,  0.1011, -0.0018,  0.0685,  0.1262,\n",
       "                       0.0686,  0.0926,  0.0759,  0.0738,  0.1000,  0.0840,  0.0826,  0.0757,\n",
       "                       0.0834,  0.0633,  0.0831,  0.0864,  0.0898,  0.0794,  0.0874,  0.0861,\n",
       "                       0.0825,  0.0900,  0.0864,  0.0577,  0.0834,  0.0457,  0.0762,  0.0819,\n",
       "                       0.0698,  0.1023,  0.0733,  0.0989,  0.1203,  0.0687,  0.0843,  0.0817,\n",
       "                       0.1009,  0.0722,  0.0842,  0.0895,  0.0875,  0.0544,  0.0867,  0.0660,\n",
       "                       0.0801,  0.0720,  0.0663,  0.0782,  0.1219,  0.0678,  0.0218,  0.0863,\n",
       "                       0.0823,  0.0804,  0.0326,  0.0962,  0.0898,  0.0996,  0.0869,  0.1078,\n",
       "                       0.0752,  0.0638,  0.1044,  0.0664,  0.0880,  0.0776,  0.0925,  0.0652,\n",
       "                       0.0867,  0.0992,  0.0908,  0.0835,  0.0722,  0.0563,  0.0798,  0.0904,\n",
       "                       0.0844,  0.0874,  0.0861,  0.1081,  0.0825,  0.0867,  0.0938,  0.0921,\n",
       "                       0.0818,  0.0924,  0.0868,  0.0604,  0.0864,  0.0932,  0.0837,  0.0652,\n",
       "                       0.0979,  0.0835,  0.0943,  0.0808,  0.0887,  0.0715,  0.0799,  0.1059,\n",
       "                       0.0811,  0.0590,  0.0822,  0.0788,  0.0901,  0.0827,  0.0841,  0.1076],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.2.layer.1.DenseReluDense.wi.weight',\n",
       "              tensor([[ 0.4509,  1.0684,  0.2039,  ...,  0.2057, -1.2752, -1.0696],\n",
       "                      [-1.4159,  1.3043,  0.0419,  ...,  1.0079,  0.1739,  1.8565],\n",
       "                      [ 0.2210, -0.0407, -1.2912,  ..., -1.3362,  1.3017, -0.9630],\n",
       "                      ...,\n",
       "                      [ 0.2344, -0.9511,  1.3722,  ...,  0.7778,  1.8543, -1.3593],\n",
       "                      [ 1.7871, -0.3095,  1.1557,  ...,  0.8027, -0.0590,  0.5667],\n",
       "                      [ 0.1265, -0.3619, -1.6369,  ..., -0.2075, -0.0659,  0.1563]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.2.layer.1.DenseReluDense.wo.weight',\n",
       "              tensor([[ 0.0813,  0.1217,  0.3148,  ..., -0.4216, -0.4640,  0.2070],\n",
       "                      [-0.1564,  0.4117,  0.2663,  ..., -0.5912, -0.1296,  0.5459],\n",
       "                      [ 0.0694, -0.3403, -0.2539,  ..., -0.5979,  0.1114, -0.6999],\n",
       "                      ...,\n",
       "                      [ 0.6203,  0.0663, -0.0433,  ..., -0.1221, -0.3794,  0.2262],\n",
       "                      [ 0.1961, -0.2512, -0.3207,  ...,  0.0037,  0.1671, -0.6673],\n",
       "                      [ 0.0974, -0.3481, -0.5978,  ...,  0.0991,  0.1456,  0.3545]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.2.layer.1.layer_norm.weight',\n",
       "              tensor([ 0.5189,  0.6113,  0.5902,  0.5734,  0.3765,  0.4928,  0.2703,  0.5466,\n",
       "                       1.3959,  0.3811,  0.5298,  0.4657,  0.4560,  0.7053,  0.3914,  0.4975,\n",
       "                       0.5590,  0.5264,  0.6157,  0.3542,  0.4493,  0.4854,  0.7198,  0.3711,\n",
       "                       0.3495,  0.5838,  0.3687,  0.6208,  0.5419,  0.4229, -0.4805,  0.4496,\n",
       "                       0.5532,  0.3203,  0.6250,  0.4214,  0.7711,  0.7654,  0.2846,  0.6521,\n",
       "                       0.5573,  0.7458,  0.6124,  0.4158,  1.5831,  0.4091,  0.8032,  0.4891,\n",
       "                       0.4532,  0.7656,  0.6743,  0.4983,  0.8371,  0.3494,  0.4665,  0.5631,\n",
       "                       0.6796,  0.5091,  0.3094,  0.4126,  0.4287,  0.3243,  0.5122,  0.3682,\n",
       "                       0.3948,  0.9593,  0.4600,  0.4367,  0.7836,  0.3670,  0.4870,  0.6519,\n",
       "                       0.3572,  0.4385,  0.3778,  0.3417,  0.7194,  0.5138,  0.4252,  0.4737,\n",
       "                       0.3706,  0.5570,  0.4189,  0.7801,  0.4157,  0.4395,  0.7254,  0.4681,\n",
       "                       0.4400,  0.3979,  0.5207,  0.3128,  0.4649,  0.5118,  0.3665,  0.3411,\n",
       "                       0.4032,  0.4304,  0.3580,  0.4218,  0.6058,  0.4377,  0.7628,  0.3799,\n",
       "                       0.5071,  0.8562,  0.4315,  0.4735,  0.4084,  0.7127,  0.4307,  0.3619,\n",
       "                       0.4560,  0.4804,  0.4179,  0.4304,  0.4313,  0.3947,  0.9772,  0.4439,\n",
       "                       0.2789,  0.3770,  0.7127,  0.3648,  0.7034,  0.4726,  0.4260,  0.4860,\n",
       "                       0.4364,  0.5899,  0.9544,  0.5601,  0.3128,  0.3915,  0.4160,  0.8953,\n",
       "                       0.6447,  0.4737,  0.5567,  0.5015,  0.4427,  0.5945,  0.7458,  0.8438,\n",
       "                       0.5396,  0.6957,  0.3736,  0.6112,  0.4044,  0.5081,  0.4155,  0.4542,\n",
       "                       0.4910,  0.7469,  0.3633,  0.3254,  0.6453,  0.3183,  0.6531, -0.4741,\n",
       "                       0.3719,  0.4423,  0.4362,  1.3000,  0.3713,  0.3449,  0.4469,  0.5909,\n",
       "                       0.3938,  0.6124,  0.7666,  0.3798,  0.3871,  0.8500,  0.4459,  0.5597,\n",
       "                       0.5059,  0.7105,  0.7513,  0.2434,  0.4103,  0.4053,  0.4581, -0.3377,\n",
       "                       0.5748,  0.6119,  0.4340,  0.6546,  0.5047,  0.5183,  0.3932,  0.3929,\n",
       "                       0.3338,  0.4693,  0.3931,  0.0979,  0.3986,  0.6292,  0.4132,  0.3516,\n",
       "                       0.3188,  0.5805,  0.4852,  0.4139,  0.4016,  0.7715,  0.5004,  0.3754,\n",
       "                       0.4017,  0.4146,  0.7039,  0.3951,  0.6070,  0.5919,  0.3950,  0.4687,\n",
       "                       0.4972,  0.4635,  0.5819,  1.1463,  0.6025,  0.4760,  0.4039,  0.4954,\n",
       "                       0.6318,  0.4321,  0.5090,  0.6136,  0.4574,  0.5805,  0.4266,  0.6070,\n",
       "                       0.4098,  0.3124,  0.3316,  0.1834,  0.0380,  0.5766,  0.6015,  0.6953,\n",
       "                       0.8221,  0.5179,  0.5089,  0.3421,  0.3636,  0.3874,  0.4409,  0.5807,\n",
       "                       0.3277,  0.4896,  1.2218,  0.4162,  0.4498,  0.3947,  0.3626,  0.3046,\n",
       "                       0.3448,  0.4795,  0.7622,  0.5176,  0.6874,  0.0265,  0.4227,  0.4321,\n",
       "                       0.4450,  0.5382,  0.4988,  0.3649,  0.4338,  0.4333,  0.6345,  0.7385,\n",
       "                       0.7873,  1.0952,  0.8105,  0.5186,  0.4681,  0.7936,  0.4609,  0.5262,\n",
       "                       0.7353,  0.7724,  0.3875,  0.6464,  0.5189,  0.6862,  0.3833,  0.4183,\n",
       "                       0.5376,  0.9168,  0.4273,  0.5118,  0.3092,  0.2805,  0.4237,  0.3662,\n",
       "                       0.6602,  0.3923,  0.3392,  0.5355,  0.5264,  0.3723,  0.8763,  0.5065,\n",
       "                       0.4399,  0.3608,  0.4553,  0.3734,  0.5013,  0.5669,  0.4687,  0.6459,\n",
       "                       0.4095,  0.5657,  0.5673,  0.5217,  0.4261,  0.6372,  0.5447,  0.3723,\n",
       "                       0.3066,  0.5964,  0.5557,  0.3137,  0.6179,  0.4069,  0.6389,  0.7132,\n",
       "                       0.5107,  0.4539,  0.3537,  0.7273,  0.6379,  0.4224,  0.3802,  0.3851,\n",
       "                       0.4113,  0.3921,  0.5435,  0.7650,  0.4760,  0.5204,  0.5415,  0.4357,\n",
       "                       0.4461,  0.5485,  0.6679,  0.5237,  0.8174,  0.8592,  0.5718,  0.3586,\n",
       "                       0.3950,  0.4258,  0.5926,  0.4049,  0.5428,  0.7839,  0.3350,  0.6098,\n",
       "                       0.3884,  0.6638,  0.9240,  0.1797,  0.4112,  0.4570,  0.7676,  0.5440,\n",
       "                       0.4760,  0.5094,  0.5682,  0.5179, -0.0110,  0.4100,  0.5595,  0.3165,\n",
       "                       0.6990,  0.4397,  0.6566,  0.7752,  0.4086,  0.6017,  0.6665,  0.4717,\n",
       "                       0.4180,  0.7882,  0.4756,  0.7068,  0.2114,  0.7012,  0.3501,  0.7416,\n",
       "                       0.4120,  0.6909,  0.6444,  0.5442,  0.6447,  0.3854,  0.4357,  0.3919,\n",
       "                       0.4060,  0.3940,  0.3611,  0.3755,  0.4235,  0.3895,  0.8248,  0.3531,\n",
       "                       0.3418,  0.3307,  0.3544,  0.2895,  0.4936,  0.5205,  0.3696,  0.5143,\n",
       "                       0.3614,  0.3003,  0.0394,  0.6704,  0.8212,  0.4828,  0.3474,  0.4068,\n",
       "                       0.4714,  0.3952,  0.5570,  0.3717,  0.5567,  0.5482,  0.6132,  0.3421,\n",
       "                       0.7827,  0.4019,  0.7184,  0.3141,  0.4397,  0.3507,  0.5574,  0.6254,\n",
       "                       0.5368,  0.7420,  0.6014,  0.6467,  0.3893,  0.5837,  0.3867,  0.5527,\n",
       "                       0.7109,  0.4455,  0.4271,  0.3605,  0.3837,  0.3781,  0.5342,  0.6448,\n",
       "                       0.7430,  0.4000,  0.5297,  0.4231,  0.3961,  0.4345,  0.4579,  0.4335,\n",
       "                       0.4503,  0.4994,  0.3814,  0.4448,  0.3350,  0.3293,  0.3485,  0.4052,\n",
       "                       0.1953,  0.3074,  0.4868,  0.3651,  0.3969,  0.4259,  0.6040,  0.4101,\n",
       "                       0.6474,  1.0014,  0.5002,  0.4208,  0.8048,  0.3639,  0.3650,  0.5562,\n",
       "                       0.3660,  0.3528,  0.4799,  0.5056,  0.7471,  0.3030,  0.5034,  0.9071,\n",
       "                       0.4913,  0.5218,  0.4358,  0.2584,  0.3930,  0.6247,  0.6219,  0.7999,\n",
       "                       0.4805,  0.6552,  0.5464,  0.5988,  0.2679,  0.5891,  0.5371,  0.2923,\n",
       "                       1.2261,  0.3446,  0.5873,  0.3793,  0.4346,  0.5697,  0.9603,  0.4955,\n",
       "                       0.3817,  0.4617,  0.3281,  0.4642,  0.4262,  0.8508,  0.6038,  0.7112,\n",
       "                       0.4890,  0.5369,  0.6066,  0.4920,  0.4415,  0.4867,  0.5782,  0.5186,\n",
       "                       0.5360,  0.4417,  0.7454,  0.5259,  0.5737,  0.3923,  0.5618,  0.6630,\n",
       "                       0.4256,  0.3851,  0.5399,  0.3620,  0.4168,  0.6832,  0.6284,  0.6708,\n",
       "                       0.3476,  0.9826,  0.4095,  0.4286,  0.5235,  0.4218,  0.5440,  0.6622,\n",
       "                       0.5047,  0.3762,  0.8069,  0.7914,  0.5038,  0.5954,  0.1481,  0.4753,\n",
       "                       0.5627,  0.3128,  0.5904,  0.1203,  0.3966,  0.4748,  0.4372,  0.5735,\n",
       "                       0.4186,  0.6539,  0.4736,  1.0478,  0.5646,  0.3684,  0.4110,  0.3650,\n",
       "                       0.3303,  0.3151,  0.3785,  0.4557,  0.4089,  0.4066,  0.4719,  0.5946,\n",
       "                       0.4099,  0.4872,  0.4432,  0.4894,  0.5021,  0.4569,  0.8971,  0.4989,\n",
       "                       0.6123,  0.3357,  0.5259,  0.3266,  0.5292,  0.3580,  0.3739,  0.5428,\n",
       "                       0.6473,  0.4949,  0.4781,  0.3669,  0.5629,  0.4351,  0.4422,  0.5816,\n",
       "                       0.4264,  0.5802,  0.3835,  0.4718,  0.4928,  0.4408,  0.4515,  0.6259,\n",
       "                       0.5408,  0.4255,  0.5304,  0.7133,  0.3881,  0.6740,  0.6568,  0.4405,\n",
       "                       0.4826,  0.8670,  0.2968,  0.7796,  0.5706,  0.4746,  0.6152,  0.3987,\n",
       "                       0.6983,  0.4676,  0.4218,  0.5783,  0.5330,  0.4902,  0.7336,  0.2632,\n",
       "                       0.3696,  0.3394,  0.3880,  0.3939,  0.4897,  0.4948,  0.5805,  0.7942,\n",
       "                       0.3934,  0.5844,  0.4759,  0.6316,  0.6067,  0.0279,  0.4261,  0.7527,\n",
       "                       0.4521,  0.6555,  0.4335, -0.3007,  0.4249,  0.4367,  0.4934,  0.6844,\n",
       "                       0.4703,  0.4420,  0.4631,  0.3755,  0.4714,  0.3594,  0.5160,  0.4485,\n",
       "                       0.5211,  0.4855,  0.5139,  0.3621,  0.4376,  0.7147,  0.4592,  0.5611,\n",
       "                       0.4327,  0.5545,  0.4098,  0.5496,  0.9077,  0.3389,  0.4185,  0.5211,\n",
       "                       0.5869,  0.4117,  0.4865,  0.5181,  0.5689, -0.2646,  0.4066,  0.4888,\n",
       "                       0.4457,  0.3889,  0.3661,  0.9211,  0.6588,  0.3751,  1.2409,  0.4606,\n",
       "                       0.4741,  0.3811,  0.1906,  0.5961,  0.5926,  0.5563,  0.4787,  0.6204,\n",
       "                       0.3361,  0.5254,  0.7494,  0.4383,  0.5365,  0.6263,  0.5144,  0.4167,\n",
       "                       0.3674,  0.5652,  0.4252,  0.3175,  0.4060,  0.2196,  0.4279,  0.3713,\n",
       "                       0.4665,  0.4775,  0.4787,  0.5997,  0.4290,  0.7169,  0.5388,  0.6550,\n",
       "                       0.3734,  0.5796,  0.4763,  0.3487,  0.5151,  0.7431,  0.5180,  0.4210,\n",
       "                       0.7331,  0.4745,  0.5564,  0.4289,  0.5523,  0.3729,  0.4897,  0.5499,\n",
       "                       0.5968,  0.3035,  0.5509,  0.3717,  0.4944,  0.4802,  0.4020,  0.7017],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.3.layer.0.SelfAttention.q.weight',\n",
       "              tensor([[ 0.0458, -0.0612, -0.0412,  ..., -0.0255,  0.0496, -0.0500],\n",
       "                      [ 0.0046,  0.0092, -0.0288,  ...,  0.0592, -0.0185,  0.0463],\n",
       "                      [-0.0250,  0.0102, -0.0360,  ...,  0.0257, -0.0813, -0.0363],\n",
       "                      ...,\n",
       "                      [-0.0447, -0.0216,  0.0551,  ..., -0.0046,  0.0305,  0.0139],\n",
       "                      [ 0.0749,  0.0531, -0.0352,  ..., -0.0756,  0.0319, -0.0331],\n",
       "                      [ 0.2351, -0.0368, -0.0309,  ...,  0.2087,  0.0464, -0.0535]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.3.layer.0.SelfAttention.k.weight',\n",
       "              tensor([[-0.6957, -0.7179, -0.0159,  ..., -0.6982,  0.0847, -0.2969],\n",
       "                      [-0.0344, -0.4825,  0.3878,  ...,  0.1106, -0.7061, -0.2452],\n",
       "                      [-0.5588,  0.9155, -0.1512,  ..., -0.3178, -0.6099,  0.0640],\n",
       "                      ...,\n",
       "                      [-1.0806,  0.3894,  0.1356,  ..., -0.8489,  0.4980,  0.2051],\n",
       "                      [-0.2362,  0.3266, -0.2797,  ..., -1.5153, -0.4742,  0.6592],\n",
       "                      [ 1.0059, -0.1497,  0.5564,  ...,  0.1334, -0.3014, -0.0823]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.3.layer.0.SelfAttention.v.weight',\n",
       "              tensor([[ 0.5732,  0.2787,  1.0590,  ...,  0.3634,  0.0526, -0.9175],\n",
       "                      [ 0.8429, -0.7884, -0.2722,  ..., -0.7883, -0.6530, -0.3665],\n",
       "                      [ 0.9387,  1.2204, -0.7289,  ...,  0.1831,  0.2646,  0.5746],\n",
       "                      ...,\n",
       "                      [ 0.0859,  1.8560, -1.2777,  ..., -0.5536, -0.9647,  0.2047],\n",
       "                      [-1.0262, -0.1747, -0.4144,  ...,  0.0799,  0.3857,  0.7658],\n",
       "                      [-0.0133,  0.9249, -1.3808,  ...,  0.3357,  0.7218, -0.4172]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.3.layer.0.SelfAttention.o.weight',\n",
       "              tensor([[-0.3915,  0.3931, -0.6059,  ...,  1.0261,  0.4314, -0.7291],\n",
       "                      [ 0.9361,  0.2122, -0.9940,  ..., -0.0901,  0.2036,  0.9249],\n",
       "                      [-2.1082, -0.5678,  0.2065,  ..., -0.7030,  0.1312,  0.5527],\n",
       "                      ...,\n",
       "                      [-0.2206,  0.6051, -1.0130,  ..., -0.8378,  0.6396, -1.1329],\n",
       "                      [-0.4131,  0.6752, -1.5062,  ..., -0.7427, -1.0760, -0.2061],\n",
       "                      [-0.1831,  0.4081, -0.4724,  ...,  0.3189,  0.3169,  2.0345]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.3.layer.0.layer_norm.weight',\n",
       "              tensor([ 1.0540e-01,  1.0714e-01,  1.1971e-01,  1.1717e-01,  8.4678e-02,\n",
       "                       8.2038e-02,  2.3071e-02,  1.1608e-01,  5.9138e-02,  8.7457e-02,\n",
       "                       1.1623e-01,  1.2016e-01,  6.0545e-02,  8.7888e-02,  7.9484e-02,\n",
       "                       1.1117e-01,  1.1091e-01,  1.0908e-01,  1.0675e-01,  8.1539e-02,\n",
       "                       8.5693e-02,  1.0559e-01, -1.9676e-03,  9.0642e-02,  8.8235e-02,\n",
       "                       1.0847e-01,  8.7032e-02,  1.0642e-01,  1.0638e-01,  9.4858e-02,\n",
       "                       1.0130e-01,  7.6532e-02,  9.7673e-02,  8.4727e-02,  1.0021e-01,\n",
       "                       1.1427e-01,  6.1397e-02,  1.3629e-01,  8.0485e-02,  1.3521e-01,\n",
       "                       9.1177e-02,  8.8671e-02,  1.2181e-01,  1.1009e-01,  3.5661e-02,\n",
       "                       9.8626e-02,  7.8077e-02,  9.0783e-02,  1.0313e-01,  6.4487e-02,\n",
       "                       1.2493e-01,  1.1660e-01,  6.8515e-02,  8.6367e-02,  1.1132e-04,\n",
       "                       8.1912e-02,  3.4462e-02,  1.0301e-01,  9.9999e-02,  8.8286e-02,\n",
       "                       1.0494e-01,  7.6948e-02,  9.9714e-02,  9.3564e-02,  9.1465e-02,\n",
       "                       3.5489e-02,  8.8854e-02,  8.7183e-02,  1.2414e-01,  9.4003e-02,\n",
       "                       1.0821e-01,  1.1217e-01,  9.0359e-02,  1.0772e-01,  1.0645e-01,\n",
       "                       8.0600e-02,  7.9034e-02,  1.0998e-01,  8.1122e-02,  1.1042e-01,\n",
       "                       8.5281e-02,  9.8454e-02,  9.6457e-02,  1.1105e-01,  1.1742e-01,\n",
       "                       1.1721e-01,  1.1003e-01,  1.1011e-01,  9.7993e-02,  7.9860e-02,\n",
       "                       1.0993e-01,  8.1931e-02,  7.8975e-02,  9.2922e-02,  9.5903e-02,\n",
       "                       8.5161e-02,  9.9051e-02,  8.5961e-02,  9.8341e-02,  7.3696e-02,\n",
       "                       1.0430e-01,  1.0736e-01,  9.7670e-02,  8.5689e-02,  9.7524e-02,\n",
       "                       1.0438e-01,  8.0580e-02,  8.8392e-02,  8.9481e-02,  7.8780e-02,\n",
       "                       9.6073e-02,  8.4008e-02,  9.1140e-02,  7.3812e-02,  9.4547e-02,\n",
       "                       8.8932e-02,  1.0844e-01,  7.4260e-02,  1.1691e-01,  9.3030e-02,\n",
       "                       7.0087e-02,  1.0208e-01,  1.2136e-01,  1.0185e-01,  1.2761e-01,\n",
       "                       9.5609e-02,  9.2880e-02,  1.0628e-01,  1.0392e-01,  1.1127e-01,\n",
       "                       4.7941e-02,  1.0690e-01,  8.2069e-02,  6.9451e-02,  7.6321e-02,\n",
       "                       7.8261e-02,  1.2259e-01,  9.5266e-02,  9.4394e-02,  1.1699e-01,\n",
       "                       9.2412e-02,  1.1905e-01,  1.2345e-01,  1.0294e-01,  1.1454e-01,\n",
       "                       1.0291e-01,  9.0441e-02,  9.6697e-02,  7.9749e-02,  9.5340e-02,\n",
       "                       9.0957e-02,  1.0049e-01,  9.8466e-02,  1.3770e-01,  7.6354e-02,\n",
       "                       8.9427e-02,  1.1247e-01,  7.7617e-02,  7.7502e-02,  8.5556e-02,\n",
       "                       8.6400e-02,  9.1332e-02,  8.1695e-02,  4.9254e-02,  7.6362e-02,\n",
       "                       6.9881e-02,  8.7410e-02,  1.2047e-01,  1.1063e-01,  1.2135e-01,\n",
       "                       1.4517e-01,  1.0376e-01,  1.0543e-01,  8.2220e-02,  1.0525e-01,\n",
       "                       1.0248e-01,  1.0578e-01,  1.3169e-01,  1.0261e-01,  1.4623e-02,\n",
       "                       9.2850e-02,  8.6659e-02,  1.0820e-01,  8.7947e-02,  1.1611e-01,\n",
       "                       1.2826e-01,  8.6312e-02,  1.4826e-01,  9.8787e-02,  1.0236e-01,\n",
       "                       6.6182e-02,  9.0467e-02,  4.8827e-02,  8.9405e-02,  8.3399e-02,\n",
       "                       4.4754e-02,  1.0458e-01,  1.1897e-01,  1.1266e-01,  6.6110e-02,\n",
       "                       7.9590e-02,  1.1046e-01,  1.1748e-01,  1.0188e-01,  8.9884e-02,\n",
       "                       1.1615e-01,  1.0108e-01,  8.4259e-02,  9.0948e-02,  9.6410e-02,\n",
       "                       9.4859e-02,  7.1458e-02,  1.0592e-01,  1.1482e-01,  1.0094e-01,\n",
       "                       1.2509e-01,  1.0558e-01,  1.0539e-01,  1.2327e-01,  5.4972e-02,\n",
       "                       5.6657e-02,  9.5631e-02,  7.4945e-02,  1.0038e-01,  9.5451e-02,\n",
       "                       9.6207e-02,  1.1007e-01,  1.1583e-01,  1.0134e-01,  1.2498e-01,\n",
       "                       7.1665e-02,  9.7364e-02,  5.5251e-02,  7.7073e-02,  8.1906e-02,\n",
       "                       3.6876e-02,  3.3003e-02,  9.3557e-02,  1.0115e-01,  1.0279e-01,\n",
       "                       1.0696e-01,  1.3753e-01,  8.5323e-02,  1.0290e-01,  8.2789e-02,\n",
       "                       1.0002e-01,  1.0883e-01,  1.0086e-01,  7.7511e-02,  1.0562e-01,\n",
       "                       6.2301e-02,  8.5557e-02,  8.0962e-02,  8.2463e-02,  7.4089e-02,\n",
       "                       7.4188e-02,  8.1452e-02,  7.3840e-02,  6.0554e-02,  1.0058e-01,\n",
       "                       1.3271e-01,  1.0193e-01,  1.0364e-01,  9.1488e-02,  8.7918e-02,\n",
       "                       8.7608e-02,  9.0097e-02,  8.3595e-02,  1.0857e-01,  9.9853e-02,\n",
       "                       3.5449e-02,  1.1744e-01,  1.1032e-01,  2.2856e-02,  1.3700e-01,\n",
       "                       1.1083e-01,  8.9576e-02,  1.0087e-01,  1.0233e-01,  1.0714e-01,\n",
       "                       1.3508e-01,  1.1746e-01,  1.0322e-01,  8.2620e-02,  1.1459e-01,\n",
       "                       9.2826e-02,  7.8072e-02,  9.7768e-02,  9.2348e-02,  6.8622e-02,\n",
       "                       6.9116e-02,  9.9167e-02,  7.5380e-02,  9.3518e-02,  9.3602e-02,\n",
       "                       7.9734e-02,  1.2975e-01,  9.4354e-02,  8.3459e-02,  1.0445e-01,\n",
       "                       1.2068e-01,  1.0071e-01,  1.5396e-01,  1.1092e-01,  9.8343e-02,\n",
       "                       8.1560e-02,  9.3705e-02,  9.4396e-02,  7.8288e-02,  9.9215e-02,\n",
       "                       9.9774e-02,  1.3357e-01,  8.4603e-02,  6.6558e-02,  1.1087e-01,\n",
       "                       1.0842e-01,  6.2572e-02,  1.3026e-01,  7.5038e-02,  8.8196e-02,\n",
       "                       9.2264e-02,  1.0408e-01,  9.9634e-02,  7.5406e-02,  1.1815e-01,\n",
       "                       9.6766e-02,  1.0878e-01,  1.2116e-01,  9.9795e-02,  1.1022e-01,\n",
       "                       9.1015e-02,  1.0339e-01,  1.0592e-01,  9.6829e-02,  9.8837e-02,\n",
       "                       9.9035e-02,  7.9558e-02,  9.2136e-02,  1.0279e-01,  1.4524e-01,\n",
       "                       9.8937e-02,  1.1327e-01,  9.0841e-02,  9.6688e-02,  1.0731e-01,\n",
       "                       1.0200e-01,  8.6630e-02,  9.4425e-02,  1.0718e-01,  5.7086e-02,\n",
       "                       1.0146e-01,  9.6927e-02,  9.6139e-02,  9.7246e-02,  1.0313e-01,\n",
       "                       9.8999e-02,  1.1782e-01,  1.2725e-01,  1.0482e-01,  1.1973e-01,\n",
       "                       8.1246e-02,  1.1964e-01,  5.4602e-02,  5.2804e-02,  9.5313e-02,\n",
       "                       9.9568e-02,  7.8334e-02,  1.0328e-01,  9.4041e-02,  1.0310e-01,\n",
       "                       8.8128e-02,  1.1092e-01,  1.9185e-02,  7.2391e-02,  1.1826e-01,\n",
       "                       8.8710e-02,  1.0871e-01,  1.0493e-01,  1.2509e-01,  6.8080e-02,\n",
       "                       1.0917e-01,  1.1575e-01,  1.1700e-01,  9.5256e-02,  9.5864e-02,\n",
       "                       2.6066e-02,  9.3922e-02,  1.2426e-01,  5.6534e-02,  1.2424e-01,\n",
       "                       9.7744e-02,  7.9339e-02,  9.8779e-02,  1.2398e-01,  1.2218e-01,\n",
       "                       1.0362e-01,  1.1323e-01,  9.1189e-02,  1.0424e-01,  9.6780e-02,\n",
       "                       6.8932e-02,  1.5234e-02,  7.3643e-02,  8.5153e-02,  7.2940e-02,\n",
       "                       6.9705e-02,  1.0628e-01,  8.2571e-02,  9.0663e-02,  8.2219e-02,\n",
       "                       9.7055e-02,  9.4062e-02,  9.5660e-02,  9.7822e-02,  8.3592e-02,\n",
       "                       1.1445e-01,  7.2241e-02,  7.9172e-02,  3.6704e-02,  9.8667e-02,\n",
       "                       8.1974e-02,  9.9946e-02,  8.2505e-02,  5.0169e-02,  1.1556e-01,\n",
       "                       8.4793e-02,  1.1183e-01,  8.4582e-02,  1.1299e-01,  1.0705e-01,\n",
       "                       1.1538e-01,  7.8683e-02,  7.4909e-02,  7.9341e-02,  1.1732e-01,\n",
       "                       9.0125e-02,  1.0394e-01,  8.9835e-02,  1.2708e-01,  1.4502e-01,\n",
       "                       1.2351e-01,  8.6801e-02,  1.1584e-01,  1.3386e-01,  8.8309e-02,\n",
       "                       1.0480e-01,  8.0314e-02,  1.0511e-01,  1.2251e-01,  1.0051e-01,\n",
       "                       1.0406e-01,  8.9133e-02,  1.0592e-01,  9.8782e-02,  1.0024e-01,\n",
       "                       1.1500e-01,  1.2648e-01,  8.3765e-02,  1.1567e-01,  8.5218e-02,\n",
       "                       9.0910e-02,  7.2775e-02,  7.3818e-02,  1.0908e-01,  1.1319e-01,\n",
       "                       9.9919e-02,  6.8546e-02,  8.1081e-02,  6.9174e-02,  8.2339e-02,\n",
       "                       8.7176e-02,  8.2242e-02,  7.3328e-02,  8.5137e-02,  1.0796e-01,\n",
       "                       1.0198e-01,  1.0186e-01,  9.2523e-02,  1.0720e-01,  9.0969e-02,\n",
       "                       1.1688e-01,  6.4825e-02,  1.1447e-01,  9.5615e-02,  1.2571e-01,\n",
       "                       8.8761e-02,  7.6589e-02,  1.1460e-01, -7.7924e-02,  1.1016e-01,\n",
       "                       1.0746e-01,  1.2033e-01,  6.1377e-02,  8.9607e-02,  9.5758e-02,\n",
       "                       9.9621e-02,  9.8302e-02,  9.4095e-02,  8.7453e-02,  8.5810e-02,\n",
       "                       8.8321e-02,  1.1907e-01,  9.6253e-02,  2.3626e-02,  1.0377e-01,\n",
       "                       1.1467e-01,  1.1055e-01,  1.2448e-01,  9.5750e-02,  1.0359e-01,\n",
       "                       1.1796e-01,  8.5011e-02,  5.8493e-02,  8.8288e-02,  1.0406e-01,\n",
       "                       1.0153e-01,  1.0824e-01,  6.2809e-02,  6.7047e-02,  1.1709e-01,\n",
       "                       8.8745e-02,  8.5612e-02,  6.9738e-02,  9.2029e-02,  1.1168e-01,\n",
       "                       5.7823e-02,  1.1568e-01,  1.2993e-01,  1.0917e-01,  1.0458e-01,\n",
       "                       1.0678e-01,  1.1947e-01,  9.6588e-02,  8.9925e-02,  9.8091e-02,\n",
       "                       8.8612e-02,  9.7851e-02,  1.0267e-01,  1.1950e-01,  9.6620e-02,\n",
       "                       1.1012e-01,  9.4412e-02,  1.1667e-01,  1.2874e-01,  9.7670e-02,\n",
       "                       9.0278e-02,  1.2063e-01,  9.8015e-02,  8.7138e-02,  9.7711e-02,\n",
       "                       1.0494e-01,  1.4176e-01,  6.7803e-02,  7.3358e-02,  8.1840e-02,\n",
       "                       9.3572e-02,  9.9807e-02,  9.3284e-02,  1.0528e-01,  1.0337e-01,\n",
       "                       1.3013e-01,  7.1320e-02,  1.2844e-01,  1.1876e-01,  1.0050e-01,\n",
       "                       9.8265e-02,  1.6022e-02,  1.0012e-01,  1.0313e-01,  9.2223e-02,\n",
       "                       1.0919e-01,  4.8724e-02,  7.1343e-02,  9.0676e-02,  8.6147e-02,\n",
       "                       1.0852e-01,  8.9409e-02,  1.2500e-01,  8.5131e-02,  1.0123e-01,\n",
       "                       7.8931e-02,  9.4163e-02,  9.4978e-02,  8.9610e-02,  7.7927e-02,\n",
       "                       8.2353e-02,  8.7188e-02,  9.8076e-02,  7.5474e-02,  8.3623e-02,\n",
       "                       9.4720e-02,  1.2621e-01,  8.7951e-02,  1.0927e-01,  1.0826e-01,\n",
       "                       1.0756e-01,  9.2180e-02,  9.4518e-02,  1.1905e-01,  9.7094e-02,\n",
       "                       1.2016e-01,  9.4974e-02,  1.0852e-01,  6.3247e-02,  1.0509e-01,\n",
       "                       7.2919e-02,  9.7224e-02,  9.2935e-02,  1.1451e-01,  9.0730e-02,\n",
       "                       1.0959e-01,  9.1129e-02,  1.0555e-01,  8.6245e-02,  8.9639e-02,\n",
       "                       1.4286e-01,  8.0480e-02,  1.0287e-01,  1.0809e-01,  1.0169e-01,\n",
       "                       1.0156e-01,  6.9623e-02,  1.0491e-01,  1.3176e-01,  1.0787e-01,\n",
       "                       8.6896e-02,  1.0864e-01,  9.9940e-02,  9.4856e-02,  9.0619e-02,\n",
       "                       9.7665e-02,  9.1778e-02,  1.0383e-01,  1.2453e-01,  8.6484e-02,\n",
       "                       1.1235e-01,  9.4663e-02,  1.0969e-01,  1.3583e-01,  9.2636e-02,\n",
       "                       1.1179e-01,  1.0616e-01,  8.4579e-02,  9.4394e-02,  1.1566e-01,\n",
       "                       1.2378e-01,  1.1423e-01,  9.4719e-02,  8.1298e-02,  8.3874e-02,\n",
       "                       9.4868e-02,  1.0020e-01,  1.0745e-01,  9.1090e-02,  1.0402e-01,\n",
       "                       1.1476e-01,  1.1399e-01,  7.9712e-02,  1.0504e-01,  1.3083e-01,\n",
       "                       1.2613e-01,  5.5423e-05,  9.4119e-02,  1.3277e-01,  8.2828e-02,\n",
       "                       1.1352e-01,  8.8361e-02,  9.8543e-02,  1.0721e-01,  9.2220e-02,\n",
       "                       9.5450e-02,  6.5756e-02,  9.2197e-02,  8.1596e-02,  1.1355e-01,\n",
       "                       9.7145e-02,  8.2376e-02,  8.1694e-02,  9.5151e-02,  1.0078e-01,\n",
       "                       9.9854e-02,  8.8745e-02,  1.1212e-01,  6.5666e-02,  1.0098e-01,\n",
       "                       4.3556e-02,  6.9313e-02,  1.0724e-01,  7.7406e-02,  1.0976e-01,\n",
       "                       9.1985e-02,  1.3819e-01,  1.1844e-01,  7.3282e-02,  9.8441e-02,\n",
       "                       8.4355e-02,  1.0825e-01,  9.4010e-02,  9.7124e-02,  1.0162e-01,\n",
       "                       1.0197e-01,  5.5473e-02,  1.0228e-01,  8.0369e-02,  9.6200e-02,\n",
       "                       9.2244e-02,  9.9874e-02,  1.0626e-01,  1.2206e-01,  8.5836e-02,\n",
       "                       3.6217e-02,  9.8364e-02,  9.0619e-02,  1.0311e-01,  2.2975e-02,\n",
       "                       1.0376e-01,  9.8308e-02,  1.3483e-01,  9.0100e-02,  1.3255e-01,\n",
       "                       9.9514e-02,  8.1278e-02,  1.2657e-01,  9.7263e-02,  1.1555e-01,\n",
       "                       1.1613e-01,  9.4306e-02,  8.3897e-02,  8.9027e-02,  1.1610e-01,\n",
       "                       1.0286e-01,  1.0168e-01,  9.9584e-02,  5.6317e-02,  8.1603e-02,\n",
       "                       1.0201e-01,  1.0341e-01,  9.3769e-02,  8.7060e-02,  1.1183e-01,\n",
       "                       9.1913e-02,  1.2118e-01,  1.0445e-01,  1.1884e-01,  9.6855e-02,\n",
       "                       1.0577e-01,  1.0266e-01,  7.8097e-02,  1.2131e-01,  1.0457e-01,\n",
       "                       1.1298e-01,  9.1555e-02,  1.2272e-01,  8.6530e-02,  1.0811e-01,\n",
       "                       1.0616e-01,  9.9983e-02,  8.4867e-02,  1.0293e-01,  1.1781e-01,\n",
       "                       9.7428e-02,  6.8507e-02,  1.0884e-01,  7.1062e-02,  1.0602e-01,\n",
       "                       1.3462e-01,  8.7244e-02,  1.1802e-01], device='cuda:0')),\n",
       "             ('encoder.block.3.layer.1.DenseReluDense.wi.weight',\n",
       "              tensor([[ 0.6782, -1.3654,  1.6698,  ...,  0.5474,  0.5066, -0.6398],\n",
       "                      [ 1.3853, -0.4484,  0.9330,  ...,  1.5728,  0.4633, -0.1386],\n",
       "                      [ 0.2656, -1.0216,  1.6102,  ..., -0.5385,  0.6403,  0.3833],\n",
       "                      ...,\n",
       "                      [ 0.9344, -1.8715, -1.8362,  ...,  0.5824, -0.1286,  2.1914],\n",
       "                      [-0.2997, -1.3600,  2.8589,  ...,  1.6339, -1.4267, -0.3721],\n",
       "                      [-0.2224, -0.4011, -0.4318,  ...,  1.4644,  0.2611,  0.0172]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.3.layer.1.DenseReluDense.wo.weight',\n",
       "              tensor([[ 0.3970,  0.1676,  0.7270,  ...,  0.1738,  0.1070,  0.5083],\n",
       "                      [ 0.2435,  0.3494,  0.5236,  ..., -0.6610,  0.2955, -0.4360],\n",
       "                      [-0.0019,  0.3638,  0.0510,  ..., -0.8843,  0.5422,  0.1997],\n",
       "                      ...,\n",
       "                      [ 0.6181,  0.4637,  0.3954,  ...,  0.0880, -0.1686, -0.2432],\n",
       "                      [ 0.5233,  0.4609, -0.2735,  ..., -0.8195,  0.5960,  0.0980],\n",
       "                      [ 0.0921, -0.3945,  0.3363,  ..., -0.1781,  0.7136,  0.1165]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.3.layer.1.layer_norm.weight',\n",
       "              tensor([ 0.7678,  0.7395,  0.6470,  0.7693,  0.4248,  0.6264,  0.2347,  0.8059,\n",
       "                       1.7856,  0.5596,  0.6504,  0.7440,  0.6086,  0.8377,  0.4236,  0.5082,\n",
       "                       0.7278,  0.6288,  0.8560,  0.4825,  0.5786,  0.6044,  0.9001,  0.5016,\n",
       "                       0.5004,  0.6034,  0.4588,  0.6435,  0.5689,  0.4277,  0.4734,  0.5346,\n",
       "                       0.7354,  0.4352,  0.7501,  0.5408,  0.9646,  0.8015,  0.4074,  0.8595,\n",
       "                       0.6783,  0.7830,  0.9180,  0.5064,  1.8856,  0.5938,  0.8381,  0.5007,\n",
       "                       0.6473,  0.9470,  0.7101,  0.5820,  0.9769,  0.4980,  0.5933,  0.6017,\n",
       "                       0.8273,  0.7033,  0.4377,  0.5094,  0.6137,  0.4199,  0.6659,  0.4969,\n",
       "                       0.5521,  1.2434,  0.4847,  0.5730,  0.7869,  0.5246,  0.4383,  0.9233,\n",
       "                       0.4911,  0.6170,  0.5617,  0.4131,  0.9361,  0.6577,  0.4924,  0.5907,\n",
       "                       0.4166,  0.6826,  0.4705,  0.9299,  0.6246,  0.5506,  0.8321,  0.5586,\n",
       "                       0.6182,  0.5109,  0.6188,  0.4103,  0.5399,  0.6889,  0.4024,  0.4661,\n",
       "                       0.4749,  0.4760,  0.4132,  0.4898,  0.7989,  0.4920,  0.9136,  0.4446,\n",
       "                       0.4781,  1.0485,  0.5689,  0.4625,  0.5339,  0.9280,  0.6451,  0.4773,\n",
       "                       0.5907,  0.5301,  0.6428,  0.5022,  0.7018,  0.4254,  1.1051,  0.5562,\n",
       "                       0.3578,  0.5197,  0.9468,  0.5632,  0.7330,  0.5837,  0.5492,  0.5574,\n",
       "                       0.6093,  0.6192,  1.2291,  0.6626,  0.4901,  0.3808,  0.4548,  0.9690,\n",
       "                       0.7655,  0.5222,  0.5697,  0.7257,  0.5887,  0.8163,  0.8275,  0.9832,\n",
       "                       0.6258,  0.7819,  0.5457,  0.6699,  0.4712,  0.6463,  0.5748,  0.5580,\n",
       "                       0.5878,  0.8819,  0.4605,  0.5175,  0.7598,  0.3635,  0.7439,  0.6112,\n",
       "                       0.5143,  0.5487,  0.5898,  1.8284,  0.4629,  0.3676,  0.5199,  0.6647,\n",
       "                       0.5108,  0.8406,  0.9045,  0.5969,  0.4476,  0.8614,  0.4359,  0.7732,\n",
       "                       0.7157,  0.9237,  0.6935,  0.3285,  0.6042,  0.5254,  0.6671,  0.4855,\n",
       "                       0.6856,  0.5298,  0.4809,  0.8761,  0.6554,  0.7308,  0.4439,  0.4304,\n",
       "                       0.2824,  0.5249,  0.5109,  0.1843,  0.5114,  0.8846,  0.5887,  0.4099,\n",
       "                       0.4423,  0.6171,  0.4816,  0.5667,  0.5119,  0.7313,  0.4612,  0.6178,\n",
       "                       0.4795,  0.6454,  0.7912,  0.5529,  0.6824,  0.8129,  0.5373,  0.6229,\n",
       "                       0.6281,  0.5610,  0.7033,  1.3613,  0.7454,  0.6403,  0.4507,  0.5877,\n",
       "                       0.8057,  0.4896,  0.7536,  0.7820,  0.6081,  0.7066,  0.4333,  0.7430,\n",
       "                      -0.3672,  0.4755,  0.4363,  0.1738,  0.0384,  0.7343,  0.8105,  0.7451,\n",
       "                       0.9370,  0.7135,  0.5990,  0.5172,  0.4198,  0.5449,  0.4867,  0.7491,\n",
       "                       0.4125,  0.5778,  1.7670,  0.5431,  0.5933,  0.5415,  0.5128,  0.4357,\n",
       "                       0.5199,  0.5307,  0.8520,  0.4882,  0.9768,  0.0784,  0.4508,  0.5501,\n",
       "                       0.5906,  0.5620,  0.5936,  0.5315,  0.5565,  0.4950,  0.6184,  0.9070,\n",
       "                       0.9042,  1.3400,  0.8847,  0.5923,  0.5791,  0.9326,  0.6533,  0.5671,\n",
       "                       1.0112,  0.9807,  0.5271,  0.8675,  0.6869,  0.9172,  0.3944,  0.5563,\n",
       "                       0.5935,  1.1941,  0.5688,  0.6350,  0.4860,  0.3760,  0.5138,  0.4898,\n",
       "                       0.7883,  0.5509,  0.4848,  0.5887,  0.6362,  0.5158,  0.8918,  0.6369,\n",
       "                       0.6131,  0.4329,  0.4900,  0.5018,  0.6213,  0.6898,  0.5772,  0.7540,\n",
       "                       0.5474, -0.7153, -0.7563,  0.6101,  0.4888,  0.7616,  0.7920,  0.4356,\n",
       "                       0.3771,  0.7770,  0.6037,  0.6160,  0.8786,  0.4043,  0.8626,  0.8497,\n",
       "                       0.5983,  0.5945,  0.4723,  0.9490,  0.7355,  0.5200,  0.4870,  0.6763,\n",
       "                       0.3818,  0.5105,  0.6741,  1.0551,  0.6965,  0.5594,  0.6688,  0.5270,\n",
       "                       0.4941,  0.6917,  0.9021,  0.6492,  0.9673,  1.2214,  0.7081,  0.4179,\n",
       "                       0.6729,  0.4931,  0.6832,  0.6959,  0.7128,  0.8670,  0.4213,  0.8301,\n",
       "                       0.5142,  0.9401,  0.9454,  0.2868,  0.4528,  0.5427,  1.0132,  0.6813,\n",
       "                       0.5072,  0.6832,  0.6927,  0.6538,  0.0136,  0.4597,  0.7505,  0.4345,\n",
       "                       0.8401,  0.5618,  0.8652,  0.8751,  0.5923,  0.6515,  0.8074,  0.5213,\n",
       "                       0.5585,  1.0061,  0.5366,  0.8729,  0.3712,  0.6803,  0.4698,  1.1422,\n",
       "                       0.5499,  0.7766,  0.6111,  0.5944,  0.7316,  0.5356,  0.6498,  0.5010,\n",
       "                       0.4672,  0.5588,  0.3774,  0.4657,  0.4589,  0.4930,  0.8527,  0.5640,\n",
       "                       0.5345,  0.5463,  0.4998,  0.4580,  0.6250,  0.6698,  0.4282,  0.6434,\n",
       "                       0.4739,  0.4924,  0.0949,  0.8520,  0.9814,  0.5291,  0.3841,  0.2357,\n",
       "                       0.4936,  0.3938,  0.6948,  0.4587,  0.6700,  0.7316,  0.7718,  0.5229,\n",
       "                       0.9398,  0.4296,  0.8611,  0.4722,  0.5539,  0.5446,  0.7613,  0.8863,\n",
       "                       0.6830,  0.7833,  0.7468,  0.7348,  0.5418,  0.6350,  0.5292,  0.6496,\n",
       "                       0.8754,  0.6224,  0.5758,  0.4561,  0.6248,  0.5435,  0.6722,  0.7546,\n",
       "                       0.8597,  0.6322,  0.6704,  0.5453,  0.4770,  0.4942,  0.4800,  0.6981,\n",
       "                       0.6167,  0.5420,  0.4392,  0.5384,  0.4460,  0.4487,  0.5375,  0.4369,\n",
       "                       0.3146,  0.5357,  0.5904,  0.3932,  0.4725,  0.4912,  0.8108,  0.6246,\n",
       "                       0.7339,  1.3841,  0.6121,  0.4394,  0.8741,  0.5466,  0.3904,  0.7750,\n",
       "                       0.5050,  0.5907,  0.5702,  0.6511,  0.9241,  0.4538,  0.6776,  0.8901,\n",
       "                       0.4867,  0.6142,  0.5798,  0.4248,  0.5174,  0.6753,  0.8439,  1.0158,\n",
       "                       0.5149,  0.7202,  0.6111,  0.7988,  0.3471,  0.7212,  0.6550,  0.4154,\n",
       "                       1.6231,  0.4738,  0.6997,  0.4675,  0.5879,  0.5964,  1.0375,  0.7436,\n",
       "                       0.3388,  0.6981,  0.4448,  0.6057,  0.5672,  1.2105,  0.6947,  0.8382,\n",
       "                       0.5535,  0.6632,  0.7598,  0.5682,  0.6651,  0.5881,  0.7774,  0.5575,\n",
       "                       0.6010,  0.5999,  0.8783,  0.6305,  0.6981,  0.4821,  0.5972,  0.7745,\n",
       "                       0.5316,  0.5453,  0.6686,  0.5192,  0.5336,  0.7676,  0.8569,  0.9008,\n",
       "                       0.4313,  0.7704,  0.4771,  0.6271,  0.7760,  0.4053,  0.6803,  0.8292,\n",
       "                       0.6976,  0.4667,  0.9462,  0.7334,  0.5669,  0.6374,  0.0047,  0.6170,\n",
       "                       0.6269,  0.4113,  0.7006,  0.0743,  0.4581,  0.5787,  0.5193,  0.6202,\n",
       "                       0.5576,  0.6963,  0.4964,  1.0798,  0.5184,  0.4424,  0.5367,  0.4595,\n",
       "                       0.4758,  0.4641,  0.4662,  0.4574,  0.4406,  0.5491,  0.5482,  0.6724,\n",
       "                       0.3990,  0.5338,  0.6094,  0.5458,  0.5562,  0.6085,  0.9585,  0.6164,\n",
       "                       0.8111,  0.3890,  0.5765,  0.3085,  0.7496,  0.3841,  0.4507,  0.7261,\n",
       "                       0.8508,  0.5179,  0.5582,  0.5501,  0.7528,  0.5621,  0.6196,  0.7137,\n",
       "                       0.4728,  0.5193,  0.5402,  0.5187,  0.7358,  0.3728,  0.5327,  0.7892,\n",
       "                       0.5933,  0.6129,  0.6562,  0.8721,  0.5100,  0.9657,  0.8047,  0.5114,\n",
       "                       0.6191,  0.9664,  0.5345,  0.9518,  0.6790,  0.6247,  0.8024,  0.5128,\n",
       "                       0.7379,  0.6011,  0.5632,  0.6542,  0.6931,  0.6767,  0.7239,  0.3839,\n",
       "                       0.4515,  0.5264,  0.5233,  0.5677,  0.5773,  0.5542,  0.7330,  1.0354,\n",
       "                       0.5223,  0.6641,  0.6042,  0.8324,  0.8228, -0.0132,  0.6144,  0.9019,\n",
       "                       0.5343,  0.6997,  0.5748,  0.5304,  0.6208,  0.4370,  0.4684,  0.9374,\n",
       "                       0.5611,  0.5294,  0.5839,  0.5200,  0.4505,  0.4954,  0.5850,  0.5639,\n",
       "                       0.6788,  0.4817,  0.6659,  0.3691,  0.5640,  1.0066,  0.5676,  0.6850,\n",
       "                       0.4431,  0.5667,  0.4329,  0.6457,  1.0421,  0.4247,  0.6176,  0.5314,\n",
       "                       0.6923,  0.4658,  0.6454,  0.6297,  0.7256,  0.3381,  0.5707,  0.5546,\n",
       "                       0.6059,  0.5030,  0.5687,  1.0639,  0.8599,  0.4652,  1.5762,  0.6057,\n",
       "                       0.5132,  0.4959,  0.2537,  0.6600,  0.7608,  0.7430,  0.5718,  0.8947,\n",
       "                       0.4994,  0.7900,  0.8457,  0.5007,  0.5438,  0.7200,  0.5711,  0.5207,\n",
       "                       0.3556,  0.7845,  0.7333,  0.4595,  0.4609,  0.3440,  0.4918,  0.4599,\n",
       "                       0.5776,  0.6541,  0.5217,  0.5745,  0.5778,  0.8938,  0.7508,  0.7415,\n",
       "                       0.4935,  0.6959,  0.6424,  0.4474,  0.6595,  0.9409,  0.6044,  0.4650,\n",
       "                       0.9026,  0.6221,  0.6764,  0.5866,  0.6857,  0.4650,  0.5696,  0.6603,\n",
       "                       0.6538,  0.2873,  0.6795,  0.5593,  0.6774,  0.6318,  0.4366,  0.6951],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.4.layer.0.SelfAttention.q.weight',\n",
       "              tensor([[ 0.0128, -0.0659,  0.0944,  ...,  0.0245,  0.0448,  0.0465],\n",
       "                      [-0.0043,  0.1644, -0.1076,  ..., -0.0724,  0.0897, -0.1402],\n",
       "                      [ 0.0655, -0.0541, -0.0311,  ..., -0.0824, -0.0333, -0.0569],\n",
       "                      ...,\n",
       "                      [ 0.0642, -0.0386, -0.0287,  ...,  0.0228, -0.1916, -0.0219],\n",
       "                      [ 0.0417, -0.0097, -0.0812,  ...,  0.0325, -0.0474,  0.0130],\n",
       "                      [-0.0588,  0.0620, -0.0095,  ...,  0.0539,  0.0525,  0.0452]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.4.layer.0.SelfAttention.k.weight',\n",
       "              tensor([[ 0.5958,  0.2090,  1.2219,  ...,  0.1962,  0.7886, -0.2582],\n",
       "                      [-0.0941,  0.8659, -0.3850,  ..., -0.2776, -0.7457, -1.0166],\n",
       "                      [-0.2504, -0.6023, -0.1114,  ..., -0.2410, -0.4114, -0.4251],\n",
       "                      ...,\n",
       "                      [ 0.4486, -0.1331, -0.0496,  ...,  0.6147, -1.6670, -0.0046],\n",
       "                      [ 0.8132, -0.0516, -0.7825,  ..., -0.2457, -0.5844,  0.0358],\n",
       "                      [ 0.5117, -0.4229, -0.3519,  ...,  0.0940,  0.6836,  0.4971]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.4.layer.0.SelfAttention.v.weight',\n",
       "              tensor([[-0.9852,  1.1668,  0.5158,  ..., -0.5026, -0.5979,  2.2402],\n",
       "                      [ 0.4987, -0.7136,  0.5477,  ..., -0.1656, -0.0757, -1.0646],\n",
       "                      [ 0.7115,  0.3697, -0.3401,  ..., -1.7220, -0.3479, -0.8681],\n",
       "                      ...,\n",
       "                      [-1.5615,  0.3579, -0.2612,  ...,  0.7340, -1.2966, -0.0219],\n",
       "                      [ 0.5377, -0.1849,  1.0028,  ...,  0.5867,  1.1927, -0.6995],\n",
       "                      [-1.2087,  0.4805, -1.0245,  ..., -0.1637,  1.0208, -0.2202]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.4.layer.0.SelfAttention.o.weight',\n",
       "              tensor([[ 0.5029, -1.2441,  1.0110,  ...,  1.0211,  1.3356,  1.0149],\n",
       "                      [-1.2249,  0.9019, -0.2018,  ...,  0.0451, -0.0268,  0.3086],\n",
       "                      [ 1.2587,  0.2237,  1.0939,  ...,  1.3778,  0.3400, -0.7481],\n",
       "                      ...,\n",
       "                      [ 0.4532, -0.5040,  1.4788,  ...,  1.8643, -1.5036,  1.8405],\n",
       "                      [-0.2028,  0.0987,  1.3847,  ..., -0.1434,  1.5235,  0.1935],\n",
       "                      [-1.5212,  1.0978,  0.9908,  ...,  1.6973, -1.0874, -1.3412]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.4.layer.0.layer_norm.weight',\n",
       "              tensor([ 0.1086,  0.0979,  0.1139,  0.1202,  0.0775,  0.0938,  0.0188,  0.1317,\n",
       "                       0.0300,  0.0942,  0.1003,  0.1217,  0.0601,  0.1025,  0.0890,  0.0986,\n",
       "                       0.0968,  0.1243,  0.1172,  0.0891,  0.0793,  0.0982,  0.0098,  0.0833,\n",
       "                       0.0921,  0.0979,  0.0841,  0.0947,  0.1100,  0.0841,  0.0884,  0.0922,\n",
       "                       0.1054,  0.0804,  0.1096,  0.1103,  0.0637,  0.1329,  0.0854,  0.1116,\n",
       "                       0.0929,  0.0955,  0.1280,  0.1032,  0.0174,  0.0930,  0.0632,  0.0828,\n",
       "                       0.1009,  0.0417,  0.1244,  0.1045,  0.0576,  0.0827,  0.0005,  0.0731,\n",
       "                       0.0398,  0.0969,  0.1020,  0.0946,  0.1038,  0.0799,  0.1010,  0.0872,\n",
       "                       0.0942,  0.0359,  0.0958,  0.0771,  0.1102,  0.0936,  0.0816,  0.1132,\n",
       "                       0.0996,  0.0883,  0.0902,  0.0941,  0.0803,  0.0885,  0.0804,  0.1267,\n",
       "                       0.0683,  0.1054,  0.0980,  0.1160,  0.0964,  0.0999,  0.1241,  0.1231,\n",
       "                       0.0947,  0.0715,  0.1041,  0.0843,  0.0755,  0.1033,  0.0843,  0.0779,\n",
       "                       0.1123,  0.0888,  0.0888,  0.0669,  0.1141,  0.0915,  0.0782,  0.0799,\n",
       "                       0.0881,  0.0855,  0.0896,  0.0817,  0.0854,  0.0645,  0.0976,  0.0904,\n",
       "                       0.0939,  0.0654,  0.0835,  0.0912,  0.1012,  0.0856,  0.0930,  0.0766,\n",
       "                       0.0665,  0.0928,  0.1042,  0.0983,  0.1163,  0.1085,  0.1086,  0.0915,\n",
       "                       0.0901,  0.1115,  0.0375,  0.1163,  0.0888,  0.0714,  0.0994,  0.0715,\n",
       "                       0.1212,  0.0938,  0.0976,  0.1017,  0.0850,  0.1439,  0.1047,  0.0870,\n",
       "                       0.1054,  0.1208,  0.0901,  0.1028,  0.0870,  0.0989,  0.0894,  0.1084,\n",
       "                       0.1037,  0.1197,  0.0811,  0.0899,  0.0919,  0.0845,  0.0833,  0.0851,\n",
       "                       0.0808,  0.0850,  0.0604,  0.0192,  0.0737,  0.0717,  0.1000,  0.1006,\n",
       "                       0.1080,  0.1260,  0.1271,  0.1124,  0.0994,  0.0796,  0.0958,  0.0838,\n",
       "                       0.1073,  0.1121,  0.0776,  0.0011,  0.0934,  0.0904,  0.0951,  0.0743,\n",
       "                       0.1077,  0.1075,  0.0772,  0.1156,  0.1070,  0.0866,  0.0805,  0.0885,\n",
       "                       0.0452,  0.0923,  0.0914,  0.0414,  0.0894,  0.1096,  0.1007,  0.0684,\n",
       "                       0.0743,  0.1170,  0.0908,  0.0848,  0.0941,  0.1035,  0.1004,  0.0930,\n",
       "                       0.0931,  0.0904,  0.1070,  0.0794,  0.1216,  0.1261,  0.0983,  0.1117,\n",
       "                       0.1066,  0.0951,  0.1071,  0.0504,  0.0567,  0.1044,  0.0806,  0.1063,\n",
       "                       0.0711,  0.0973,  0.1162,  0.0999,  0.0981,  0.1145,  0.0745,  0.0912,\n",
       "                       0.0523,  0.0691,  0.0785,  0.0434,  0.0384,  0.1179,  0.0925,  0.1086,\n",
       "                       0.1072,  0.1206,  0.1002,  0.0842,  0.0719,  0.1096,  0.1016,  0.1036,\n",
       "                       0.0747,  0.0846,  0.0542,  0.0850,  0.0672,  0.0784,  0.0674,  0.0763,\n",
       "                       0.0855,  0.0714,  0.0613,  0.0884,  0.1139,  0.0888,  0.0883,  0.0965,\n",
       "                       0.0876,  0.0905,  0.0943,  0.0969,  0.0821,  0.0908,  0.0353,  0.1245,\n",
       "                       0.0993, -0.0009,  0.1486,  0.1098,  0.0919,  0.0929,  0.1020,  0.0833,\n",
       "                       0.1357,  0.1076,  0.0854,  0.0840,  0.1090,  0.0772,  0.0788,  0.1037,\n",
       "                       0.0873,  0.0438,  0.0836,  0.0901,  0.0800,  0.0816,  0.0842,  0.0644,\n",
       "                       0.1132,  0.0940,  0.0925,  0.1182,  0.1232,  0.0927,  0.1534,  0.1202,\n",
       "                       0.1054,  0.0876,  0.1126,  0.0908,  0.0878,  0.0882,  0.0921,  0.1388,\n",
       "                       0.0815,  0.0589,  0.0946,  0.0842,  0.0573,  0.1280,  0.0608,  0.0856,\n",
       "                       0.0866,  0.1014,  0.0990,  0.0791,  0.1082,  0.0721,  0.1014,  0.1246,\n",
       "                       0.1111,  0.1107,  0.0936,  0.0783,  0.0833,  0.0865,  0.1017,  0.1018,\n",
       "                       0.0743,  0.0876,  0.0987,  0.1383,  0.1142,  0.1156,  0.0941,  0.0863,\n",
       "                       0.1040,  0.1056,  0.0678,  0.0909,  0.1018,  0.0420,  0.1097,  0.0800,\n",
       "                       0.0909,  0.1017,  0.1167,  0.0925,  0.0949,  0.1192,  0.0883,  0.1125,\n",
       "                       0.0830,  0.1108,  0.0709,  0.0550,  0.0710,  0.0900,  0.0690,  0.0814,\n",
       "                       0.0892,  0.0994,  0.1053,  0.1055,  0.0230,  0.0789,  0.1209,  0.0746,\n",
       "                       0.0856,  0.0972,  0.1183,  0.0486,  0.1020,  0.0991,  0.1079,  0.1073,\n",
       "                       0.0919,  0.0287,  0.1019,  0.1167,  0.0623,  0.1180,  0.0854,  0.0502,\n",
       "                       0.1037,  0.1219,  0.1066,  0.1020,  0.1047,  0.0810,  0.1044,  0.0967,\n",
       "                       0.0704, -0.0016,  0.0735,  0.0951,  0.0837,  0.0736,  0.1091,  0.0815,\n",
       "                       0.0923,  0.0811,  0.0761,  0.0882,  0.0861,  0.0934,  0.0802,  0.0872,\n",
       "                       0.0610,  0.0857,  0.0305,  0.0879,  0.0901,  0.1014,  0.0723,  0.0485,\n",
       "                       0.1198,  0.0975,  0.1073,  0.0810,  0.1262,  0.0993,  0.0928,  0.0870,\n",
       "                       0.0645,  0.0863,  0.1055,  0.0871,  0.1029,  0.0887,  0.1250,  0.1290,\n",
       "                       0.0942,  0.0820,  0.1172,  0.1128,  0.0884,  0.1190,  0.0813,  0.1042,\n",
       "                       0.1014,  0.1170,  0.0772,  0.0832,  0.1115,  0.0768,  0.1149,  0.1023,\n",
       "                       0.1180,  0.0904,  0.1038,  0.0824,  0.0736,  0.0727,  0.0868,  0.1048,\n",
       "                       0.1005,  0.1082,  0.0539,  0.0789,  0.0744,  0.0821,  0.0751,  0.0786,\n",
       "                       0.0657,  0.0836,  0.0957,  0.1017,  0.0805,  0.0872,  0.0729,  0.0924,\n",
       "                       0.1072,  0.0519,  0.0954,  0.0835,  0.1212,  0.0875,  0.0799,  0.1052,\n",
       "                       0.0716,  0.1086,  0.1017,  0.1036,  0.0490,  0.0893,  0.0862,  0.1052,\n",
       "                       0.1051,  0.0824,  0.0861,  0.0823,  0.0964,  0.1048,  0.0872,  0.0286,\n",
       "                       0.0993,  0.1154,  0.1014,  0.1146,  0.0797,  0.1173,  0.1135,  0.0864,\n",
       "                       0.0441,  0.0735,  0.1262,  0.0821,  0.1000,  0.0573,  0.0771,  0.1046,\n",
       "                       0.0866,  0.0749,  0.0622,  0.0821,  0.1012,  0.0516,  0.1212,  0.1229,\n",
       "                       0.0962,  0.0896,  0.1264,  0.1044,  0.0814,  0.1041,  0.0823,  0.1014,\n",
       "                       0.0932,  0.0992,  0.1223,  0.1047,  0.1148,  0.0923,  0.0946,  0.1263,\n",
       "                       0.0806,  0.0982,  0.1141,  0.0971,  0.0873,  0.0877,  0.0931,  0.1071,\n",
       "                       0.0609,  0.1096,  0.0785,  0.0798,  0.0879,  0.0859,  0.1089,  0.0858,\n",
       "                       0.1152,  0.0667,  0.1171,  0.1134,  0.1102,  0.0988,  0.0174,  0.1043,\n",
       "                       0.0991,  0.0822,  0.1157,  0.0352,  0.0614,  0.0964,  0.0920,  0.0847,\n",
       "                       0.1044,  0.1186,  0.0932,  0.1053,  0.0924,  0.0833,  0.0886,  0.0932,\n",
       "                       0.0770,  0.0780,  0.0749,  0.0913,  0.0681,  0.0875,  0.0989,  0.1104,\n",
       "                       0.0826,  0.1013,  0.0979,  0.1191,  0.0936,  0.0977,  0.1190,  0.0974,\n",
       "                       0.1283,  0.0907,  0.1115,  0.0645,  0.1029,  0.0756,  0.0823,  0.1066,\n",
       "                       0.1145,  0.0917,  0.1155,  0.0879,  0.1087,  0.0837,  0.0947,  0.1385,\n",
       "                       0.0909,  0.1139,  0.0951,  0.1007,  0.1136,  0.0880,  0.1064,  0.1217,\n",
       "                       0.1080,  0.0980,  0.1059,  0.0822,  0.0959,  0.0736,  0.0916,  0.0898,\n",
       "                       0.0962,  0.1322,  0.0776,  0.0918,  0.0952,  0.0905,  0.1266,  0.1030,\n",
       "                       0.1007,  0.0898,  0.0890,  0.0907,  0.1224,  0.1100,  0.0986,  0.0740,\n",
       "                       0.0707,  0.0848,  0.0723,  0.0926,  0.0965,  0.0750,  0.0924,  0.1085,\n",
       "                       0.1000,  0.0873,  0.1013,  0.1209,  0.1322,  0.0014,  0.0816,  0.1445,\n",
       "                       0.1005,  0.0992,  0.0891,  0.0833,  0.1275,  0.0945,  0.1043,  0.0714,\n",
       "                       0.0917,  0.0685,  0.0949,  0.0863,  0.0951,  0.0865,  0.0931,  0.1101,\n",
       "                       0.0913,  0.0938,  0.1039,  0.0706,  0.0911,  0.0471,  0.0779,  0.1048,\n",
       "                       0.0838,  0.1005,  0.0800,  0.1050,  0.1213,  0.0824,  0.0880,  0.0878,\n",
       "                       0.1223,  0.0819,  0.1029,  0.0936,  0.0940,  0.0541,  0.1155,  0.0827,\n",
       "                       0.0983,  0.0861,  0.0836,  0.0891,  0.1221,  0.0698,  0.0006,  0.1061,\n",
       "                       0.1010,  0.0995,  0.0401,  0.1101,  0.1004,  0.1123,  0.1026,  0.1297,\n",
       "                       0.0806,  0.0698,  0.1173,  0.0784,  0.1106,  0.0975,  0.1179,  0.0651,\n",
       "                       0.0961,  0.1004,  0.0995,  0.0973,  0.0711,  0.0616,  0.0697,  0.1141,\n",
       "                       0.0989,  0.0938,  0.0946,  0.1004,  0.1042,  0.0878,  0.1076,  0.1143,\n",
       "                       0.0935,  0.0949,  0.0981,  0.0743,  0.1221,  0.0913,  0.0947,  0.0786,\n",
       "                       0.1142,  0.0869,  0.0856,  0.0962,  0.1017,  0.0921,  0.1078,  0.1173,\n",
       "                       0.0898,  0.0627,  0.0961,  0.0732,  0.1193,  0.1016,  0.0824,  0.1247],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.4.layer.1.DenseReluDense.wi.weight',\n",
       "              tensor([[-4.6868e-01, -1.9018e+00, -2.3681e-03,  ...,  1.3947e+00,\n",
       "                       -1.5065e-01,  2.6891e-01],\n",
       "                      [ 1.5333e+00,  1.0357e+00, -1.2216e+00,  ..., -3.3127e-01,\n",
       "                       -7.7472e-01,  1.0300e+00],\n",
       "                      [ 1.6275e+00, -1.6946e+00,  2.2655e-01,  ..., -1.4296e+00,\n",
       "                        1.2040e+00, -3.3829e-01],\n",
       "                      ...,\n",
       "                      [-1.3337e-02, -1.3960e-01,  2.5419e-01,  ...,  6.3784e-01,\n",
       "                        8.4977e-01,  5.8776e-01],\n",
       "                      [ 1.0690e+00,  2.5673e+00,  6.6995e-01,  ...,  3.0127e+00,\n",
       "                       -1.8756e+00,  3.1345e-01],\n",
       "                      [ 1.9065e-01, -1.1805e+00, -3.0928e-01,  ...,  1.9719e+00,\n",
       "                       -1.9953e+00, -2.0573e-01]], device='cuda:0')),\n",
       "             ('encoder.block.4.layer.1.DenseReluDense.wo.weight',\n",
       "              tensor([[-0.5091, -0.2456,  0.1046,  ...,  0.3425,  1.2616, -0.5034],\n",
       "                      [ 0.1145, -0.6635, -1.3488,  ..., -0.1292, -0.4628,  0.1258],\n",
       "                      [ 0.4204, -0.4161,  0.4222,  ...,  0.9346, -0.8901, -0.2752],\n",
       "                      ...,\n",
       "                      [ 0.6293, -0.2222, -0.3922,  ...,  0.0285, -0.0496,  0.1276],\n",
       "                      [ 0.7351, -1.0661,  0.6711,  ...,  0.6862,  0.3727,  0.6475],\n",
       "                      [ 0.2092, -1.2446, -0.7416,  ..., -0.5569,  0.0035,  0.6408]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.4.layer.1.layer_norm.weight',\n",
       "              tensor([ 8.0912e-01,  8.0667e-01,  6.4786e-01,  8.2254e-01,  4.4421e-01,\n",
       "                       6.5691e-01,  2.6531e-01,  7.2746e-01,  1.6709e+00,  6.0396e-01,\n",
       "                       8.2158e-01,  7.0964e-01,  5.2880e-01,  8.4561e-01,  5.7076e-01,\n",
       "                       6.0794e-01,  7.5990e-01,  6.6973e-01,  8.6120e-01,  5.9928e-01,\n",
       "                       5.3811e-01,  6.4469e-01,  1.0346e+00,  5.0991e-01,  5.1910e-01,\n",
       "                       7.7780e-01,  4.9495e-01,  7.1531e-01,  7.4711e-01,  4.3339e-01,\n",
       "                       5.7557e-01,  5.1605e-01,  7.8588e-01,  6.0756e-01,  7.5907e-01,\n",
       "                       6.8457e-01,  8.6897e-01,  9.0724e-01,  5.1238e-01,  9.9470e-01,\n",
       "                       7.8285e-01,  8.7463e-01,  8.6503e-01,  7.3358e-01,  1.8455e+00,\n",
       "                       5.1329e-01,  8.6406e-01,  5.6596e-01,  6.0744e-01,  1.0119e+00,\n",
       "                       7.1015e-01,  6.7133e-01,  1.1623e+00,  5.4481e-01,  6.0270e-01,\n",
       "                       6.6971e-01,  8.6544e-01,  6.8905e-01,  5.9422e-01,  6.4561e-01,\n",
       "                       7.4506e-01,  4.8879e-01,  6.7775e-01,  5.9565e-01,  5.1308e-01,\n",
       "                       1.3461e+00,  5.6012e-01,  5.4130e-01,  8.2909e-01,  5.1183e-01,\n",
       "                       6.2495e-01,  1.0914e+00,  5.4803e-01,  5.4388e-01,  6.5213e-01,\n",
       "                       5.6723e-01,  9.7508e-01,  7.4547e-01,  5.7551e-01,  7.4353e-01,\n",
       "                       4.7610e-01,  7.4520e-01,  5.9699e-01,  9.0543e-01,  7.7594e-01,\n",
       "                       6.5301e-01,  8.4430e-01,  5.9256e-01,  5.5308e-01,  4.8001e-01,\n",
       "                       6.9883e-01,  6.3869e-01,  5.8145e-01,  8.3541e-01,  5.2811e-01,\n",
       "                       4.8048e-01,  7.0704e-01,  5.5009e-01,  5.2381e-01,  5.6726e-01,\n",
       "                       8.3202e-01,  6.3891e-01,  9.8759e-01,  5.2769e-01,  6.7207e-01,\n",
       "                       1.0297e+00,  6.6235e-01,  6.0059e-01,  4.3562e-01,  1.2025e+00,\n",
       "                       7.3100e-01,  5.7547e-01,  6.1991e-01,  6.3886e-01,  5.2859e-01,\n",
       "                       5.1178e-01,  6.0556e-01,  4.5791e-01,  1.1763e+00,  5.6189e-01,\n",
       "                       4.1180e-01,  5.5815e-01,  9.8534e-01,  6.9419e-01,  8.8446e-01,\n",
       "                       6.0644e-01,  6.7924e-01,  7.1333e-01,  5.7826e-01,  7.3949e-01,\n",
       "                       1.1855e+00,  6.8557e-01,  5.2213e-01,  5.5109e-01,  5.5283e-01,\n",
       "                       1.0122e+00,  8.7797e-01,  4.9492e-01,  7.6516e-01,  7.4235e-01,\n",
       "                       5.5864e-01,  7.9070e-01,  9.4267e-01,  9.6699e-01,  6.6107e-01,\n",
       "                       8.6895e-01,  6.4051e-01,  8.1244e-01,  5.8434e-01,  6.5163e-01,\n",
       "                       5.7239e-01,  7.0196e-01,  6.6988e-01,  7.7651e-01,  5.6605e-01,\n",
       "                       4.8805e-01,  7.6860e-01,  4.2135e-01,  8.2887e-01,  6.4527e-01,\n",
       "                       5.5317e-01,  5.9459e-01,  6.0236e-01,  1.9404e+00,  5.3620e-01,\n",
       "                       5.1103e-01,  5.5572e-01,  7.2764e-01,  5.9643e-01,  7.5886e-01,\n",
       "                       9.2556e-01,  5.4659e-01,  6.4316e-01,  9.4981e-01,  6.1650e-01,\n",
       "                       7.3849e-01,  6.9565e-01,  7.6025e-01,  7.7661e-01,  3.7406e-01,\n",
       "                       6.2933e-01,  4.5676e-01,  7.1229e-01,  4.9672e-01,  6.6036e-01,\n",
       "                       8.2433e-01,  5.4904e-01,  9.1147e-01,  6.1679e-01,  6.9988e-01,\n",
       "                       4.6882e-01,  5.7573e-01,  2.8760e-01,  5.4852e-01,  5.3087e-01,\n",
       "                       2.2123e-01,  6.5448e-01,  8.6300e-01,  6.4431e-01,  4.9187e-01,\n",
       "                       4.4729e-01,  7.0796e-01,  6.0741e-01,  5.8737e-01,  6.3360e-01,\n",
       "                       9.0528e-01,  6.2362e-01,  6.2521e-01,  6.8155e-01,  6.7670e-01,\n",
       "                       6.7248e-01,  5.6119e-01,  8.1754e-01,  8.7736e-01,  5.8521e-01,\n",
       "                       6.4901e-01,  7.0671e-01,  5.9947e-01,  7.9019e-01,  1.3803e+00,\n",
       "                       7.5989e-01,  6.4238e-01,  4.0362e-01,  6.7042e-01,  8.1902e-01,\n",
       "                       6.2671e-01,  8.4634e-01,  7.4366e-01,  5.9055e-01,  7.9516e-01,\n",
       "                       5.2358e-01,  7.5010e-01,  3.3506e-01,  5.1700e-01,  4.7000e-01,\n",
       "                       2.9248e-01, -1.4272e-03,  8.3806e-01,  7.4762e-01,  8.8206e-01,\n",
       "                       9.2146e-01,  7.9850e-01,  6.5705e-01,  5.6428e-01,  4.8001e-01,\n",
       "                       5.8952e-01,  6.6039e-01,  7.7267e-01,  4.6830e-01,  6.6127e-01,\n",
       "                       1.9529e+00,  5.4485e-01,  6.1732e-01,  5.4553e-01,  5.1871e-01,\n",
       "                       3.9481e-01,  5.7977e-01,  5.2526e-01,  8.0868e-01,  4.9771e-01,\n",
       "                       9.1362e-01, -2.3834e-02,  5.5846e-01,  6.2868e-01,  6.2656e-01,\n",
       "                       5.6241e-01,  5.1611e-01,  6.8321e-01,  5.6342e-01,  5.6396e-01,\n",
       "                       4.9102e-01,  9.3608e-01,  9.2659e-01,  1.3018e+00,  9.4041e-01,\n",
       "                       7.3536e-01,  6.6659e-01,  9.1822e-01,  5.9315e-01,  6.1663e-01,\n",
       "                       8.7734e-01,  8.7960e-01,  6.2792e-01,  1.0629e+00,  6.7336e-01,\n",
       "                       9.0414e-01,  5.5893e-01,  5.6628e-01,  6.6973e-01,  1.3351e+00,\n",
       "                       4.9148e-01,  7.0449e-01,  4.8060e-01,  4.8166e-01,  5.3350e-01,\n",
       "                       5.4217e-01,  9.2123e-01,  6.1376e-01,  5.6788e-01,  8.0988e-01,\n",
       "                       8.6674e-01,  6.5821e-01,  1.0355e+00,  6.9772e-01,  5.7329e-01,\n",
       "                       6.0869e-01,  6.3236e-01,  6.3369e-01,  6.8334e-01,  6.8417e-01,\n",
       "                       5.8688e-01,  8.5071e-01,  5.2861e-01,  6.6540e-01,  6.7390e-01,\n",
       "                       6.8822e-01,  4.3860e-01,  8.7903e-01,  9.0473e-01,  5.8167e-01,\n",
       "                       3.7591e-01,  8.3903e-01,  6.5939e-01,  4.6420e-01,  8.7568e-01,\n",
       "                       4.8189e-01,  9.6307e-01,  8.6999e-01,  6.7835e-01,  6.3825e-01,\n",
       "                       5.8589e-01,  1.0273e+00,  7.3938e-01,  6.2750e-01,  5.9227e-01,\n",
       "                       6.7292e-01,  5.0379e-01,  4.6630e-01,  6.6383e-01,  1.0261e+00,\n",
       "                       8.3423e-01,  6.1966e-01,  6.7464e-01,  5.9305e-01,  7.0710e-01,\n",
       "                       6.0298e-01,  8.2226e-01,  7.1942e-01,  9.1516e-01,  1.2998e+00,\n",
       "                       7.6164e-01,  4.6560e-01,  6.4419e-01,  5.7119e-01,  7.9307e-01,\n",
       "                       6.6556e-01,  6.0699e-01,  9.0771e-01,  5.4158e-01,  8.5984e-01,\n",
       "                       5.4957e-01,  9.3063e-01,  1.0012e+00,  2.9871e-01,  5.2660e-01,\n",
       "                       6.6910e-01,  9.5948e-01,  7.2947e-01,  5.9986e-01,  7.4385e-01,\n",
       "                       7.9518e-01,  7.8125e-01,  3.3266e-02,  5.5584e-01,  9.0217e-01,\n",
       "                       4.9174e-01,  8.3277e-01,  5.6131e-01,  9.6895e-01,  9.0451e-01,\n",
       "                       6.5994e-01,  7.7969e-01,  8.5386e-01,  5.9868e-01,  5.4810e-01,\n",
       "                       9.8270e-01,  5.6338e-01,  1.0019e+00,  3.4034e-01,  7.7140e-01,\n",
       "                       6.4899e-01,  1.4446e+00,  5.9122e-01,  7.7334e-01,  7.6089e-01,\n",
       "                       6.6424e-01,  7.9947e-01,  4.9215e-01,  6.6847e-01,  5.3866e-01,\n",
       "                       4.3991e-01,  5.9294e-01,  4.5349e-01,  5.1418e-01,  5.9796e-01,\n",
       "                       5.5463e-01,  9.7452e-01,  5.3086e-01,  5.9010e-01,  5.2482e-01,\n",
       "                       6.2603e-01,  4.8903e-01,  6.8403e-01,  6.6275e-01,  4.9023e-01,\n",
       "                       6.1491e-01,  5.6859e-01,  5.4219e-01,  1.0821e-02,  8.5095e-01,\n",
       "                       1.0043e+00,  6.9413e-01,  4.5785e-01,  2.2355e-01,  6.4090e-01,\n",
       "                       5.8384e-01,  6.9912e-01,  5.5511e-01,  7.4108e-01,  7.4068e-01,\n",
       "                       7.7576e-01,  5.8863e-01,  1.0680e+00,  4.8953e-01,  9.8496e-01,\n",
       "                       6.0656e-01,  6.2752e-01,  5.4803e-01,  7.3936e-01,  7.6915e-01,\n",
       "                       6.4792e-01,  7.8214e-01,  7.5969e-01,  9.4398e-01,  6.0796e-01,\n",
       "                       7.0348e-01,  4.8637e-01,  7.0529e-01,  8.8120e-01,  6.9496e-01,\n",
       "                       6.5853e-01,  4.8626e-01,  7.0064e-01,  5.8736e-01,  7.3128e-01,\n",
       "                       8.1610e-01,  7.8269e-01,  6.1798e-01,  8.7071e-01,  4.5078e-01,\n",
       "                       4.9164e-01,  5.5376e-01,  5.6373e-01,  6.8499e-01,  6.8201e-01,\n",
       "                       6.1730e-01,  4.0989e-01,  5.7342e-01,  5.1024e-01,  5.7991e-01,\n",
       "                       3.8921e-01,  5.1525e-01,  4.7550e-01,  5.6268e-01,  8.1083e-01,\n",
       "                       4.6479e-01,  5.0739e-01,  5.3841e-01,  9.4581e-01,  5.9591e-01,\n",
       "                       7.5247e-01,  1.4120e+00,  5.9618e-01,  5.7752e-01,  7.8538e-01,\n",
       "                       6.1721e-01,  5.1909e-01,  7.9781e-01,  5.1759e-01,  7.6171e-01,\n",
       "                       6.4308e-01,  6.9504e-01,  9.6992e-01,  5.1588e-01,  7.9993e-01,\n",
       "                       1.0090e+00,  6.7512e-01,  7.3515e-01,  5.6126e-01,  5.2034e-01,\n",
       "                       6.1417e-01,  7.8103e-01,  8.1583e-01,  1.1396e+00,  5.6358e-01,\n",
       "                       6.9242e-01,  6.6759e-01,  7.8534e-01,  4.7850e-01,  8.4190e-01,\n",
       "                       7.3776e-01,  5.7902e-01,  1.5975e+00,  4.8315e-01,  9.1205e-01,\n",
       "                       5.7246e-01,  5.8097e-01,  6.0209e-01,  1.0476e+00,  8.2769e-01,\n",
       "                       5.5661e-01,  7.0036e-01,  4.4029e-01,  6.0149e-01,  6.3917e-01,\n",
       "                       1.4038e+00,  7.7994e-01,  9.9084e-01,  6.0649e-01,  7.8680e-01,\n",
       "                       8.2208e-01,  7.2064e-01,  6.8973e-01,  5.7688e-01,  7.4905e-01,\n",
       "                       7.2896e-01,  6.6018e-01,  5.6769e-01,  9.1473e-01,  6.1081e-01,\n",
       "                       7.3796e-01,  5.0513e-01,  6.7561e-01,  8.8204e-01,  5.4895e-01,\n",
       "                       6.0809e-01,  7.1038e-01,  6.5037e-01,  5.8289e-01,  5.6883e-01,\n",
       "                       9.1969e-01,  8.2958e-01,  4.2438e-01,  7.9672e-01,  5.5963e-01,\n",
       "                       5.7838e-01,  7.2204e-01,  5.9510e-01,  6.6632e-01,  9.0116e-01,\n",
       "                       7.8112e-01,  4.6068e-01,  9.6448e-01,  9.0123e-01,  6.6805e-01,\n",
       "                       6.6362e-01,  3.0094e-03,  7.5322e-01,  6.8262e-01,  5.8571e-01,\n",
       "                       6.8563e-01,  7.2788e-02,  4.7872e-01,  6.2373e-01,  5.8574e-01,\n",
       "                       7.1516e-01,  6.0502e-01,  6.0767e-01,  5.0005e-01,  1.1538e+00,\n",
       "                       5.3658e-01,  5.1399e-01,  6.2864e-01,  4.6240e-01,  4.6257e-01,\n",
       "                       4.9900e-01,  4.4721e-01,  5.2009e-01,  5.7511e-01,  5.6133e-01,\n",
       "                       6.7644e-01,  8.1125e-01,  5.8747e-01,  6.9943e-01,  7.3421e-01,\n",
       "                       6.0859e-01,  6.4314e-01,  5.9609e-01,  1.2480e+00,  6.1539e-01,\n",
       "                       8.9786e-01,  5.1437e-01,  6.9587e-01,  2.0974e-01,  6.5780e-01,\n",
       "                       4.5756e-01,  3.6925e-01,  7.5479e-01,  9.3821e-01,  5.6975e-01,\n",
       "                       6.8763e-01,  5.8730e-01,  7.3072e-01,  6.0692e-01,  5.7103e-01,\n",
       "                       8.8570e-01,  4.9188e-01,  6.2524e-01,  6.2492e-01,  6.5536e-01,\n",
       "                       7.5797e-01,  5.0408e-01,  6.9339e-01,  9.0261e-01,  6.3658e-01,\n",
       "                       6.3400e-01,  6.6148e-01,  1.0077e+00,  6.1611e-01,  9.5629e-01,\n",
       "                       8.5228e-01,  5.9822e-01,  6.6994e-01,  1.0402e+00,  5.2699e-01,\n",
       "                       7.1471e-01,  7.3971e-01,  6.1843e-01,  7.8347e-01,  6.3065e-01,\n",
       "                       7.8596e-01,  7.0959e-01,  5.8528e-01,  7.6585e-01,  7.5273e-01,\n",
       "                       7.5462e-01,  7.6823e-01,  4.8533e-01,  4.8637e-01,  5.2333e-01,\n",
       "                       5.4313e-01,  6.6850e-01,  6.0356e-01,  7.1784e-01,  7.8656e-01,\n",
       "                       8.5553e-01,  5.2596e-01,  6.3178e-01,  6.7229e-01,  7.5615e-01,\n",
       "                       7.5050e-01,  2.3359e-02,  5.7555e-01,  7.9555e-01,  6.2999e-01,\n",
       "                       7.7743e-01,  6.7380e-01,  5.8539e-01,  5.9682e-01,  5.9793e-01,\n",
       "                       7.1037e-01,  9.4210e-01,  6.5084e-01,  6.1064e-01,  6.1709e-01,\n",
       "                       6.1586e-01,  5.9355e-01,  6.1077e-01,  6.4063e-01,  6.5441e-01,\n",
       "                       6.1662e-01,  6.1744e-01,  6.6437e-01,  5.4202e-01,  6.4356e-01,\n",
       "                       1.1332e+00,  4.8265e-01,  7.8876e-01,  4.9013e-01,  7.3390e-01,\n",
       "                       5.0919e-01,  7.5125e-01,  1.1681e+00,  4.5407e-01,  6.5528e-01,\n",
       "                       5.8842e-01,  9.0641e-01,  4.8338e-01,  6.4422e-01,  7.9098e-01,\n",
       "                       7.8625e-01,  3.6200e-01,  6.6443e-01,  5.9481e-01,  7.0152e-01,\n",
       "                       5.2057e-01,  6.1553e-01,  9.6403e-01,  8.7170e-01,  5.1802e-01,\n",
       "                       1.4961e+00,  6.3104e-01,  6.4907e-01,  5.9159e-01,  9.4676e-02,\n",
       "                       6.7086e-01,  8.2604e-01,  8.6550e-01,  6.8126e-01,  8.9566e-01,\n",
       "                       5.0497e-01,  6.6470e-01,  8.3302e-01,  5.5231e-01,  5.2436e-01,\n",
       "                       7.0208e-01,  5.5636e-01,  4.7158e-01,  4.2812e-01,  7.7858e-01,\n",
       "                       7.5619e-01,  5.8529e-01,  5.5406e-01,  2.9791e-01,  6.0966e-01,\n",
       "                       5.9958e-01,  5.2629e-01,  6.9656e-01,  7.2023e-01,  6.7040e-01,\n",
       "                       6.5968e-01,  9.8341e-01,  6.8649e-01,  7.9033e-01,  5.1222e-01,\n",
       "                       6.8488e-01,  7.4416e-01,  5.1420e-01,  7.3761e-01,  8.3020e-01,\n",
       "                       5.8286e-01,  6.5631e-01,  9.4789e-01,  6.6457e-01,  7.2198e-01,\n",
       "                       6.0204e-01,  6.5955e-01,  5.7544e-01,  6.0642e-01,  7.7447e-01,\n",
       "                       6.6377e-01,  3.3017e-01,  6.7513e-01,  4.6683e-01,  6.9973e-01,\n",
       "                       6.4471e-01,  6.2878e-01,  8.7100e-01], device='cuda:0')),\n",
       "             ('encoder.block.5.layer.0.SelfAttention.q.weight',\n",
       "              tensor([[ 0.0490, -0.0139,  0.0218,  ...,  0.0660, -0.0188,  0.0820],\n",
       "                      [-0.0023,  0.1023, -0.0239,  ..., -0.0830,  0.1080,  0.1109],\n",
       "                      [ 0.0918, -0.0083,  0.0145,  ...,  0.0908,  0.1070, -0.0120],\n",
       "                      ...,\n",
       "                      [-0.0963, -0.0873,  0.0312,  ...,  0.0485,  0.0932,  0.0259],\n",
       "                      [-0.0325, -0.1130,  0.0950,  ...,  0.1584,  0.0190,  0.0453],\n",
       "                      [ 0.0562, -0.0219, -0.0319,  ..., -0.0404, -0.1154, -0.0098]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.5.layer.0.SelfAttention.k.weight',\n",
       "              tensor([[-0.3242, -0.5623, -0.0873,  ...,  0.2611,  0.7032,  0.5736],\n",
       "                      [ 0.6358, -0.0529, -0.2594,  ..., -0.0946,  0.6027,  0.1594],\n",
       "                      [ 0.5042, -0.2816, -0.6466,  ...,  0.4402,  0.5013,  0.3972],\n",
       "                      ...,\n",
       "                      [-0.1759, -0.6814,  0.5353,  ...,  0.1182,  0.4518,  0.6276],\n",
       "                      [ 0.0610,  0.4857,  0.0723,  ..., -0.6442,  0.8104,  0.8160],\n",
       "                      [ 0.4860, -0.1304, -0.4141,  ..., -0.2270, -0.4257, -0.3474]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.5.layer.0.SelfAttention.v.weight',\n",
       "              tensor([[-0.3660,  0.9141, -0.4646,  ..., -0.6444,  0.5977,  0.3617],\n",
       "                      [-0.3493, -0.4008, -0.7041,  ..., -0.8183,  0.2677, -0.8613],\n",
       "                      [-0.2785, -2.4629,  0.0400,  ...,  0.6658,  1.2083, -0.8166],\n",
       "                      ...,\n",
       "                      [ 0.5352, -0.6987,  0.5145,  ...,  1.3359, -2.4062, -1.1461],\n",
       "                      [-0.9547,  1.3564,  2.0911,  ...,  1.1922, -0.7675, -0.4391],\n",
       "                      [-0.7474,  0.9020,  0.0691,  ..., -0.3260,  0.1282,  0.8559]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.5.layer.0.SelfAttention.o.weight',\n",
       "              tensor([[-1.2766e+00,  3.5724e-01, -1.2440e+00,  ..., -5.9756e-01,\n",
       "                       -1.9284e+00, -7.6766e-01],\n",
       "                      [-1.2659e+00, -2.3140e-01,  3.3579e+00,  ..., -1.0361e+00,\n",
       "                        7.9112e-01, -1.0287e-01],\n",
       "                      [-1.0640e+00,  1.3552e+00,  3.0037e-01,  ...,  7.1269e-01,\n",
       "                        9.4553e-01,  1.4874e-01],\n",
       "                      ...,\n",
       "                      [ 8.9491e-01, -6.5411e-01, -8.7079e-01,  ..., -2.1735e-02,\n",
       "                       -1.4905e+00, -1.3937e+00],\n",
       "                      [ 4.5010e-01, -3.6263e-01, -6.1055e-01,  ...,  2.3359e-01,\n",
       "                       -2.4273e-04, -8.4988e-02],\n",
       "                      [ 2.8535e-01,  7.9693e-01, -8.6340e-01,  ...,  1.3279e+00,\n",
       "                        2.3334e+00,  1.1935e+00]], device='cuda:0')),\n",
       "             ('encoder.block.5.layer.0.layer_norm.weight',\n",
       "              tensor([ 0.1209,  0.1081,  0.1104,  0.1206,  0.0814,  0.1099,  0.0179,  0.1205,\n",
       "                       0.0279,  0.0977,  0.1180,  0.1147,  0.0569,  0.0916,  0.0935,  0.0932,\n",
       "                       0.1149,  0.1284,  0.1095,  0.0858,  0.0813,  0.1027,  0.0179,  0.0856,\n",
       "                       0.0886,  0.0898,  0.0771,  0.1110,  0.1064,  0.0868,  0.0882,  0.0951,\n",
       "                       0.1061,  0.0812,  0.1071,  0.1128,  0.0613,  0.1324,  0.0652,  0.1198,\n",
       "                       0.1089,  0.1058,  0.1372,  0.1039, -0.0098,  0.1015,  0.0892,  0.0770,\n",
       "                       0.1217,  0.0268,  0.1162,  0.1152,  0.0626,  0.0768,  0.0021,  0.1068,\n",
       "                       0.0428,  0.0933,  0.1079,  0.0978,  0.1240,  0.0658,  0.1042,  0.0807,\n",
       "                       0.0935,  0.0383,  0.1059,  0.0880,  0.1292,  0.1152,  0.0963,  0.1089,\n",
       "                       0.1058,  0.0904,  0.0783,  0.0908,  0.0835,  0.1164,  0.0724,  0.1172,\n",
       "                       0.0863,  0.1280,  0.1061,  0.1220,  0.1006,  0.1108,  0.1172,  0.1088,\n",
       "                       0.0999,  0.0841,  0.1050,  0.0745,  0.0813,  0.0942,  0.0810,  0.0746,\n",
       "                       0.1015,  0.0969,  0.0960,  0.0745,  0.1241,  0.0863,  0.0808,  0.0916,\n",
       "                       0.0866,  0.0865,  0.0874,  0.0834,  0.0872,  0.0466,  0.0877,  0.0915,\n",
       "                       0.0955,  0.0731,  0.0914,  0.0861,  0.1104,  0.0901,  0.1122,  0.0877,\n",
       "                       0.0712,  0.1007,  0.1098,  0.1041,  0.1202,  0.1067,  0.1270,  0.1011,\n",
       "                       0.0812,  0.1331,  0.0465,  0.1160,  0.0869,  0.0632,  0.0972,  0.0799,\n",
       "                       0.1133,  0.1070,  0.1086,  0.1137,  0.0814,  0.1324,  0.1182,  0.0829,\n",
       "                       0.1136,  0.1207,  0.0783,  0.0981,  0.0874,  0.1068,  0.1062,  0.1082,\n",
       "                       0.1103,  0.1263,  0.0787,  0.0866,  0.1073, -0.0812,  0.0773,  0.1034,\n",
       "                       0.0825,  0.0944,  0.0600,  0.0228,  0.0792,  0.0707,  0.0852,  0.1010,\n",
       "                       0.1234,  0.1310,  0.1237,  0.1038,  0.1162,  0.0721,  0.1046,  0.0940,\n",
       "                       0.1227,  0.0854,  0.0871,  0.0094,  0.0826,  0.1000,  0.1048,  0.0928,\n",
       "                       0.1124,  0.1109,  0.0783,  0.1333,  0.1035,  0.0980,  0.0867,  0.0815,\n",
       "                       0.0487,  0.0968,  0.0761,  0.0383,  0.0981,  0.1161,  0.1076,  0.0724,\n",
       "                       0.0849,  0.1004,  0.1026,  0.1085,  0.0964,  0.1127,  0.0893,  0.1029,\n",
       "                       0.0843,  0.0891,  0.0986,  0.0764,  0.1102,  0.1176,  0.1115,  0.1078,\n",
       "                       0.1012,  0.1119,  0.1232,  0.0575,  0.0577,  0.0929,  0.0739,  0.1162,\n",
       "                       0.0784,  0.0906,  0.1135,  0.1287,  0.0980,  0.1108,  0.0776,  0.1004,\n",
       "                       0.0580,  0.0778,  0.0940,  0.0467,  0.0518,  0.1117,  0.0892,  0.1055,\n",
       "                       0.1119,  0.1234,  0.0985,  0.0920,  0.0875,  0.0947,  0.0949,  0.1241,\n",
       "                       0.0721,  0.0918,  0.0638,  0.0842,  0.0746, -0.0769,  0.0905,  0.0801,\n",
       "                       0.0962,  0.0709,  0.0587,  0.0991,  0.1126,  0.0852,  0.0989,  0.1056,\n",
       "                       0.0867,  0.0842,  0.0983,  0.0862,  0.1043,  0.0965,  0.0385,  0.1474,\n",
       "                       0.1043,  0.0053,  0.1465,  0.0994,  0.1050,  0.1041,  0.1039,  0.1006,\n",
       "                       0.1452,  0.1280,  0.0934,  0.0904,  0.1015,  0.0883,  0.0862,  0.0938,\n",
       "                       0.0949,  0.0585,  0.0848,  0.0944,  0.0777,  0.0771,  0.0869,  0.0669,\n",
       "                       0.1248,  0.0997,  0.0883,  0.1202,  0.1237,  0.1054,  0.1440,  0.1095,\n",
       "                       0.0954,  0.0820,  0.1104,  0.0859,  0.0850,  0.1096,  0.1148,  0.1440,\n",
       "                       0.0731,  0.0661,  0.0947,  0.0907,  0.0588,  0.1362,  0.0683,  0.0944,\n",
       "                       0.0798,  0.1171,  0.1079,  0.0822,  0.1140,  0.0833,  0.1204,  0.1329,\n",
       "                       0.0966,  0.1099,  0.0697,  0.0857,  0.1028,  0.0929,  0.0956,  0.1051,\n",
       "                       0.0738,  0.0954,  0.0964,  0.1449,  0.0973,  0.1126,  0.0898,  0.0967,\n",
       "                       0.1018,  0.1092,  0.0855,  0.1105,  0.1034,  0.0477,  0.0960,  0.0920,\n",
       "                       0.0868,  0.0934,  0.1258,  0.1106,  0.1084,  0.1194,  0.0728,  0.1261,\n",
       "                       0.0978,  0.1304,  0.0969,  0.0520,  0.0813,  0.0946,  0.0669,  0.0952,\n",
       "                       0.0766,  0.1102,  0.0835,  0.1163,  0.0230,  0.0749,  0.1327,  0.0819,\n",
       "                       0.0998,  0.0873,  0.1386,  0.0567,  0.0874,  0.1070,  0.1279,  0.0952,\n",
       "                       0.1073,  0.0254,  0.0830,  0.1062,  0.0630,  0.1191,  0.0991,  0.0535,\n",
       "                       0.0960,  0.1312,  0.1248,  0.0923,  0.1225,  0.0875,  0.0975,  0.0928,\n",
       "                       0.0825, -0.0010,  0.0774,  0.1125,  0.0871,  0.0965,  0.1036,  0.0926,\n",
       "                       0.0828,  0.0724,  0.0910,  0.0868,  0.0909,  0.1012,  0.0904,  0.0896,\n",
       "                       0.0745,  0.0841,  0.0370,  0.1122,  0.1017,  0.1084,  0.0740,  0.0494,\n",
       "                       0.1210,  0.0953,  0.1267,  0.1024,  0.1210,  0.1071,  0.1256,  0.1122,\n",
       "                       0.0710,  0.0811,  0.1138,  0.0926,  0.0969,  0.0906,  0.1084,  0.1385,\n",
       "                       0.1175,  0.0707,  0.1075,  0.1263,  0.0852,  0.1201,  0.0909,  0.1146,\n",
       "                       0.1160,  0.1168,  0.0758,  0.1121,  0.1090,  0.0900,  0.1120,  0.1154,\n",
       "                       0.1229,  0.1094,  0.0958,  0.0828,  0.0886,  0.0838,  0.0945,  0.1152,\n",
       "                       0.1193,  0.1024,  0.0542,  0.0873,  0.0771,  0.0883,  0.0715,  0.0779,\n",
       "                       0.0734,  0.0778,  0.1099,  0.0814,  0.1054,  0.0988,  0.0661,  0.1020,\n",
       "                       0.1233,  0.0544,  0.0926,  0.0878,  0.1366,  0.1000,  0.0926,  0.1017,\n",
       "                       0.0826,  0.1018,  0.0933,  0.1022,  0.0450,  0.0877,  0.0908,  0.1180,\n",
       "                       0.1058,  0.0998,  0.0750,  0.0812,  0.0902,  0.1179,  0.0910,  0.0270,\n",
       "                       0.1019,  0.1165,  0.1193,  0.1165,  0.0669,  0.1153,  0.1201,  0.0808,\n",
       "                       0.0489,  0.0834,  0.1173,  0.0833,  0.0907,  0.0792,  0.0805,  0.1088,\n",
       "                       0.0788,  0.1024,  0.0775,  0.0908,  0.1004,  0.0479,  0.1323,  0.1220,\n",
       "                       0.0832,  0.1251,  0.1230,  0.0957,  0.1064,  0.0943,  0.1103,  0.1113,\n",
       "                       0.0956,  0.1153,  0.1309,  0.1174,  0.1208,  0.0896,  0.1046,  0.1307,\n",
       "                       0.0957,  0.0953,  0.1110,  0.0969,  0.0963,  0.0638,  0.1023,  0.1248,\n",
       "                       0.0778,  0.1467,  0.0980,  0.0925,  0.1051,  0.0932,  0.1007,  0.0887,\n",
       "                       0.1163,  0.0712,  0.1186,  0.1102,  0.1137,  0.1103,  0.0177,  0.0979,\n",
       "                       0.1032,  0.0956,  0.1084,  0.0430,  0.0607,  0.1009,  0.1019,  0.1247,\n",
       "                       0.1017,  0.1176,  0.0841,  0.1061,  0.0981,  0.0781,  0.1020,  0.0885,\n",
       "                       0.0742,  0.0854,  0.0827,  0.0923,  0.0860,  0.0964,  0.0963,  0.1146,\n",
       "                       0.0936,  0.1099,  0.1014,  0.1129,  0.1035,  0.0981,  0.1386,  0.0938,\n",
       "                       0.1333,  0.0972,  0.1176,  0.0670,  0.1098,  0.0704,  0.0819,  0.1137,\n",
       "                       0.1320,  0.1057,  0.1113,  0.0954,  0.1132,  0.0790,  0.1074,  0.1267,\n",
       "                       0.0807,  0.1091,  0.1049,  0.0960,  0.1100,  0.0880,  0.0907,  0.1418,\n",
       "                       0.1192,  0.0866,  0.0994,  0.0938,  0.1053,  0.0956,  0.1108,  0.0948,\n",
       "                       0.1091,  0.1261,  0.0994,  0.0959,  0.1005,  0.1095,  0.1397,  0.0991,\n",
       "                       0.0975,  0.1062,  0.0882,  0.0910,  0.1223,  0.1209,  0.1092,  0.0895,\n",
       "                       0.0703,  0.0693,  0.0793,  0.1005,  0.0996,  0.0923,  0.0979,  0.1141,\n",
       "                       0.0951,  0.0892,  0.1064,  0.1113,  0.1116,  0.0011,  0.0881,  0.1413,\n",
       "                       0.0928,  0.1072,  0.1015,  0.0792,  0.1182,  0.0891,  0.1065,  0.0636,\n",
       "                       0.0976,  0.0771,  0.1219,  0.0910,  0.0932,  0.0880,  0.0897,  0.1038,\n",
       "                       0.1066,  0.0832,  0.1059,  0.0821,  0.1011,  0.0489,  0.0683,  0.1143,\n",
       "                       0.0889,  0.1184,  0.0832,  0.1157,  0.1231,  0.0825,  0.0885,  0.0795,\n",
       "                       0.1249,  0.0785,  0.0989,  0.1097,  0.1057,  0.0602,  0.1092,  0.1083,\n",
       "                       0.0996,  0.0786,  0.0865,  0.1065,  0.1416,  0.0918,  0.0131,  0.1035,\n",
       "                       0.1077,  0.0903,  0.0370,  0.1169,  0.0996,  0.1143,  0.1017,  0.1252,\n",
       "                       0.0842,  0.0686,  0.1235,  0.0682,  0.1029,  0.1088,  0.1106,  0.0787,\n",
       "                       0.0944,  0.1147,  0.1172,  0.0910,  0.0772,  0.0521,  0.0824,  0.1087,\n",
       "                       0.0954,  0.1152,  0.0892,  0.1172,  0.1113,  0.1090,  0.1132,  0.1204,\n",
       "                       0.0890,  0.1056,  0.1192,  0.0833,  0.1143,  0.1178,  0.1089,  0.0937,\n",
       "                       0.1224,  0.0966,  0.1033,  0.1112,  0.0899,  0.0936,  0.1038,  0.1207,\n",
       "                       0.1071,  0.0633,  0.0934,  0.0856,  0.1072,  0.1249,  0.0930,  0.1248],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.5.layer.1.DenseReluDense.wi.weight',\n",
       "              tensor([[ 1.0424,  1.0035,  0.1802,  ..., -1.0453, -1.4203,  0.3773],\n",
       "                      [-1.9681, -0.8651,  3.6970,  ..., -0.5469, -0.2802,  1.4832],\n",
       "                      [-0.2360, -0.7988,  1.6973,  ...,  1.8541,  0.7780,  0.9519],\n",
       "                      ...,\n",
       "                      [-0.6020, -0.0912,  0.5264,  ..., -1.7351, -0.4900,  0.4479],\n",
       "                      [ 0.7068, -1.0215, -0.4548,  ..., -0.7827, -0.2961,  1.8065],\n",
       "                      [-0.0704, -1.3339,  0.6873,  ...,  0.2529,  1.7637, -0.0479]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.5.layer.1.DenseReluDense.wo.weight',\n",
       "              tensor([[-0.3301, -1.0440, -0.0331,  ...,  0.1782, -1.3983, -0.4781],\n",
       "                      [ 0.5963, -0.7306, -1.6807,  ...,  0.5824,  0.6236,  0.2341],\n",
       "                      [ 0.3927, -0.7068,  0.8062,  ...,  0.3867,  0.3161, -0.9514],\n",
       "                      ...,\n",
       "                      [-0.8660, -0.0113,  0.7884,  ..., -0.2408, -0.2422,  0.0247],\n",
       "                      [ 0.6489,  1.2579, -0.0392,  ...,  0.5442,  0.6427, -0.3291],\n",
       "                      [-0.4106, -0.3117,  0.4570,  ...,  0.1624, -0.1173, -0.0298]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.5.layer.1.layer_norm.weight',\n",
       "              tensor([ 0.6758,  0.7409,  0.6311,  0.8423,  0.5205,  0.6317,  0.1995,  0.8064,\n",
       "                       1.4250,  0.6413,  0.8123,  0.6966,  0.5348,  0.7349,  0.5434,  0.6945,\n",
       "                       0.7123,  0.7549,  0.7712,  0.6098,  0.5359,  0.6835,  1.0300,  0.5893,\n",
       "                       0.5146,  0.7298,  0.5338,  0.7432,  0.7767,  0.5624,  0.6473,  0.5336,\n",
       "                       0.6933,  0.5418,  0.8118,  0.5201,  1.0081,  0.8807,  0.5578,  0.9343,\n",
       "                       0.6725,  0.8921,  0.7632,  0.6321,  1.4709,  0.6304,  0.7667,  0.7266,\n",
       "                       0.6538,  0.9404,  0.8677,  0.7351,  1.1829,  0.5342,  0.5852,  0.6194,\n",
       "                       0.9303,  0.6795,  0.5716,  0.7312,  0.7184,  0.5336,  0.6468,  0.5397,\n",
       "                       0.4942,  1.2058,  0.5533,  0.5695,  0.8028,  0.5262,  0.5717,  1.0144,\n",
       "                       0.5886,  0.6964,  0.5920,  0.5217,  0.9338,  0.6158,  0.5708,  0.7234,\n",
       "                       0.5272,  0.7459,  0.6501,  0.7492,  0.6509,  0.6028,  0.7793,  0.7293,\n",
       "                       0.7299,  0.5009,  0.7262,  0.6241,  0.5469,  0.7411,  0.6037,  0.5740,\n",
       "                       0.5975,  0.5707,  0.4473,  0.5427,  0.8108,  0.6265,  0.8480,  0.4614,\n",
       "                       0.7635,  1.0082,  0.7085,  0.5549,  0.5708,  1.1118,  0.7074,  0.5674,\n",
       "                       0.6427,  0.5774,  0.5583,  0.5172,  0.5761,  0.4991,  1.0246,  0.4847,\n",
       "                       0.4834,  0.6340,  1.0348,  0.5575,  0.8574,  0.5717,  0.6184,  0.6363,\n",
       "                       0.6190,  0.6416,  1.0354,  0.6332,  0.5759,  0.5363,  0.5276,  0.9677,\n",
       "                       0.8324,  0.5478,  0.5856,  0.6993,  0.4658,  0.8402,  0.9930,  0.8751,\n",
       "                       0.6913,  0.8670,  0.5722,  0.6740,  0.6084,  0.5432,  0.5419,  0.6514,\n",
       "                       0.6098,  0.8131,  0.5240,  0.5112,  0.7902,  0.4419,  0.7588,  0.5523,\n",
       "                       0.5038,  0.6154,  0.6027,  1.6934,  0.5846,  0.5319,  0.5613,  0.8273,\n",
       "                       0.6368,  0.7539,  0.8588,  0.6012,  0.6190,  0.8827,  0.5318,  0.6598,\n",
       "                       0.6993,  0.4744,  0.7796,  0.3575,  0.5789,  0.5376,  0.6723,  0.4938,\n",
       "                       0.7522,  0.7730,  0.4825,  0.8953,  0.6375,  0.5808,  0.4529,  0.5767,\n",
       "                       0.3961,  0.4779,  0.5679,  0.2281,  0.5457,  0.7992,  0.6174,  0.5071,\n",
       "                       0.5100,  0.7355,  0.6323,  0.5628,  0.5971,  0.8354,  0.6470,  0.6334,\n",
       "                       0.5723,  0.6389,  0.7275,  0.6482,  0.7502,  0.8839,  0.5698,  0.5995,\n",
       "                       0.5963,  0.5852,  0.7207,  1.4410,  0.7233,  0.6445,  0.4845,  0.5528,\n",
       "                       0.6752,  0.5712,  0.7245,  0.6593,  0.6493,  0.7347,  0.5514,  0.6547,\n",
       "                       0.2747,  0.5043,  0.5384,  0.1915,  0.1588,  0.7026,  0.7861,  0.8740,\n",
       "                       0.9172,  0.7533,  0.6267,  0.5690,  0.5260,  0.5397,  0.5838,  0.8037,\n",
       "                       0.4393,  0.5849,  1.9162,  0.6216,  0.6219,  0.5773,  0.5739,  0.5018,\n",
       "                       0.5073,  0.4997,  0.7649,  0.6337,  0.7816,  0.0187,  0.5832,  0.6223,\n",
       "                       0.6299,  0.5338,  0.6106,  0.6386,  0.5945,  0.6326,  0.3381,  0.8420,\n",
       "                       0.9064,  1.2343,  0.7905,  0.6870,  0.6744,  0.8486,  0.6105,  0.5765,\n",
       "                       0.8987,  0.8213,  0.6627,  0.8307,  0.6697,  0.8707,  0.5352,  0.6183,\n",
       "                       0.6992,  1.3471,  0.5096,  0.7429,  0.5516,  0.4580,  0.5090,  0.5414,\n",
       "                       0.6790,  0.6063,  0.5163,  0.7448,  0.8149,  0.6102,  0.9037,  0.6518,\n",
       "                       0.6364,  0.6021,  0.5920,  0.4669,  0.6702,  0.7579,  0.4965,  0.8752,\n",
       "                       0.6469,  0.5273,  0.7317,  0.6485,  0.4749,  0.6824,  0.7742,  0.5970,\n",
       "                       0.4844,  0.9291,  0.6700,  0.4997,  0.8169,  0.4809,  0.7719,  0.7488,\n",
       "                       0.5710,  0.6567,  0.5688,  1.0402,  0.6978,  0.6459,  0.5234,  0.5761,\n",
       "                       0.5047,  0.4421,  0.5764,  0.9880,  0.7516,  0.6046,  0.6847,  0.6164,\n",
       "                       0.6597,  0.7354,  0.8461,  0.5744,  0.9033,  1.2793,  0.6875,  0.5268,\n",
       "                       0.6455,  0.5665,  0.7367,  0.6264,  0.6416,  0.7091,  0.4587,  0.8072,\n",
       "                       0.5824,  0.8352,  0.8691,  0.3475,  0.5139,  0.6711,  0.7508,  0.7071,\n",
       "                       0.6423,  0.8043,  0.7347,  0.8412,  0.0357,  0.4909,  0.7926,  0.4296,\n",
       "                       0.8270,  0.5401,  0.8759,  0.8436,  0.7054,  0.7282,  0.8075,  0.5775,\n",
       "                       0.6296,  0.9489,  0.5503,  0.8516,  0.3078,  0.8457,  0.6124,  1.4436,\n",
       "                       0.5970,  0.8271,  0.6834,  0.6317,  0.7015,  0.6247,  0.6475,  0.5627,\n",
       "                       0.4664,  0.5116,  0.5761,  0.5648,  0.5512,  0.5989,  0.7733,  0.5839,\n",
       "                       0.5880,  0.5221,  0.5725,  0.5143,  0.6821,  0.5682,  0.4803,  0.6867,\n",
       "                       0.4677,  0.5800,  0.0668,  0.8596,  1.0185,  0.7273,  0.4231,  0.2760,\n",
       "                       0.6516,  0.5578,  0.6137,  0.4995,  0.7200,  0.5503,  0.7227,  0.6219,\n",
       "                       0.8678,  0.5288,  0.9709,  0.5985,  0.6850,  0.5482,  0.8065,  0.7672,\n",
       "                       0.6938,  0.6698,  0.7844,  0.8595,  0.5377,  0.6372,  0.5457,  0.7463,\n",
       "                       0.7911,  0.5579,  0.6809,  0.4954,  0.6539,  0.4798,  0.6920,  0.8331,\n",
       "                       0.7619,  0.6548,  0.7696,  0.5411,  0.5922,  0.4982,  0.5417,  0.5892,\n",
       "                       0.6283,  0.5893,  0.4919,  0.5563,  0.4192,  0.5202,  0.5371,  0.4859,\n",
       "                       0.5684,  0.4936,  0.6200,  0.5396,  0.5639,  0.5934,  0.9414,  0.5848,\n",
       "                       0.8251,  1.4286,  0.6158,  0.6256,  0.8600,  0.5610,  0.5362,  0.8047,\n",
       "                       0.5165,  0.6862,  0.6235,  0.7582,  0.9435,  0.4740,  0.7188,  0.8495,\n",
       "                       0.5847,  0.5512,  0.6140,  0.5585,  0.5612,  0.6973,  0.8082,  1.0717,\n",
       "                       0.6236,  0.7577,  0.6003,  0.6979,  0.4744,  0.7983,  0.6984,  0.5076,\n",
       "                       1.4235,  0.5082,  0.7234,  0.5525,  0.6362,  0.6553,  0.9742,  0.8093,\n",
       "                       0.5305,  0.7393,  0.5020,  0.5630,  0.5551,  1.3434,  0.7248,  0.8196,\n",
       "                       0.6541,  0.6218,  0.8505,  0.6626,  0.6547,  0.6774,  0.7526,  0.7065,\n",
       "                       0.7032,  0.6742,  0.9074,  0.7546,  0.6941,  0.5565,  0.6705,  0.7978,\n",
       "                       0.5582,  0.4715,  0.6513,  0.6351,  0.6076,  0.4540,  0.8280,  0.8470,\n",
       "                       0.4584,  0.8526,  0.5702,  0.5605,  0.7137,  0.6177,  0.7318,  0.8767,\n",
       "                       0.7591,  0.5435,  0.8571,  0.9162,  0.5463,  0.7241, -0.0123,  0.7272,\n",
       "                       0.6855,  0.5883,  0.6775,  0.0789,  0.4789,  0.6332,  0.6008,  0.7059,\n",
       "                       0.6455,  0.7519,  0.5458,  0.9897,  0.5151,  0.4716,  0.5749,  0.5415,\n",
       "                       0.4745,  0.4515,  0.5915,  0.4898,  0.5917,  0.5602,  0.5085,  0.6884,\n",
       "                       0.5023,  0.6413,  0.6548,  0.6460,  0.5505,  0.6570,  1.0540,  0.5626,\n",
       "                       0.8333,  0.5753,  0.6893,  0.3684,  0.6515,  0.5361,  0.4138,  0.6714,\n",
       "                       0.8034,  0.6453,  0.6881,  0.5429,  0.7851,  0.6193,  0.6383,  0.8676,\n",
       "                       0.5613,  0.4953,  0.6269,  0.6835,  0.7419,  0.5826,  0.5718,  0.8698,\n",
       "                       0.6966,  0.6473,  0.6265,  0.9616,  0.5583,  0.9421,  0.7318,  0.6212,\n",
       "                       0.6246,  0.9337,  0.4895,  0.6014,  0.6019,  0.6103,  0.8216,  0.5343,\n",
       "                       0.7576,  0.6598,  0.5447,  0.7128,  0.8337,  0.6316,  0.8379,  0.5087,\n",
       "                       0.5258,  0.5506,  0.6427,  0.5335,  0.6159,  0.5688,  0.7619,  0.8340,\n",
       "                       0.5560,  0.6550,  0.6204,  0.8153,  0.6838,  0.0314,  0.5544,  0.8687,\n",
       "                       0.5942,  0.8379,  0.6071,  0.5720,  0.6230,  0.5867,  0.6096,  0.9975,\n",
       "                       0.5799,  0.5083,  0.5288,  0.5714,  0.5967,  0.5699,  0.6458,  0.6724,\n",
       "                       0.5868,  0.5965,  0.5955,  0.4460,  0.6120,  1.1239,  0.4461,  0.7603,\n",
       "                       0.6473,  0.6352,  0.5588,  0.6896,  0.9708,  0.5332,  0.6803,  0.6514,\n",
       "                       0.7449,  0.5176,  0.6203,  0.6722,  0.7291,  0.4457,  0.6568,  0.6463,\n",
       "                       0.6953,  0.5163,  0.6597,  1.0581,  0.8276,  0.5437,  1.2548,  0.6636,\n",
       "                       0.5537,  0.5677,  0.1853,  0.5963,  0.7774,  0.7538,  0.5876,  0.7347,\n",
       "                       0.5600,  0.6776,  0.8175,  0.6802,  0.6792,  0.6054,  0.6269,  0.5117,\n",
       "                       0.5359,  0.8483,  0.6448,  0.5724,  0.5127,  0.2560,  0.5428,  0.6432,\n",
       "                       0.6688,  0.5595,  0.6253,  0.7219,  0.7076,  0.9249,  0.6972,  0.7713,\n",
       "                       0.5202,  0.7080,  0.6065,  0.5740,  0.7648,  0.8809,  0.6296,  0.5755,\n",
       "                       0.9392,  0.6949,  0.6774,  0.6187,  0.5704,  0.5299,  0.6473,  0.6731,\n",
       "                       0.6055,  0.2834,  0.6748,  0.5947,  0.6102,  0.6539,  0.7383,  0.7813],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.6.layer.0.SelfAttention.q.weight',\n",
       "              tensor([[-0.0460, -0.0132,  0.0609,  ..., -0.0084,  0.0259,  0.0318],\n",
       "                      [ 0.0960, -0.0538,  0.0299,  ...,  0.0567,  0.0500, -0.0148],\n",
       "                      [-0.0461,  0.1710,  0.0670,  ..., -0.0261, -0.0060, -0.0559],\n",
       "                      ...,\n",
       "                      [-0.0780, -0.0532, -0.1135,  ..., -0.0692,  0.1045, -0.0852],\n",
       "                      [ 0.0497,  0.0289, -0.0174,  ...,  0.0318,  0.0725,  0.0473],\n",
       "                      [-0.0178, -0.0568, -0.0067,  ...,  0.0821,  0.0426,  0.0265]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.6.layer.0.SelfAttention.k.weight',\n",
       "              tensor([[-0.1166,  0.2911,  0.2407,  ..., -1.5616,  0.3203,  0.5577],\n",
       "                      [ 0.0707, -0.4795, -0.3589,  ..., -0.6910, -0.7591,  0.0234],\n",
       "                      [ 0.9238,  1.1813, -0.1645,  ..., -0.2208, -0.2494,  0.4959],\n",
       "                      ...,\n",
       "                      [-0.8962, -0.6397, -0.1923,  ..., -0.2337,  0.4984, -0.4542],\n",
       "                      [ 0.1905,  0.2881, -0.4147,  ...,  0.2692,  0.7149,  0.2555],\n",
       "                      [ 0.0536, -0.1666, -0.4303,  ...,  0.7489, -0.4715,  0.3956]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.6.layer.0.SelfAttention.v.weight',\n",
       "              tensor([[-0.1136, -2.1657, -0.1243,  ...,  1.0724,  1.4333,  1.8197],\n",
       "                      [ 1.3021,  0.6094,  1.3856,  ..., -0.4919,  0.6605,  0.2812],\n",
       "                      [ 1.3490,  0.5999,  1.7064,  ..., -0.3221,  0.8570,  1.3135],\n",
       "                      ...,\n",
       "                      [-0.1857, -0.4765,  0.7698,  ...,  0.4935, -0.1810,  1.1538],\n",
       "                      [-0.6991, -1.6768, -0.3563,  ..., -1.6408, -0.5401,  1.1238],\n",
       "                      [-0.6401,  2.7958, -0.7669,  ..., -0.9647, -0.1167, -0.8292]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.6.layer.0.SelfAttention.o.weight',\n",
       "              tensor([[-0.7907, -0.4158, -1.5497,  ..., -0.0176, -0.0045, -1.7521],\n",
       "                      [ 1.3434, -1.8450, -1.5011,  ...,  3.3048, -0.6207,  0.3824],\n",
       "                      [ 0.0191, -0.5870, -3.3020,  ..., -1.0014,  0.5481,  2.6274],\n",
       "                      ...,\n",
       "                      [-0.0388,  1.4353,  0.9914,  ...,  1.1396, -0.5115,  1.2198],\n",
       "                      [-0.2651, -0.8581,  0.0816,  ...,  0.4969,  1.3330, -1.2559],\n",
       "                      [-1.5475, -0.5878, -1.4710,  ..., -0.1987, -1.9844, -1.3389]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.6.layer.0.layer_norm.weight',\n",
       "              tensor([ 1.0649e-01,  1.1259e-01,  9.6419e-02,  1.0606e-01,  8.5769e-02,\n",
       "                       8.9330e-02,  1.4067e-02,  1.2822e-01, -1.7971e-02,  9.9955e-02,\n",
       "                       1.2170e-01,  9.6862e-02,  6.2682e-02,  8.4524e-02,  8.2259e-02,\n",
       "                       9.2612e-02,  8.2060e-02,  1.1940e-01,  1.1569e-01,  8.9112e-02,\n",
       "                       7.7134e-02,  1.0246e-01, -2.9409e-04,  9.0166e-02,  8.4293e-02,\n",
       "                       9.8284e-02,  9.4863e-02,  9.2618e-02,  1.0728e-01,  7.4326e-02,\n",
       "                       8.8335e-02,  9.2340e-02,  9.9023e-02,  8.4781e-02,  1.0249e-01,\n",
       "                       9.0435e-02,  6.3597e-02,  1.2823e-01,  7.2094e-02,  1.0251e-01,\n",
       "                       9.0623e-02,  9.5459e-02,  1.0666e-01,  1.0699e-01,  9.9975e-05,\n",
       "                       9.2215e-02,  6.9720e-02,  8.8961e-02,  1.1523e-01,  2.5666e-02,\n",
       "                       1.0414e-01,  9.8807e-02,  4.5237e-02,  8.6247e-02,  8.0314e-04,\n",
       "                       9.0733e-02,  4.0902e-02, -9.8001e-02,  1.0065e-01,  9.3706e-02,\n",
       "                       1.2614e-01,  7.7109e-02,  1.0517e-01,  9.2727e-02,  8.1823e-02,\n",
       "                       3.1679e-02,  9.8523e-02,  8.8155e-02,  1.0335e-01,  9.1598e-02,\n",
       "                       9.0396e-02,  9.5322e-02,  9.2075e-02,  9.1200e-02,  9.0348e-02,\n",
       "                       9.6037e-02,  8.1690e-02,  1.0290e-01,  9.5439e-02,  1.1307e-01,\n",
       "                       8.0768e-02,  1.0543e-01,  9.4907e-02,  1.1886e-01,  9.3179e-02,\n",
       "                       1.0503e-01,  1.1461e-01,  1.0235e-01,  1.0307e-01,  7.8905e-02,\n",
       "                       1.0037e-01,  6.9550e-02,  8.4316e-02,  8.6422e-02,  6.5351e-02,\n",
       "                       8.5096e-02,  9.7322e-02,  8.9684e-02,  8.9243e-02,  8.6065e-02,\n",
       "                       1.0929e-01,  8.6881e-02,  7.6598e-02,  9.1863e-02,  8.4926e-02,\n",
       "                       7.8390e-02,  8.4793e-02,  8.8888e-02,  7.8442e-02,  5.0953e-02,\n",
       "                       1.0688e-01,  1.0282e-01,  8.6716e-02,  7.2851e-02,  9.6489e-02,\n",
       "                       8.3737e-02,  9.1762e-02,  8.2654e-02,  9.1747e-02,  8.8917e-02,\n",
       "                       7.8272e-02,  8.5114e-02,  9.6321e-02,  9.0350e-02,  1.0048e-01,\n",
       "                       1.0387e-01,  1.0192e-01,  9.8526e-02,  9.5513e-02,  1.1547e-01,\n",
       "                       4.2568e-02,  1.0501e-01,  8.7666e-02,  7.1023e-02,  9.3064e-02,\n",
       "                      -8.0856e-02,  1.1650e-01,  8.6406e-02,  1.0111e-01,  1.0362e-01,\n",
       "                       8.3190e-02,  1.1860e-01,  1.0873e-01,  8.9209e-02,  1.0627e-01,\n",
       "                       1.1088e-01,  7.3621e-02,  1.1283e-01,  1.0316e-01,  1.0100e-01,\n",
       "                       9.2737e-02,  1.0937e-01,  1.0517e-01,  1.1010e-01,  8.7729e-02,\n",
       "                       8.1998e-02,  9.8904e-02,  7.0853e-02,  9.7557e-02,  7.5732e-02,\n",
       "                       8.1563e-02,  8.3406e-02,  5.6479e-02,  3.3633e-03,  7.7832e-02,\n",
       "                       6.3850e-02,  9.7004e-02,  9.3643e-02,  1.0937e-01,  1.1305e-01,\n",
       "                       1.3394e-01,  1.1456e-01,  9.4064e-02,  6.7689e-02,  9.2212e-02,\n",
       "                       8.3025e-02,  1.1587e-01,  6.2411e-02,  7.6213e-02,  1.4678e-02,\n",
       "                       7.8390e-02,  8.1142e-02,  1.1168e-01,  8.1199e-02,  1.2004e-01,\n",
       "                       1.0428e-01,  7.0738e-02,  1.1920e-01,  1.0358e-01,  1.0369e-01,\n",
       "                       7.7288e-02,  8.3878e-02,  5.4905e-02,  9.9538e-02,  7.5959e-02,\n",
       "                       3.3599e-02,  8.8932e-02,  1.0494e-01,  9.9862e-02, -7.9894e-02,\n",
       "                       8.3655e-02,  1.0909e-01,  8.9903e-02,  9.8745e-02,  9.4957e-02,\n",
       "                       1.0761e-01,  9.8516e-02,  1.0442e-01,  8.7618e-02,  1.0435e-01,\n",
       "                       9.3025e-02,  8.2409e-02,  9.6920e-02,  1.1262e-01,  9.0006e-02,\n",
       "                       1.1302e-01,  8.8203e-02,  8.8509e-02,  1.1352e-01,  4.8297e-02,\n",
       "                       6.0917e-02,  9.0810e-02,  8.0914e-02,  9.8708e-02,  7.0018e-02,\n",
       "                       9.8587e-02,  1.1958e-01,  1.2058e-01,  8.5643e-02,  1.0910e-01,\n",
       "                       6.8351e-02,  9.7314e-02,  5.7018e-02,  7.2964e-02,  8.7966e-02,\n",
       "                       4.6793e-02,  6.2517e-02,  1.0131e-01,  9.2734e-02,  9.6164e-02,\n",
       "                       8.2892e-02,  9.8642e-02,  9.7084e-02,  8.8044e-02,  7.2162e-02,\n",
       "                       8.6013e-02,  9.8745e-02,  1.0728e-01,  7.1810e-02,  9.4142e-02,\n",
       "                       5.2234e-02,  7.7185e-02,  8.1959e-02,  6.7865e-02,  7.8817e-02,\n",
       "                       7.2176e-02,  8.8221e-02,  7.4060e-02,  5.6234e-02,  9.4588e-02,\n",
       "                       1.1162e-01,  7.7733e-02,  9.0093e-02,  1.0124e-01,  7.1561e-02,\n",
       "                       8.9524e-02,  1.0087e-01,  1.0296e-01,  9.2097e-02,  8.7683e-02,\n",
       "                       3.7514e-02,  1.4076e-01,  7.6380e-02, -2.7394e-04,  1.3931e-01,\n",
       "                       1.0392e-01,  9.6478e-02,  9.6350e-02,  1.0224e-01,  8.5385e-02,\n",
       "                       1.2631e-01,  1.2758e-01,  8.7656e-02,  8.5951e-02,  9.6630e-02,\n",
       "                       6.4358e-02,  8.3097e-02,  1.0249e-01,  7.2748e-02,  4.4104e-02,\n",
       "                       6.7622e-02,  9.0174e-02,  8.1093e-02,  8.1763e-02,  1.0534e-01,\n",
       "                       7.5496e-02,  1.1226e-01,  1.0604e-01,  9.1289e-02,  9.7729e-02,\n",
       "                       1.2693e-01,  8.8929e-02,  1.3069e-01,  1.0644e-01,  1.0204e-01,\n",
       "                       8.8456e-02,  1.0467e-01,  9.2821e-02,  9.4014e-02,  9.3433e-02,\n",
       "                       1.0760e-01,  1.3211e-01,  8.0487e-02,  5.7450e-02,  1.1192e-01,\n",
       "                       1.0206e-01,  7.0593e-02,  1.2559e-01,  5.9396e-02,  9.3588e-02,\n",
       "                       7.5765e-02,  1.1174e-01,  9.9213e-02,  7.9427e-02,  1.1306e-01,\n",
       "                       7.7905e-02,  9.6973e-02,  1.2598e-01,  1.0316e-01,  1.0032e-01,\n",
       "                       9.2424e-02,  7.3048e-02,  9.7695e-02,  8.5781e-02,  9.7060e-02,\n",
       "                       8.7762e-02,  7.7190e-02,  8.0775e-02,  1.0133e-01,  1.2220e-01,\n",
       "                       1.2275e-01,  1.0376e-01,  1.0145e-01,  8.7343e-02,  9.9386e-02,\n",
       "                       1.0305e-01,  8.5108e-02,  9.4259e-02,  9.0382e-02,  4.1880e-02,\n",
       "                       9.8366e-02,  8.2493e-02,  9.8877e-02,  9.7334e-02,  9.7668e-02,\n",
       "                       9.9338e-02,  1.0441e-01,  1.1156e-01,  8.1001e-02,  1.0798e-01,\n",
       "                       7.4862e-02,  1.1316e-01,  9.4350e-02,  5.8402e-02,  8.1116e-02,\n",
       "                       8.7852e-02,  5.2819e-02,  9.0624e-02,  8.2809e-02,  9.5765e-02,\n",
       "                       8.4696e-02,  1.0418e-01,  2.3758e-02,  6.2644e-02,  1.3065e-01,\n",
       "                       7.5842e-02,  7.9665e-02,  8.2382e-02,  1.1393e-01,  6.2748e-02,\n",
       "                       9.4876e-02,  9.9865e-02,  1.2930e-01,  9.0666e-02,  9.2160e-02,\n",
       "                       2.3230e-02,  7.8115e-02,  9.9769e-02,  4.4550e-02,  1.1438e-01,\n",
       "                       7.7981e-02,  4.1109e-02,  1.0029e-01,  1.2751e-01,  1.0995e-01,\n",
       "                       8.0585e-02,  1.1465e-01,  9.3161e-02,  9.8529e-02,  8.7965e-02,\n",
       "                       8.1941e-02, -1.4581e-03,  6.7382e-02,  9.3658e-02,  8.0923e-02,\n",
       "                       8.8960e-02,  1.0470e-01,  8.8174e-02,  9.9115e-02,  7.6071e-02,\n",
       "                       8.3631e-02,  8.3088e-02,  7.6172e-02,  8.2552e-02,  9.5751e-02,\n",
       "                       9.1516e-02, -7.1783e-02,  7.9422e-02,  3.1582e-02,  1.0248e-01,\n",
       "                       8.8347e-02,  9.6239e-02,  6.4432e-02,  4.4861e-02,  1.1105e-01,\n",
       "                       9.5439e-02,  1.1042e-01,  9.9655e-02,  1.1941e-01,  1.1142e-01,\n",
       "                       1.1490e-01,  1.0374e-01,  6.2373e-02,  8.1795e-02,  9.4374e-02,\n",
       "                       9.5140e-02,  9.6869e-02,  8.2497e-02,  1.0318e-01,  1.3256e-01,\n",
       "                       9.6374e-02,  7.0332e-02,  1.0037e-01,  1.1186e-01,  8.4477e-02,\n",
       "                       1.1088e-01,  7.3465e-02,  1.0546e-01,  9.9308e-02,  1.0936e-01,\n",
       "                       8.1610e-02,  7.5863e-02,  1.0076e-01,  8.8916e-02,  9.9724e-02,\n",
       "                       1.2077e-01,  1.0860e-01,  8.9096e-02,  9.9063e-02,  8.0386e-02,\n",
       "                       7.1064e-02,  8.9045e-02,  9.0504e-02,  1.1690e-01,  1.1324e-01,\n",
       "                       9.5138e-02,  5.3795e-02,  8.0728e-02,  6.6608e-02,  8.4046e-02,\n",
       "                       7.6647e-02,  8.0620e-02, -7.2494e-02,  8.8175e-02,  1.1174e-01,\n",
       "                       9.0793e-02,  8.9966e-02,  9.1420e-02,  4.6425e-02,  1.1110e-01,\n",
       "                       1.2317e-01,  4.0267e-02,  8.8144e-02,  8.3410e-02,  1.2825e-01,\n",
       "                       8.4708e-02,  8.2362e-02,  9.5534e-02,  7.6267e-02,  1.0223e-01,\n",
       "                       1.0195e-01,  9.9599e-02,  4.1640e-02,  7.4759e-02,  9.1923e-02,\n",
       "                       9.1617e-02,  9.8869e-02,  9.6665e-02,  7.1290e-02,  6.8949e-02,\n",
       "                       8.1663e-02,  1.1318e-01,  9.6383e-02,  2.7267e-02,  9.5681e-02,\n",
       "                       1.1890e-01,  9.1088e-02,  1.1594e-01,  6.5659e-02,  1.0723e-01,\n",
       "                       1.2017e-01,  8.2201e-02,  3.3219e-02,  9.3420e-02,  1.1490e-01,\n",
       "                       9.3135e-02,  1.0058e-01,  8.1620e-02,  7.2894e-02,  1.0245e-01,\n",
       "                       7.3778e-02,  7.6342e-02,  7.5666e-02,  8.5825e-02,  1.0103e-01,\n",
       "                       3.8583e-02,  1.0905e-01,  9.8696e-02,  8.3894e-02,  1.0750e-01,\n",
       "                       1.2504e-01,  1.0196e-01,  9.7465e-02,  9.2883e-02,  9.1031e-02,\n",
       "                       1.1090e-01,  1.0471e-01,  9.6591e-02,  1.1822e-01,  9.7187e-02,\n",
       "                       1.0198e-01,  8.8859e-02,  1.0867e-01,  1.2678e-01,  8.6032e-02,\n",
       "                       9.5591e-02,  9.9985e-02,  1.0048e-01,  9.2620e-02,  6.0103e-02,\n",
       "                       1.0713e-01,  1.1368e-01,  8.3964e-02,  1.3668e-01,  9.2395e-02,\n",
       "                       9.5843e-02,  9.6837e-02,  9.2501e-02,  9.3703e-02,  9.1013e-02,\n",
       "                       1.0538e-01,  8.8978e-02,  1.0417e-01,  1.1044e-01,  1.0236e-01,\n",
       "                       9.8252e-02,  1.7133e-02,  1.0589e-01,  1.0319e-01,  9.6369e-02,\n",
       "                       1.0932e-01,  4.4392e-02,  8.7492e-02,  1.0333e-01,  9.8289e-02,\n",
       "                       9.4280e-02,  9.5045e-02,  1.2253e-01,  9.0984e-02,  8.7259e-02,\n",
       "                       9.6543e-02,  9.0923e-02,  1.0114e-01,  8.6247e-02,  7.1150e-02,\n",
       "                       8.2546e-02,  7.6056e-02,  8.8870e-02,  7.2580e-02,  9.1430e-02,\n",
       "                       8.8155e-02,  1.1615e-01,  9.3758e-02,  9.9517e-02,  9.2821e-02,\n",
       "                       1.0794e-01,  8.7911e-02,  9.6584e-02,  9.7098e-02,  8.8293e-02,\n",
       "                       1.2277e-01,  8.5814e-02,  1.0835e-01,  6.3672e-02,  1.2174e-01,\n",
       "                       8.8711e-02,  8.5852e-02,  1.2100e-01,  9.7840e-02,  9.7868e-02,\n",
       "                       1.0518e-01,  7.4982e-02,  1.0216e-01,  9.3851e-02,  9.3148e-02,\n",
       "                       1.1252e-01,  8.3472e-02,  1.1683e-01,  9.0671e-02,  9.2736e-02,\n",
       "                       1.1116e-01,  8.4067e-02,  1.0040e-01,  1.2071e-01,  1.1824e-01,\n",
       "                       9.2300e-02,  1.1368e-01,  8.4916e-02,  9.3676e-02,  7.5540e-02,\n",
       "                       1.0489e-01,  9.3445e-02,  1.1575e-01,  1.3075e-01,  8.4618e-02,\n",
       "                       8.1238e-02,  1.0269e-01,  1.0083e-01,  1.2164e-01,  1.0043e-01,\n",
       "                       8.8967e-02,  1.1275e-01,  7.7939e-02,  8.5014e-02,  1.2449e-01,\n",
       "                       1.0954e-01,  1.2116e-01,  1.0063e-01,  8.3257e-02,  8.9238e-02,\n",
       "                       8.6822e-02,  8.5284e-02,  9.3880e-02,  7.7865e-02,  9.8603e-02,\n",
       "                       1.0884e-01,  1.0144e-01,  8.4004e-02,  9.5707e-02,  1.0415e-01,\n",
       "                       9.5011e-02, -7.3192e-04,  1.0255e-01,  1.3690e-01,  9.4020e-02,\n",
       "                       9.5566e-02,  8.1060e-02,  8.1765e-02,  1.1031e-01,  9.8777e-02,\n",
       "                       1.0620e-01,  4.8400e-02,  8.7352e-02,  8.6232e-02,  1.0923e-01,\n",
       "                       9.1201e-02,  1.0295e-01,  8.0702e-02,  7.4729e-02,  9.6748e-02,\n",
       "                       9.5114e-02,  8.7061e-02,  1.0022e-01,  7.2074e-02,  9.2616e-02,\n",
       "                       4.2211e-02,  7.5699e-02,  1.0542e-01,  8.7506e-02,  1.0698e-01,\n",
       "                       7.6855e-02,  1.0887e-01,  9.2910e-02,  8.6253e-02,  8.9078e-02,\n",
       "                       8.5273e-02,  1.2824e-01,  7.8379e-02,  1.0550e-01,  9.4398e-02,\n",
       "                       1.0585e-01,  5.9766e-02,  9.6023e-02,  9.0408e-02,  9.4153e-02,\n",
       "                       8.1873e-02,  1.0376e-01,  8.3304e-02,  1.2860e-01,  8.9547e-02,\n",
       "                       4.2752e-04,  1.1489e-01,  1.0353e-01,  8.4434e-02,  3.9924e-02,\n",
       "                       1.0075e-01,  9.6242e-02,  1.1523e-01,  1.0292e-01,  1.2428e-01,\n",
       "                       9.6214e-02,  5.7899e-02,  1.2197e-01,  7.9577e-02,  1.1117e-01,\n",
       "                       9.5947e-02,  9.7985e-02,  7.6495e-02,  9.3740e-02,  9.2264e-02,\n",
       "                       1.0549e-01,  9.8499e-02,  8.8596e-02,  5.7493e-02,  7.6091e-02,\n",
       "                       1.0249e-01,  9.1234e-02,  9.7426e-02,  1.1052e-01,  1.1880e-01,\n",
       "                       1.0275e-01,  1.0367e-01,  1.2110e-01,  1.1334e-01,  8.8522e-02,\n",
       "                       9.4570e-02,  9.9428e-02,  7.9531e-02,  1.1203e-01,  9.7914e-02,\n",
       "                       8.8385e-02,  9.4062e-02,  1.0931e-01,  9.3135e-02,  9.9206e-02,\n",
       "                       9.3123e-02,  9.0913e-02,  1.0428e-01,  1.1045e-01,  1.1329e-01,\n",
       "                       1.0324e-01,  7.3559e-02,  9.4901e-02,  8.4090e-02,  1.0454e-01,\n",
       "                       1.1290e-01,  9.0427e-02,  1.2645e-01], device='cuda:0')),\n",
       "             ('encoder.block.6.layer.1.DenseReluDense.wi.weight',\n",
       "              tensor([[-0.3758, -0.5043,  0.6116,  ..., -0.5564, -0.7668, -1.6208],\n",
       "                      [ 0.4865, -0.6329,  1.9801,  ...,  1.3850,  0.2873,  1.1303],\n",
       "                      [-0.8679, -1.6065, -0.3228,  ..., -2.4432,  0.4809, -0.3037],\n",
       "                      ...,\n",
       "                      [-1.1080,  1.4675,  2.3760,  ..., -0.2632,  1.3073, -1.0595],\n",
       "                      [ 0.1085, -0.8778,  1.4106,  ..., -1.8695,  0.0709,  0.1423],\n",
       "                      [-0.2214, -0.5239, -0.2130,  ...,  0.4846, -1.2223,  0.4436]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.6.layer.1.DenseReluDense.wo.weight',\n",
       "              tensor([[ 1.6350, -0.2702,  0.6469,  ...,  0.5094,  0.5936,  0.1445],\n",
       "                      [-1.1327,  0.9240,  2.5228,  ...,  0.2164, -0.0671,  0.6233],\n",
       "                      [-0.2760,  0.3287, -0.6117,  ..., -0.9367,  0.9878, -0.0916],\n",
       "                      ...,\n",
       "                      [-0.3562, -0.1154,  0.4747,  ..., -0.6159, -0.4302, -0.3705],\n",
       "                      [-0.3245, -0.1064,  0.6549,  ...,  0.4812, -0.4580, -0.0221],\n",
       "                      [-0.7799, -1.7392,  0.5009,  ...,  1.1318,  0.8547, -0.3605]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.6.layer.1.layer_norm.weight',\n",
       "              tensor([ 7.8781e-01,  7.7410e-01,  7.1684e-01,  8.0704e-01,  5.6258e-01,\n",
       "                       7.4241e-01,  7.3951e-02,  8.7325e-01,  1.3701e+00,  6.5344e-01,\n",
       "                       8.1437e-01,  7.8602e-01,  6.1751e-01,  8.3626e-01,  6.8191e-01,\n",
       "                       6.7870e-01,  7.8299e-01,  7.2299e-01,  7.7564e-01,  6.0194e-01,\n",
       "                       6.4725e-01,  7.2152e-01,  1.2030e+00,  6.3738e-01,  6.6322e-01,\n",
       "                       6.6941e-01,  6.2353e-01,  7.0117e-01,  8.1951e-01,  6.2178e-01,\n",
       "                       6.7484e-01,  4.9844e-01,  7.6548e-01,  6.4420e-01,  8.5254e-01,\n",
       "                       6.9851e-01,  8.7395e-01,  9.1186e-01,  6.8393e-01,  1.1146e+00,\n",
       "                       7.7001e-01,  9.2472e-01,  8.0163e-01,  6.6093e-01,  1.4269e+00,\n",
       "                       7.7137e-01,  8.1096e-01,  5.9979e-01,  8.2361e-01,  6.3528e-01,\n",
       "                       9.1043e-01,  6.8366e-01,  1.0803e+00,  5.9877e-01,  6.5454e-01,\n",
       "                       7.2356e-01,  1.0586e+00,  7.4960e-01,  6.7210e-01,  6.1857e-01,\n",
       "                       8.1386e-01,  5.0063e-01,  7.3002e-01,  6.6563e-01,  6.5715e-01,\n",
       "                       1.2554e+00,  7.0940e-01,  7.1212e-01,  7.7463e-01,  6.1156e-01,\n",
       "                       7.1427e-01,  1.2228e+00,  6.0323e-01,  6.1036e-01,  6.6215e-01,\n",
       "                       5.4975e-01,  9.7333e-01,  7.6812e-01,  6.1599e-01,  7.3572e-01,\n",
       "                       6.3922e-01,  7.7926e-01,  7.0476e-01,  8.1785e-01,  6.4793e-01,\n",
       "                       7.3900e-01,  8.0501e-01,  6.9649e-01,  7.0432e-01,  5.6271e-01,\n",
       "                       8.2807e-01,  7.1962e-01,  6.1704e-01,  8.6559e-01,  6.0060e-01,\n",
       "                       6.4639e-01,  5.8138e-01,  6.2728e-01,  4.5895e-01,  6.4701e-01,\n",
       "                       8.3904e-01,  6.4991e-01,  8.6595e-01,  5.8044e-01,  7.1371e-01,\n",
       "                       1.0433e+00,  6.7841e-01,  6.6639e-01,  6.1044e-01,  1.3064e+00,\n",
       "                       7.7569e-01,  6.5075e-01,  7.1940e-01,  6.6043e-01,  6.3512e-01,\n",
       "                       6.4650e-01,  6.9518e-01,  5.2201e-01,  1.0794e+00,  4.5866e-01,\n",
       "                       5.5471e-01,  5.9593e-01,  9.6674e-01,  6.7251e-01,  7.3565e-01,\n",
       "                       6.7299e-01,  6.6871e-01,  7.0999e-01,  6.8507e-01,  7.9112e-01,\n",
       "                       1.0189e+00,  8.0409e-01,  5.6283e-01,  5.2851e-01,  6.2157e-01,\n",
       "                       9.5676e-01,  9.2055e-01,  5.3527e-01,  6.9437e-01,  7.2979e-01,\n",
       "                       6.1235e-01,  9.2447e-01,  1.0256e+00,  8.4300e-01,  6.4065e-01,\n",
       "                       9.1481e-01,  5.5579e-01,  7.7112e-01,  6.3885e-01,  6.7313e-01,\n",
       "                       5.9387e-01,  7.1156e-01,  6.7867e-01,  8.4450e-01,  6.3283e-01,\n",
       "                       5.8333e-01,  7.1413e-01,  5.9794e-01,  7.8406e-01,  6.3850e-01,\n",
       "                       6.1014e-01,  7.0965e-01,  5.2885e-01,  1.7629e+00,  5.2439e-01,\n",
       "                       6.2180e-01,  6.0813e-01,  8.3878e-01,  6.9286e-01,  8.9363e-01,\n",
       "                       8.5343e-01,  6.9352e-01,  6.5067e-01,  1.0011e+00,  5.9205e-01,\n",
       "                       6.9853e-01,  8.3299e-01,  5.1291e-01,  8.1333e-01,  4.3108e-01,\n",
       "                       6.1936e-01,  6.7140e-01,  7.4620e-01,  5.5770e-01,  7.0543e-01,\n",
       "                       8.6833e-01,  6.8324e-01,  7.4927e-01,  8.3692e-01,  8.2391e-01,\n",
       "                       5.6503e-01,  6.6861e-01,  4.6854e-01,  6.5318e-01,  6.1034e-01,\n",
       "                       2.0909e-01,  6.5689e-01,  8.6470e-01,  6.9273e-01,  4.7768e-01,\n",
       "                       4.5655e-01,  7.4434e-01,  7.2439e-01,  6.6679e-01,  7.0333e-01,\n",
       "                       8.9917e-01,  7.2025e-01,  6.0875e-01,  7.1543e-01,  6.6846e-01,\n",
       "                       7.4149e-01,  6.9297e-01,  6.9793e-01,  8.6309e-01,  6.3184e-01,\n",
       "                       7.5806e-01,  7.4183e-01,  5.9792e-01,  8.0582e-01,  1.2756e+00,\n",
       "                       6.9951e-01,  7.0206e-01,  5.7411e-01,  6.7261e-01,  7.5458e-01,\n",
       "                       6.8674e-01,  9.1860e-01,  7.1278e-01,  7.4696e-01,  8.4137e-01,\n",
       "                       5.6628e-01,  7.8511e-01,  3.5161e-01,  5.1497e-01,  5.0396e-01,\n",
       "                       3.0872e-01,  2.2492e-01,  8.0544e-01,  7.7330e-01,  8.2784e-01,\n",
       "                       9.1858e-01,  8.2102e-01,  6.5364e-01,  6.2921e-01,  6.0883e-01,\n",
       "                       5.8969e-01,  6.7720e-01,  7.5560e-01,  5.4342e-01,  5.9594e-01,\n",
       "                       1.8696e+00,  6.5123e-01,  6.6907e-01,  6.1408e-01,  6.6821e-01,\n",
       "                       4.2693e-01,  6.7446e-01,  6.1448e-01,  6.0543e-01,  5.9676e-01,\n",
       "                       8.7459e-01,  2.2259e-01,  6.4576e-01,  6.8634e-01,  6.9704e-01,\n",
       "                       6.4509e-01,  6.7042e-01,  6.9945e-01,  6.3153e-01,  5.9299e-01,\n",
       "                       5.4282e-01,  8.9949e-01,  1.0430e+00,  1.2041e+00,  9.2062e-01,\n",
       "                       6.9105e-01,  6.2891e-01,  8.8439e-01,  6.9081e-01,  7.1532e-01,\n",
       "                       9.4628e-01,  1.0567e+00,  6.7793e-01,  8.6996e-01,  6.8458e-01,\n",
       "                       8.8111e-01,  5.9075e-01,  5.7387e-01,  7.6198e-01,  1.3706e+00,\n",
       "                       5.3710e-01,  7.2312e-01,  6.8319e-01,  5.4939e-01,  5.7666e-01,\n",
       "                       6.2504e-01,  7.2481e-01,  7.0196e-01,  6.3089e-01,  8.6110e-01,\n",
       "                       8.5221e-01,  6.4582e-01,  9.9677e-01,  7.4983e-01,  7.0770e-01,\n",
       "                       6.4644e-01,  5.9349e-01,  6.5586e-01,  6.9491e-01,  6.9612e-01,\n",
       "                       7.7124e-01,  7.4727e-01,  5.4442e-01,  4.6448e-01,  7.6323e-01,\n",
       "                       6.8043e-01,  4.8342e-01,  8.1879e-01,  7.5632e-01,  6.4454e-01,\n",
       "                       5.7604e-01,  9.2733e-01,  7.0057e-01,  5.8619e-01,  8.3566e-01,\n",
       "                       5.6591e-01,  9.9631e-01,  9.6459e-01,  6.3555e-01,  6.8755e-01,\n",
       "                       5.5977e-01,  9.1304e-01,  6.9846e-01,  6.1196e-01,  6.4399e-01,\n",
       "                       6.4884e-01,  7.1635e-01,  6.3750e-01,  6.3753e-01,  9.9569e-01,\n",
       "                       9.3866e-01,  6.9321e-01,  7.1270e-01,  7.5785e-01,  7.1594e-01,\n",
       "                       8.6308e-01,  7.6528e-01,  7.6444e-01,  8.6185e-01,  1.3397e+00,\n",
       "                       7.4105e-01,  5.7697e-01,  6.9621e-01,  6.2004e-01,  7.0132e-01,\n",
       "                       8.2091e-01,  6.8556e-01,  7.2683e-01,  4.9682e-01,  9.1670e-01,\n",
       "                       6.5236e-01,  9.0673e-01,  7.8920e-01,  3.5342e-01,  6.6761e-01,\n",
       "                       6.6096e-01,  7.9182e-01,  7.9206e-01,  5.8419e-01,  8.5445e-01,\n",
       "                       7.7186e-01,  8.4575e-01, -1.2683e-02,  5.4243e-01,  8.6219e-01,\n",
       "                       6.3583e-01,  9.1656e-01,  6.2275e-01,  8.1167e-01,  7.7092e-01,\n",
       "                       6.6350e-01,  7.2202e-01,  8.0534e-01,  7.2756e-01,  6.7440e-01,\n",
       "                       1.0002e+00,  6.9941e-01,  8.3702e-01,  3.4281e-01,  7.6277e-01,\n",
       "                       6.3327e-01,  1.6245e+00,  7.5768e-01,  8.7366e-01,  8.3416e-01,\n",
       "                       8.1128e-01,  7.8155e-01,  5.8405e-01,  6.6660e-01,  6.4033e-01,\n",
       "                       5.3372e-01,  5.3727e-01,  5.0637e-01,  5.9759e-01,  5.9814e-01,\n",
       "                       7.7037e-01,  8.0265e-01,  6.0883e-01,  6.0386e-01,  6.2146e-01,\n",
       "                       6.3954e-01,  5.9256e-01,  7.0189e-01,  7.6398e-01,  6.1356e-01,\n",
       "                       6.8921e-01,  5.0158e-01,  6.4884e-01,  3.4153e-02,  9.8213e-01,\n",
       "                       8.8217e-01,  6.9638e-01,  4.8002e-01,  2.9811e-01,  6.5603e-01,\n",
       "                       6.0000e-01,  7.5475e-01,  5.1647e-01,  7.7595e-01,  6.8160e-01,\n",
       "                       7.5377e-01,  7.2157e-01,  8.9575e-01,  5.5599e-01,  9.6970e-01,\n",
       "                       5.7855e-01,  8.2755e-01,  6.3844e-01,  9.1263e-01,  8.5873e-01,\n",
       "                       7.1345e-01,  7.2159e-01,  8.6585e-01,  8.6767e-01,  6.2817e-01,\n",
       "                       6.9128e-01,  6.0217e-01,  7.0393e-01,  7.5052e-01,  7.0144e-01,\n",
       "                       6.5875e-01,  5.7337e-01,  7.2566e-01,  7.3359e-01,  8.1783e-01,\n",
       "                       9.3119e-01,  6.9739e-01,  5.7527e-01,  7.1756e-01,  5.6464e-01,\n",
       "                       5.7625e-01,  7.5057e-01,  6.4681e-01,  7.6279e-01,  7.4323e-01,\n",
       "                       7.3554e-01,  5.5231e-01,  6.5333e-01,  5.7972e-01,  6.4151e-01,\n",
       "                       4.9371e-01,  5.5553e-01,  5.0163e-01,  5.9793e-01,  7.3102e-01,\n",
       "                       4.5037e-01,  6.6477e-01,  5.5336e-01,  1.0421e+00,  8.0627e-01,\n",
       "                       8.6282e-01,  1.4115e+00,  6.2977e-01,  5.9790e-01,  8.9581e-01,\n",
       "                       5.8200e-01,  6.3664e-01,  8.0275e-01,  5.6664e-01,  7.0346e-01,\n",
       "                       6.7427e-01,  7.5463e-01,  1.0794e+00,  5.6587e-01,  6.3735e-01,\n",
       "                       7.6882e-01,  6.6787e-01,  6.7655e-01,  6.8627e-01,  6.3225e-01,\n",
       "                       5.9564e-01,  7.4147e-01,  7.8722e-01,  1.2711e+00,  6.5115e-01,\n",
       "                       6.5009e-01,  7.7525e-01,  8.2753e-01,  5.0594e-01,  8.0029e-01,\n",
       "                       7.7584e-01,  5.1159e-01,  1.2333e+00,  5.7864e-01,  7.6482e-01,\n",
       "                       6.0936e-01,  7.3987e-01,  6.0325e-01,  8.9762e-01,  6.9905e-01,\n",
       "                       5.8446e-01,  7.1446e-01,  4.8295e-01,  6.0509e-01,  6.8778e-01,\n",
       "                       1.2714e+00,  7.4127e-01,  1.0443e+00,  5.6142e-01,  8.2284e-01,\n",
       "                       9.0585e-01,  6.5505e-01,  7.0712e-01,  6.1738e-01,  8.1487e-01,\n",
       "                       7.2964e-01,  6.7273e-01,  6.0362e-01,  8.8621e-01,  7.7700e-01,\n",
       "                       7.5604e-01,  6.2278e-01,  6.9920e-01,  7.6186e-01,  6.4761e-01,\n",
       "                       6.1963e-01,  6.4186e-01,  5.8844e-01,  5.6912e-01,  3.0353e-01,\n",
       "                       9.7198e-01,  7.9773e-01, -4.8807e-01,  8.3651e-01,  5.7622e-01,\n",
       "                       6.6664e-01,  7.1741e-01,  6.1757e-01,  7.3328e-01,  8.6726e-01,\n",
       "                       8.6769e-01,  5.3921e-01,  7.8051e-01,  9.8712e-01,  7.1386e-01,\n",
       "                       8.2777e-01,  5.6062e-04,  7.3437e-01,  8.2262e-01,  6.3391e-01,\n",
       "                       7.7224e-01,  1.3613e-01,  5.7147e-01,  6.7139e-01,  7.3708e-01,\n",
       "                       7.3690e-01,  7.1464e-01,  7.8629e-01,  6.1656e-01,  1.0282e+00,\n",
       "                       6.1088e-01,  4.8234e-01,  6.9112e-01,  5.5502e-01,  4.6146e-01,\n",
       "                       5.2746e-01,  6.1236e-01,  6.0223e-01,  5.5320e-01,  5.7647e-01,\n",
       "                       6.6986e-01,  7.5150e-01,  6.1566e-01,  6.2744e-01,  7.6509e-01,\n",
       "                       7.3840e-01,  7.2437e-01,  6.5858e-01,  1.0401e+00,  6.7501e-01,\n",
       "                       7.8724e-01,  5.9703e-01,  7.5093e-01,  3.0713e-01,  6.7121e-01,\n",
       "                       6.4329e-01,  5.7581e-01,  7.8039e-01,  8.6517e-01,  7.0122e-01,\n",
       "                       7.9904e-01,  5.5060e-01,  8.3033e-01,  6.0483e-01,  7.0154e-01,\n",
       "                       9.1224e-01,  5.8993e-01,  5.0960e-01,  6.8924e-01,  6.7070e-01,\n",
       "                       6.9674e-01,  5.6715e-01,  6.5201e-01,  7.4287e-01,  6.9430e-01,\n",
       "                       6.3442e-01,  7.6189e-01,  8.0697e-01,  5.6763e-01,  9.6852e-01,\n",
       "                       7.7128e-01,  5.4492e-01,  8.0056e-01,  9.9324e-01,  5.7025e-01,\n",
       "                       5.2387e-01,  6.6478e-01,  6.7726e-01,  7.7561e-01,  5.5503e-01,\n",
       "                       7.6647e-01,  7.3529e-01,  6.5821e-01,  7.3694e-01,  8.9311e-01,\n",
       "                       6.6834e-01,  7.5887e-01,  5.9177e-01,  6.5962e-01,  6.1855e-01,\n",
       "                       6.3731e-01,  7.4491e-01,  6.4914e-01,  6.9337e-01,  6.7940e-01,\n",
       "                       8.5828e-01,  6.0908e-01,  7.6411e-01,  6.9987e-01,  8.3989e-01,\n",
       "                       8.0519e-01,  6.2482e-02,  5.5476e-01,  9.4150e-01,  6.8411e-01,\n",
       "                       8.4849e-01,  6.5043e-01,  5.8570e-01,  7.0568e-01,  6.5622e-01,\n",
       "                       5.9690e-01,  1.0140e+00,  7.3088e-01,  6.1210e-01,  6.6331e-01,\n",
       "                       6.6037e-01,  6.1980e-01,  6.2699e-01,  6.4783e-01,  5.8998e-01,\n",
       "                       6.4733e-01,  6.3459e-01,  7.3119e-01,  5.6699e-01,  7.0703e-01,\n",
       "                       1.1781e+00,  5.7183e-01,  8.0117e-01,  6.3303e-01,  8.1664e-01,\n",
       "                       5.5812e-01,  6.6847e-01,  9.5829e-01,  5.3499e-01,  5.9874e-01,\n",
       "                       6.9565e-01,  7.0423e-01,  5.7906e-01,  6.9630e-01,  8.2492e-01,\n",
       "                       7.7046e-01,  4.6905e-01,  7.1058e-01,  6.9716e-01,  8.1171e-01,\n",
       "                       5.2170e-01,  6.3856e-01,  8.6641e-01,  9.2154e-01,  5.6203e-01,\n",
       "                       1.1977e+00,  7.7600e-01,  6.5790e-01,  6.3069e-01,  2.1340e-01,\n",
       "                       7.9669e-01,  8.5007e-01,  8.1711e-01,  5.9356e-01,  7.9269e-01,\n",
       "                       6.5498e-01,  7.0928e-01,  8.8628e-01,  5.8468e-01,  8.1219e-01,\n",
       "                       6.3686e-01,  6.5082e-01,  6.3235e-01,  5.4956e-01,  7.4084e-01,\n",
       "                       7.7720e-01,  6.6772e-01,  6.2178e-01,  3.1697e-01,  6.2861e-01,\n",
       "                       6.3224e-01,  7.4090e-01,  7.9948e-01,  7.4980e-01,  7.5436e-01,\n",
       "                       7.5565e-01,  9.6803e-01,  8.7247e-01,  8.3519e-01,  6.4813e-01,\n",
       "                       7.2089e-01,  6.7685e-01,  5.5869e-01,  7.9707e-01,  7.7709e-01,\n",
       "                       6.6884e-01,  6.9704e-01,  8.3039e-01,  6.8449e-01,  7.7939e-01,\n",
       "                       6.8930e-01,  6.7965e-01,  5.6347e-01,  6.4582e-01,  7.9182e-01,\n",
       "                       6.0876e-01,  3.5072e-01,  7.0551e-01,  6.0134e-01,  7.1249e-01,\n",
       "                       7.3186e-01,  6.3499e-01,  7.8261e-01], device='cuda:0')),\n",
       "             ('encoder.block.7.layer.0.SelfAttention.q.weight',\n",
       "              tensor([[-0.0310, -0.0846, -0.0792,  ..., -0.0605, -0.0274, -0.0127],\n",
       "                      [ 0.0233,  0.0603,  0.0277,  ...,  0.0232, -0.0306,  0.0129],\n",
       "                      [-0.0628,  0.0162,  0.0072,  ..., -0.0524,  0.0030, -0.0053],\n",
       "                      ...,\n",
       "                      [-0.1137,  0.1182, -0.0332,  ...,  0.0064,  0.0873,  0.0612],\n",
       "                      [-0.1470, -0.0292, -0.0831,  ..., -0.0639,  0.0309, -0.0739],\n",
       "                      [-0.0239, -0.0081, -0.0349,  ...,  0.1976,  0.0071,  0.1013]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.7.layer.0.SelfAttention.k.weight',\n",
       "              tensor([[ 0.1041,  0.3004, -0.0262,  ..., -0.2627, -0.0637, -0.4108],\n",
       "                      [ 0.1849,  0.1873, -0.1466,  ..., -0.0997, -0.0716, -0.0508],\n",
       "                      [ 0.0875, -0.2904, -0.7917,  ..., -0.5755,  0.5007,  0.0483],\n",
       "                      ...,\n",
       "                      [-1.0354,  0.2264,  0.5534,  ..., -0.5380,  0.5013,  0.5305],\n",
       "                      [-0.4871, -0.2659,  0.1258,  ...,  0.1089,  0.6902,  0.2198],\n",
       "                      [ 0.7679, -0.4291, -0.4487,  ...,  0.7954,  0.3081,  0.9218]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.7.layer.0.SelfAttention.v.weight',\n",
       "              tensor([[ 2.6146,  0.5595,  2.4326,  ..., -0.2523, -0.9208, -0.0565],\n",
       "                      [-1.4815, -1.0598, -1.0626,  ...,  2.1327, -2.4253, -0.0601],\n",
       "                      [-0.2955,  1.7192,  0.7278,  ..., -1.2028,  2.0560, -1.5853],\n",
       "                      ...,\n",
       "                      [-1.5206,  0.5935,  0.2969,  ...,  0.7594, -0.7160, -1.0026],\n",
       "                      [-0.4619, -0.1336, -0.2876,  ...,  2.8939, -1.4813,  0.0126],\n",
       "                      [-2.6928,  0.4030,  0.5393,  ...,  2.2432,  3.2151,  0.3554]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.7.layer.0.SelfAttention.o.weight',\n",
       "              tensor([[-2.5850,  0.7620,  0.6317,  ...,  0.6953, -0.2907, -1.5562],\n",
       "                      [-2.2600,  1.7506,  0.6578,  ..., -1.6075,  2.5450,  3.5215],\n",
       "                      [ 0.1135,  0.4959, -0.6208,  ...,  0.1424,  2.6159, -0.8508],\n",
       "                      ...,\n",
       "                      [ 0.7350, -1.7604,  3.1479,  ..., -1.9312,  1.3823, -0.1223],\n",
       "                      [ 1.6533,  2.5181, -0.9890,  ...,  0.7499,  0.9193,  1.2747],\n",
       "                      [ 0.1787, -0.7795,  0.3764,  ...,  2.0282,  0.8696,  1.0807]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.7.layer.0.layer_norm.weight',\n",
       "              tensor([ 1.0094e-01,  1.1176e-01,  1.0056e-01,  1.0065e-01,  7.7356e-02,\n",
       "                       9.1248e-02,  1.2783e-02,  1.0363e-01,  2.8736e-02,  8.4902e-02,\n",
       "                       1.1568e-01,  1.0618e-01,  5.8376e-02,  8.4254e-02,  6.7583e-02,\n",
       "                       8.9998e-02,  9.0603e-02,  1.0044e-01,  1.0527e-01,  8.1724e-02,\n",
       "                       8.9607e-02,  9.8960e-02,  1.2473e-03,  7.2922e-02,  8.9213e-02,\n",
       "                       8.9621e-02,  8.3409e-02,  8.5951e-02,  9.7036e-02,  8.4898e-02,\n",
       "                       8.0439e-02,  7.3897e-02,  9.7152e-02,  7.2003e-02,  1.0524e-01,\n",
       "                       1.0665e-01,  7.0583e-02,  1.1472e-01,  7.1225e-02,  1.0593e-01,\n",
       "                       8.6305e-02,  8.7468e-02,  8.9946e-02,  9.4802e-02,  6.3200e-04,\n",
       "                       8.3514e-02,  6.5544e-02,  8.9880e-02,  1.0569e-01,  3.1564e-02,\n",
       "                       1.0334e-01,  9.3167e-02,  5.5837e-02,  8.4327e-02,  3.2274e-04,\n",
       "                       7.8866e-02,  4.8193e-02,  8.7790e-02,  9.6142e-02,  7.8360e-02,\n",
       "                       1.1148e-01,  7.6726e-02,  8.9923e-02,  8.4373e-02,  9.2801e-02,\n",
       "                       3.0987e-02,  8.5533e-02,  8.1425e-02,  1.0085e-01,  8.6980e-02,\n",
       "                       8.2519e-02,  9.7801e-02,  8.4815e-02,  8.6047e-02,  7.3490e-02,\n",
       "                       8.8154e-02,  8.5304e-02,  9.5643e-02,  8.8852e-02,  1.0362e-01,\n",
       "                       7.1225e-02,  9.7656e-02,  8.7224e-02,  9.8922e-02,  9.4906e-02,\n",
       "                       8.6038e-02,  1.1189e-01,  1.0100e-01,  1.0589e-01,  8.3102e-02,\n",
       "                       9.0518e-02,  7.2978e-02,  9.6154e-02,  8.7684e-02,  8.6044e-02,\n",
       "                       7.8339e-02,  8.9414e-02,  8.4859e-02,  8.4581e-02,  7.0488e-02,\n",
       "                       1.1096e-01,  9.7746e-02,  6.8017e-02,  7.8998e-02,  8.6606e-02,\n",
       "                       7.7166e-02,  9.0375e-02,  7.4406e-02,  7.4042e-02,  3.4659e-02,\n",
       "                       9.1703e-02,  8.1177e-02,  9.9349e-02,  7.4345e-02,  8.6605e-02,\n",
       "                       8.4715e-02,  9.4165e-02,  7.5852e-02,  7.5421e-02,  8.2354e-02,\n",
       "                       6.7871e-02,  8.8817e-02,  9.8112e-02,  8.8574e-02,  8.9717e-02,\n",
       "                       9.2724e-02,  9.2329e-02,  9.5464e-02,  9.0448e-02,  1.0486e-01,\n",
       "                       4.8240e-02,  1.1361e-01,  6.2950e-02,  7.0261e-02,  9.1008e-02,\n",
       "                       8.3786e-02,  1.1566e-01,  9.4276e-02,  9.3892e-02,  1.1312e-01,\n",
       "                       8.8196e-02,  1.2371e-01,  1.0310e-01,  9.8781e-02,  9.0391e-02,\n",
       "                       1.0689e-01,  8.6331e-02,  1.0530e-01,  7.8897e-02,  1.0181e-01,\n",
       "                       8.5276e-02,  1.0358e-01,  1.0030e-01,  1.0312e-01,  7.6209e-02,\n",
       "                       7.6143e-02,  9.0853e-02,  7.9093e-02,  8.1746e-02,  7.8853e-02,\n",
       "                       8.6707e-02,  8.4666e-02,  6.0222e-02, -5.8875e-06,  7.7539e-02,\n",
       "                       8.3976e-02,  8.8533e-02,  9.9439e-02,  9.7711e-02,  1.1313e-01,\n",
       "                       1.2226e-01,  9.2476e-02,  9.2053e-02,  5.8734e-02,  8.5206e-02,\n",
       "                       7.9227e-02,  1.1056e-01,  6.0169e-02,  8.3429e-02,  1.3917e-02,\n",
       "                       8.1182e-02,  8.3206e-02,  1.0164e-01,  7.6652e-02,  1.0048e-01,\n",
       "                       9.9054e-02,  6.7462e-02,  1.0636e-01,  9.8942e-02,  8.5100e-02,\n",
       "                       7.4013e-02,  8.6379e-02,  6.3084e-02,  9.5507e-02,  7.9130e-02,\n",
       "                       2.8579e-02,  9.0006e-02,  9.0778e-02,  9.7330e-02,  5.8924e-02,\n",
       "                       8.2468e-02,  9.0801e-02,  8.2664e-02,  1.0543e-01,  7.6483e-02,\n",
       "                       1.1715e-01,  8.2308e-02,  9.7887e-02,  7.6718e-02,  9.0001e-02,\n",
       "                       8.6422e-02,  8.5179e-02,  1.0010e-01,  1.0956e-01,  8.2617e-02,\n",
       "                       1.0816e-01,  9.2234e-02,  9.1637e-02,  9.4192e-02,  4.8206e-02,\n",
       "                       5.6255e-02,  9.3608e-02,  8.4317e-02,  1.0232e-01,  7.0504e-02,\n",
       "                       9.0116e-02,  1.1251e-01,  1.0693e-01,  9.2405e-02,  1.0456e-01,\n",
       "                       6.7945e-02,  8.6921e-02,  5.7842e-02,  6.7814e-02,  7.8973e-02,\n",
       "                       4.9336e-02,  5.8929e-02,  9.2011e-02,  1.0874e-01,  9.8607e-02,\n",
       "                       8.8347e-02,  1.0834e-01,  8.2240e-02,  9.5164e-02,  8.0727e-02,\n",
       "                       9.6613e-02,  8.9829e-02,  1.1235e-01,  6.8202e-02,  9.1250e-02,\n",
       "                       4.4339e-02,  7.3379e-02,  7.5780e-02,  7.6503e-02,  7.1508e-02,\n",
       "                       7.7421e-02,  7.9147e-02,  6.8766e-02,  5.7494e-02,  9.2960e-02,\n",
       "                       1.0939e-01,  8.7145e-02,  7.9757e-02,  8.4143e-02,  8.9852e-02,\n",
       "                       7.3187e-02,  9.1188e-02,  9.2189e-02,  7.8796e-02,  7.9308e-02,\n",
       "                       4.0878e-02,  1.2038e-01,  9.0235e-02, -7.0595e-05,  1.0605e-01,\n",
       "                       1.1276e-01,  1.0192e-01,  8.6478e-02,  1.0470e-01,  9.9865e-02,\n",
       "                       1.0994e-01,  1.1305e-01,  8.3688e-02,  8.6882e-02,  1.0410e-01,\n",
       "                       7.4878e-02,  6.9712e-02,  9.1813e-02,  7.1877e-02,  4.3503e-02,\n",
       "                       7.8124e-02,  7.7279e-02,  8.0079e-02,  6.9292e-02,  9.1018e-02,\n",
       "                       7.6312e-02,  1.0487e-01,  8.6361e-02,  8.7065e-02,  9.7383e-02,\n",
       "                       1.1415e-01,  8.9226e-02,  1.2411e-01,  1.1529e-01,  8.6052e-02,\n",
       "                       7.4253e-02,  9.6670e-02,  9.3213e-02,  9.1784e-02,  8.6825e-02,\n",
       "                       1.0755e-01,  1.1647e-01,  9.2316e-02,  6.2052e-02,  9.5428e-02,\n",
       "                       8.4103e-02,  5.9665e-02,  1.0782e-01,  6.8864e-02,  9.1113e-02,\n",
       "                       7.3166e-02,  9.7406e-02,  9.5904e-02,  7.8998e-02,  1.0364e-01,\n",
       "                       8.5708e-02,  9.3151e-02,  1.2847e-01,  9.4749e-02,  1.1541e-01,\n",
       "                       7.8771e-02,  6.2442e-02,  8.3076e-02,  8.6899e-02,  9.1436e-02,\n",
       "                       9.1959e-02,  7.1790e-02,  7.8157e-02,  9.6105e-02,  1.1580e-01,\n",
       "                       1.1107e-01,  9.8342e-02,  8.9966e-02,  8.1621e-02,  1.0003e-01,\n",
       "                       1.0741e-01,  7.9416e-02,  9.4723e-02,  8.7462e-02,  4.4542e-02,\n",
       "                       1.0293e-01,  9.2965e-02,  9.5834e-02,  8.8171e-02,  9.6573e-02,\n",
       "                       1.0521e-01,  1.1305e-01,  1.0121e-01,  7.1192e-02,  1.1250e-01,\n",
       "                       9.1004e-02,  9.3492e-02,  9.6956e-02,  5.9898e-02,  7.8523e-02,\n",
       "                       7.3382e-02,  5.1908e-02,  8.3635e-02,  8.3467e-02,  1.0752e-01,\n",
       "                       8.3652e-02,  1.0617e-01,  2.4587e-02,  7.0699e-02,  1.1669e-01,\n",
       "                       7.6266e-02,  8.4678e-02,  7.8509e-02,  1.0739e-01,  5.2097e-02,\n",
       "                       9.9866e-02,  1.0053e-01,  1.1680e-01,  9.4712e-02,  9.1976e-02,\n",
       "                       2.1164e-02,  8.8816e-02,  8.4035e-02,  5.9362e-02,  1.1091e-01,\n",
       "                       1.0070e-01,  3.1901e-02,  9.2226e-02,  1.2013e-01,  1.1416e-01,\n",
       "                       7.9864e-02,  1.0347e-01,  7.8401e-02,  9.5574e-02,  9.0080e-02,\n",
       "                       8.0272e-02, -7.0602e-04,  6.6431e-02,  1.0497e-01,  8.7083e-02,\n",
       "                       8.3912e-02,  9.8582e-02,  8.0806e-02,  9.0049e-02,  7.7035e-02,\n",
       "                       8.5374e-02,  8.0457e-02,  7.5461e-02,  8.3120e-02,  8.2262e-02,\n",
       "                       8.5394e-02,  6.8893e-02,  7.2235e-02,  2.6246e-02,  8.3240e-02,\n",
       "                       8.1040e-02,  1.0364e-01,  8.0759e-02,  4.6753e-02,  1.0390e-01,\n",
       "                       8.9864e-02,  8.2951e-02,  9.0177e-02,  1.0085e-01,  1.0302e-01,\n",
       "                       1.0032e-01,  8.6162e-02,  6.9036e-02,  7.6362e-02,  9.0316e-02,\n",
       "                       8.4365e-02,  8.4763e-02,  8.1160e-02,  9.5834e-02,  1.2113e-01,\n",
       "                       9.1081e-02,  7.5777e-02,  1.0004e-01,  1.0612e-01,  6.8492e-02,\n",
       "                       9.6006e-02,  8.1331e-02,  1.1965e-01,  1.0844e-01,  9.7246e-02,\n",
       "                       8.0625e-02,  8.3704e-02,  8.8301e-02,  8.3940e-02,  9.7666e-02,\n",
       "                       1.1103e-01,  9.5201e-02,  9.4173e-02,  9.0297e-02,  9.1695e-02,\n",
       "                       8.4337e-02,  8.1390e-02,  8.5445e-02,  1.0298e-01,  9.7185e-02,\n",
       "                       9.5363e-02,  6.4103e-02,  8.3568e-02,  6.6925e-02,  8.1889e-02,\n",
       "                       7.8179e-02,  8.5498e-02,  6.7069e-02,  7.2800e-02,  9.9040e-02,\n",
       "                       8.0132e-02,  8.3762e-02,  8.9199e-02,  4.4367e-02,  1.0238e-01,\n",
       "                       1.0408e-01,  4.3095e-02,  9.2683e-02,  6.9975e-02,  1.1860e-01,\n",
       "                       8.9829e-02,  7.3995e-02,  8.6966e-02,  7.4729e-02,  9.0627e-02,\n",
       "                       9.7402e-02,  9.0410e-02,  4.6530e-02,  8.0809e-02,  8.5211e-02,\n",
       "                       9.1779e-02,  9.3622e-02,  8.8492e-02,  8.1546e-02,  7.7717e-02,\n",
       "                       8.5454e-02,  1.0436e-01,  7.5969e-02,  2.7345e-02,  8.1118e-02,\n",
       "                       1.0292e-01,  9.8001e-02,  9.6150e-02,  7.5436e-02,  1.1074e-01,\n",
       "                       1.0441e-01,  7.7875e-02,  3.5449e-02,  8.6978e-02,  1.0275e-01,\n",
       "                       7.5906e-02,  9.0445e-02,  8.1449e-02,  8.3749e-02,  1.0482e-01,\n",
       "                       8.0801e-02,  7.6240e-02,  7.2390e-02,  7.6786e-02,  1.0391e-01,\n",
       "                       4.4335e-02,  1.0271e-01,  9.2291e-02,  8.6184e-02,  1.0190e-01,\n",
       "                       1.1171e-01,  8.3049e-02,  7.3710e-02,  9.6975e-02,  9.0706e-02,\n",
       "                       1.0112e-01,  9.3175e-02,  9.5671e-02,  1.1201e-01,  9.2305e-02,\n",
       "                       1.0293e-01,  8.4305e-02,  9.4946e-02,  1.0390e-01,  8.1551e-02,\n",
       "                       8.5702e-02,  9.8246e-02,  9.5923e-02,  9.1972e-02,  6.1787e-02,\n",
       "                       9.4762e-02,  1.0797e-01,  6.8009e-02,  1.1782e-01,  8.4217e-02,\n",
       "                       1.0343e-01,  8.2999e-02,  7.8134e-02,  8.6537e-02,  8.4474e-02,\n",
       "                       1.0451e-01,  7.4458e-02,  1.0934e-01,  8.9960e-02,  1.0093e-01,\n",
       "                       1.0080e-01,  1.7836e-02,  9.8336e-02,  1.1325e-01,  8.0492e-02,\n",
       "                       1.0656e-01,  4.0070e-02,  6.3526e-02,  8.5350e-02,  9.8521e-02,\n",
       "                       9.7129e-02,  8.7481e-02,  1.0442e-01,  7.9747e-02,  9.0810e-02,\n",
       "                       8.5592e-02,  7.5722e-02,  7.9446e-02,  9.0825e-02,  7.9561e-02,\n",
       "                       7.3369e-02,  7.2641e-02,  8.8567e-02,  7.8432e-02,  7.5277e-02,\n",
       "                       9.3233e-02,  1.1373e-01,  8.1198e-02,  1.0611e-01,  8.2156e-02,\n",
       "                       9.5739e-02,  7.9425e-02,  7.6343e-02,  1.0742e-01,  9.1448e-02,\n",
       "                       1.2177e-01,  8.3377e-02,  1.0724e-01,  6.7378e-02,  1.1253e-01,\n",
       "                       8.3970e-02,  8.0029e-02,  1.0753e-01,  8.6209e-02,  9.3670e-02,\n",
       "                       1.0428e-01,  8.2274e-02,  1.0684e-01,  8.1456e-02,  1.0663e-01,\n",
       "                       1.1076e-01,  7.6618e-02,  1.1216e-01,  1.0051e-01,  9.6992e-02,\n",
       "                       9.2666e-02,  7.2731e-02,  8.4718e-02,  1.1072e-01,  9.2560e-02,\n",
       "                       7.9889e-02,  9.1069e-02,  8.2510e-02,  8.9756e-02,  7.3947e-02,\n",
       "                       9.9126e-02,  8.9138e-02,  9.5141e-02,  1.0862e-01,  7.6861e-02,\n",
       "                       8.3854e-02,  8.8922e-02,  9.7452e-02,  1.1755e-01,  9.0247e-02,\n",
       "                       8.8318e-02,  8.2817e-02,  8.7819e-02,  8.6519e-02,  1.0862e-01,\n",
       "                       7.8234e-02,  1.0670e-01,  8.7388e-02,  6.5499e-02,  7.3849e-02,\n",
       "                       7.0381e-02,  8.9025e-02,  8.9015e-02,  7.5217e-02,  9.2801e-02,\n",
       "                       9.8398e-02,  8.7061e-02,  7.9236e-02,  1.0814e-01,  9.9658e-02,\n",
       "                       8.7080e-02, -1.0084e-03,  8.0753e-02,  1.1102e-01,  8.4024e-02,\n",
       "                       8.7164e-02,  7.3030e-02,  7.8836e-02,  9.8645e-02,  8.7781e-02,\n",
       "                       1.0180e-01,  4.7727e-02,  8.2193e-02,  7.9026e-02,  1.0366e-01,\n",
       "                       8.6523e-02,  8.1349e-02,  8.3614e-02,  8.2592e-02,  8.7503e-02,\n",
       "                       9.3903e-02,  9.5648e-02,  1.0368e-01,  7.3951e-02,  8.8070e-02,\n",
       "                       4.5508e-02,  7.6688e-02,  1.0021e-01,  9.0009e-02,  1.0492e-01,\n",
       "                       8.1413e-02,  9.0428e-02,  9.9181e-02,  7.5771e-02,  8.3071e-02,\n",
       "                       7.6178e-02,  1.0158e-01,  7.8402e-02,  8.7765e-02,  9.8629e-02,\n",
       "                       1.0675e-01,  6.2654e-02,  9.0285e-02,  8.8532e-02,  8.7115e-02,\n",
       "                       8.1207e-02,  9.3904e-02,  8.7682e-02,  1.1531e-01,  7.5998e-02,\n",
       "                      -9.7994e-04,  1.0719e-01,  9.1463e-02,  9.5149e-02,  4.0725e-02,\n",
       "                       1.1201e-01,  7.4965e-02,  1.0252e-01,  1.0691e-01,  1.0939e-01,\n",
       "                       9.5667e-02,  6.3751e-02,  1.1004e-01,  7.9943e-02,  1.0664e-01,\n",
       "                       9.2143e-02,  9.6506e-02,  6.7014e-02,  8.9305e-02,  8.7058e-02,\n",
       "                       9.9742e-02,  9.1513e-02,  7.9679e-02,  6.4779e-02,  7.9231e-02,\n",
       "                       1.0492e-01,  8.7336e-02,  9.7956e-02,  9.8860e-02,  1.0996e-01,\n",
       "                       9.3574e-02,  1.0867e-01,  1.1086e-01,  1.0029e-01,  8.8347e-02,\n",
       "                       1.0298e-01,  1.1798e-01,  7.3599e-02,  1.0062e-01,  9.5605e-02,\n",
       "                       8.1268e-02,  8.9169e-02,  1.0383e-01,  8.8200e-02,  9.2282e-02,\n",
       "                       9.3446e-02,  9.1443e-02,  9.0566e-02,  1.0541e-01,  1.0771e-01,\n",
       "                       8.8547e-02,  8.6399e-02,  9.4975e-02,  7.5749e-02,  9.9921e-02,\n",
       "                       1.1128e-01,  8.5102e-02,  1.1138e-01], device='cuda:0')),\n",
       "             ('encoder.block.7.layer.1.DenseReluDense.wi.weight',\n",
       "              tensor([[-0.3257,  0.6253, -0.3678,  ..., -0.4000,  0.8940,  0.0957],\n",
       "                      [ 0.2203,  2.6904,  0.4011,  ..., -2.1445, -0.2867, -1.9805],\n",
       "                      [-1.4920, -0.4528, -1.7199,  ...,  1.2092, -1.2925,  0.5956],\n",
       "                      ...,\n",
       "                      [ 1.4025, -2.9153, -1.6171,  ..., -0.2657, -1.5953, -1.0253],\n",
       "                      [ 1.9102,  0.5735,  1.3851,  ..., -0.9902,  0.4276,  0.4600],\n",
       "                      [ 1.0040,  0.9460, -2.5681,  ...,  0.5187,  1.5985, -0.9536]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.7.layer.1.DenseReluDense.wo.weight',\n",
       "              tensor([[-0.4140,  0.2130, -1.5361,  ...,  0.8153,  0.4024,  0.0601],\n",
       "                      [ 0.4044, -0.8372, -0.1593,  ..., -1.0712, -0.8044,  0.0615],\n",
       "                      [-0.2310, -0.0889,  1.5150,  ...,  0.0422, -0.7347,  0.0807],\n",
       "                      ...,\n",
       "                      [ 1.0533,  0.3508,  0.5444,  ..., -1.7005, -0.1071, -0.7483],\n",
       "                      [ 1.0260,  1.0428, -0.9370,  ..., -0.1620,  0.9484,  0.1999],\n",
       "                      [-0.1672,  0.7723,  0.6160,  ..., -1.2347,  0.4278,  0.0074]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.7.layer.1.layer_norm.weight',\n",
       "              tensor([0.7049, 0.6493, 0.7319, 0.7057, 0.6664, 0.7323, 0.0453, 0.7406, 1.1910,\n",
       "                      0.6263, 0.7509, 0.6778, 0.6117, 0.7488, 0.5882, 0.7432, 0.7478, 0.7530,\n",
       "                      0.6904, 0.6912, 0.7267, 0.7755, 1.1984, 0.5154, 0.5375, 0.7446, 0.5705,\n",
       "                      0.6608, 0.7518, 0.5171, 0.6381, 0.6237, 0.6471, 0.6006, 0.7877, 0.6857,\n",
       "                      0.8795, 0.8085, 0.5067, 1.1818, 0.7566, 0.8952, 0.8875, 0.6817, 1.2085,\n",
       "                      0.6354, 0.8013, 0.5879, 0.7571, 0.4377, 0.7796, 0.7074, 0.9722, 0.5362,\n",
       "                      0.6464, 0.6247, 1.0691, 0.7089, 0.6524, 0.6865, 0.7772, 0.5603, 0.6970,\n",
       "                      0.6263, 0.5681, 1.1041, 0.5329, 0.6332, 0.7861, 0.7049, 0.6079, 1.1324,\n",
       "                      0.5654, 0.6010, 0.5771, 0.5893, 0.9680, 0.6324, 0.7139, 0.8062, 0.5323,\n",
       "                      0.7308, 0.5920, 0.8720, 0.6580, 0.6493, 0.7602, 0.6570, 0.6497, 0.5978,\n",
       "                      0.6986, 0.6830, 0.6038, 0.7956, 0.6369, 0.4900, 0.6728, 0.6000, 0.5790,\n",
       "                      0.5356, 0.9256, 0.7247, 0.8428, 0.7047, 0.7859, 1.0109, 0.6908, 0.6500,\n",
       "                      0.7568, 1.2206, 0.7469, 0.6004, 0.6497, 0.6898, 0.6572, 0.6611, 0.6495,\n",
       "                      0.6592, 0.9265, 0.7208, 0.5377, 0.5643, 0.8607, 0.6100, 0.6529, 0.7860,\n",
       "                      0.6536, 0.6971, 0.6645, 0.7149, 0.8846, 0.6953, 0.6225, 0.5585, 0.6417,\n",
       "                      0.7601, 0.8748, 0.5650, 0.7492, 0.6804, 0.6028, 0.8577, 0.9338, 0.8767,\n",
       "                      0.6345, 0.9232, 0.6320, 0.7715, 0.5616, 0.6548, 0.5862, 0.6741, 0.6631,\n",
       "                      0.8594, 0.6145, 0.6047, 0.7060, 0.5867, 0.7324, 0.7076, 0.5950, 0.7186,\n",
       "                      0.5291, 1.7854, 0.5562, 0.5784, 0.6087, 0.7853, 0.6917, 0.7332, 0.7920,\n",
       "                      0.6146, 0.6738, 0.9156, 0.5911, 0.6657, 0.7308, 0.3345, 0.7175, 0.3959,\n",
       "                      0.6658, 0.6245, 0.6995, 0.5702, 0.7355, 0.8120, 0.5322, 0.7726, 0.6851,\n",
       "                      0.6721, 0.5210, 0.6720, 0.6242, 0.6283, 0.6490, 0.1894, 0.6552, 0.8809,\n",
       "                      0.7584, 0.5334, 0.5201, 0.6761, 0.6625, 0.6271, 0.6106, 0.7491, 0.6724,\n",
       "                      0.6954, 0.6634, 0.6498, 0.7431, 0.6539, 0.8069, 0.8265, 0.6717, 0.7514,\n",
       "                      0.6467, 0.6495, 0.6525, 1.2032, 0.6881, 0.7165, 0.4952, 0.5886, 0.6651,\n",
       "                      0.6990, 0.7715, 0.5437, 0.6654, 0.6650, 0.5477, 0.7659, 0.3806, 0.6075,\n",
       "                      0.5591, 0.3380, 0.3041, 0.7287, 0.8366, 0.6603, 0.8217, 0.8082, 0.6531,\n",
       "                      0.6261, 0.4718, 0.6788, 0.6782, 0.7278, 0.4605, 0.6800, 1.6987, 0.6395,\n",
       "                      0.6241, 0.6139, 0.6059, 0.5180, 0.6252, 0.4844, 0.3942, 0.6219, 0.7134,\n",
       "                      0.1078, 0.6478, 0.6404, 0.7070, 0.6076, 0.6964, 0.6981, 0.6711, 0.5581,\n",
       "                      0.4608, 0.8320, 0.9673, 1.0184, 0.8340, 0.7246, 0.6080, 0.7544, 0.6137,\n",
       "                      0.5813, 0.9469, 0.8550, 0.6245, 0.8736, 0.6858, 0.8813, 0.6113, 0.6679,\n",
       "                      0.6679, 1.2234, 0.5989, 0.7774, 0.6063, 0.5668, 0.5751, 0.6974, 0.6734,\n",
       "                      0.6942, 0.6349, 0.7200, 0.7935, 0.6249, 0.8053, 0.6837, 0.6040, 0.6398,\n",
       "                      0.6660, 0.7028, 0.7252, 0.7632, 0.6844, 0.8274, 0.6063, 0.4556, 0.6361,\n",
       "                      0.6853, 0.5026, 0.8375, 0.7128, 0.6058, 0.5836, 0.8149, 0.7033, 0.4822,\n",
       "                      0.7903, 0.5373, 0.8506, 0.9337, 0.6923, 0.6633, 0.6526, 1.0792, 0.6414,\n",
       "                      0.5429, 0.6558, 0.7018, 0.6053, 0.6602, 0.6958, 0.9127, 0.7891, 0.7117,\n",
       "                      0.6553, 0.7325, 0.7125, 0.7290, 0.7363, 0.7199, 0.8722, 1.2058, 0.7380,\n",
       "                      0.6051, 0.6697, 0.5948, 0.6431, 0.7385, 0.6455, 0.7156, 0.6127, 0.8402,\n",
       "                      0.6786, 0.8351, 0.7282, 0.5155, 0.6491, 0.6860, 0.6561, 0.7522, 0.6410,\n",
       "                      0.7844, 0.7468, 0.7520, 0.0969, 0.5318, 0.8168, 0.6472, 0.8726, 0.6957,\n",
       "                      0.6523, 0.6308, 0.6885, 0.7774, 0.8695, 0.7236, 0.6827, 0.8077, 0.6248,\n",
       "                      0.8166, 0.4103, 0.7576, 0.6650, 1.7039, 0.6581, 0.8663, 0.7123, 0.7207,\n",
       "                      0.7369, 0.5503, 0.6979, 0.6131, 0.5371, 0.5318, 0.5430, 0.6404, 0.6752,\n",
       "                      0.7195, 0.8139, 0.7073, 0.6312, 0.5714, 0.6787, 0.6984, 0.7613, 0.6858,\n",
       "                      0.5745, 0.6922, 0.5882, 0.6276, 0.0666, 0.9558, 0.8119, 0.6770, 0.5481,\n",
       "                      0.2396, 0.7022, 0.7347, 0.6935, 0.5419, 0.6311, 0.6893, 0.7962, 0.5957,\n",
       "                      0.7288, 0.5825, 0.9097, 0.5855, 0.6759, 0.6301, 0.8990, 0.7157, 0.7361,\n",
       "                      0.6936, 0.8336, 0.8035, 0.6544, 0.6094, 0.6787, 0.7617, 0.6505, 0.6388,\n",
       "                      0.6740, 0.5636, 0.6691, 0.6759, 0.8581, 0.8301, 0.7587, 0.6799, 0.8447,\n",
       "                      0.5403, 0.5345, 0.6919, 0.6488, 0.7120, 0.7872, 0.6394, 0.5833, 0.6550,\n",
       "                      0.5317, 0.6007, 0.5672, 0.5497, 0.5000, 0.5676, 0.5742, 0.5353, 0.7247,\n",
       "                      0.6653, 0.9437, 0.7067, 0.7470, 1.4913, 0.6551, 0.7067, 0.7223, 0.5795,\n",
       "                      0.5765, 0.9540, 0.6124, 0.7100, 0.7136, 0.6694, 1.0289, 0.6888, 0.7320,\n",
       "                      0.9099, 0.7478, 0.6319, 0.6363, 0.5873, 0.6149, 0.7057, 0.7325, 1.2407,\n",
       "                      0.6328, 0.6138, 0.6334, 0.7011, 0.5112, 0.8065, 0.6956, 0.5713, 1.0232,\n",
       "                      0.5795, 0.7168, 0.5683, 0.5960, 0.6114, 0.8429, 0.8059, 0.6143, 0.6833,\n",
       "                      0.5309, 0.6174, 0.7105, 1.1994, 0.8284, 0.9963, 0.6796, 0.7251, 0.7999,\n",
       "                      0.6523, 0.5809, 0.5987, 0.7745, 0.6805, 0.6413, 0.6439, 0.7906, 0.6286,\n",
       "                      0.7313, 0.5434, 0.6290, 0.6920, 0.6712, 0.6896, 0.6601, 0.6732, 0.6274,\n",
       "                      0.2444, 0.8887, 0.8125, 0.5372, 0.8836, 0.6295, 0.6971, 0.6273, 0.6275,\n",
       "                      0.8116, 0.8992, 0.8402, 0.6230, 0.8820, 0.8381, 0.6743, 0.7596, 0.1422,\n",
       "                      0.6812, 0.6926, 0.5959, 0.7292, 0.1288, 0.5273, 0.7003, 0.6241, 0.6840,\n",
       "                      0.7351, 0.7464, 0.6023, 0.8019, 0.6365, 0.4626, 0.6756, 0.6372, 0.6511,\n",
       "                      0.5230, 0.6124, 0.6127, 0.5227, 0.6069, 0.6763, 0.7699, 0.6266, 0.6141,\n",
       "                      0.7359, 0.7039, 0.6241, 0.6463, 0.9085, 0.6883, 0.8576, 0.6388, 0.6891,\n",
       "                      0.3795, 0.7173, 0.6094, 0.5301, 0.7272, 0.9225, 0.6685, 0.6825, 0.7065,\n",
       "                      0.8341, 0.6622, 0.6863, 0.8674, 0.6831, 0.5917, 0.7048, 0.7064, 0.7191,\n",
       "                      0.5740, 0.6737, 0.6548, 0.6506, 0.6297, 0.6790, 0.8480, 0.6889, 0.8689,\n",
       "                      0.7314, 0.6038, 0.6652, 0.9244, 0.5659, 0.2987, 0.5807, 0.6892, 0.7618,\n",
       "                      0.7101, 0.6474, 0.7064, 0.6321, 0.7690, 0.6979, 0.6530, 0.6884, 0.6093,\n",
       "                      0.6423, 0.5298, 0.6556, 0.6549, 0.6523, 0.7696, 0.6884, 0.7306, 0.6136,\n",
       "                      0.6479, 0.6928, 0.7817, 0.7256, 0.1564, 0.5787, 0.8196, 0.6394, 0.7980,\n",
       "                      0.5373, 0.6106, 0.7218, 0.5621, 0.6743, 0.9800, 0.6794, 0.6304, 0.7425,\n",
       "                      0.6147, 0.5787, 0.4795, 0.6225, 0.7085, 0.6911, 0.6197, 0.7643, 0.6368,\n",
       "                      0.6631, 1.0745, 0.5629, 0.7978, 0.6524, 0.7798, 0.5752, 0.6512, 0.9450,\n",
       "                      0.5393, 0.6995, 0.6844, 0.6981, 0.5753, 0.7434, 0.6353, 0.7561, 0.4813,\n",
       "                      0.6953, 0.7167, 0.7056, 0.6593, 0.6734, 0.8525, 0.8141, 0.6354, 1.0016,\n",
       "                      0.7210, 0.6176, 0.6568, 0.1765, 0.7185, 0.7563, 0.7498, 0.7391, 0.6900,\n",
       "                      0.5857, 0.7356, 0.8314, 0.5728, 0.7081, 0.6286, 0.6630, 0.5630, 0.6172,\n",
       "                      0.7556, 0.6025, 0.6605, 0.6151, 0.3399, 0.5957, 0.6392, 0.6932, 0.7282,\n",
       "                      0.6914, 0.7747, 0.7127, 0.9195, 0.7563, 0.6650, 0.6235, 0.7830, 0.6745,\n",
       "                      0.6362, 0.7181, 0.6612, 0.6764, 0.6776, 0.8204, 0.6923, 0.7893, 0.6618,\n",
       "                      0.6486, 0.6381, 0.5694, 0.7234, 0.6066, 0.3833, 0.6710, 0.6037, 0.6406,\n",
       "                      0.8011, 0.6107, 0.7707], device='cuda:0')),\n",
       "             ('encoder.block.8.layer.0.SelfAttention.q.weight',\n",
       "              tensor([[-0.1346,  0.0838,  0.0138,  ...,  0.0949, -0.0499,  0.0288],\n",
       "                      [-0.0411, -0.0334,  0.0138,  ...,  0.0129,  0.0717,  0.1140],\n",
       "                      [-0.0192,  0.0016, -0.1119,  ..., -0.0536, -0.0902,  0.1404],\n",
       "                      ...,\n",
       "                      [ 0.0940, -0.0573, -0.0626,  ...,  0.1105,  0.0113,  0.0659],\n",
       "                      [ 0.0592,  0.0451, -0.0213,  ...,  0.0253, -0.1464,  0.0973],\n",
       "                      [-0.0759, -0.0816, -0.0318,  ..., -0.0094, -0.0584, -0.0632]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.8.layer.0.SelfAttention.k.weight',\n",
       "              tensor([[-0.4703,  0.1493, -0.1459,  ..., -0.1127,  0.1780,  0.6716],\n",
       "                      [-0.2942, -0.0789, -0.3868,  ..., -0.0282,  0.3320,  0.3500],\n",
       "                      [-0.4157,  0.3369, -0.7463,  ..., -0.1096,  0.1329,  0.4853],\n",
       "                      ...,\n",
       "                      [ 0.6295, -0.1137,  0.8580,  ...,  0.1819, -0.3596,  1.6670],\n",
       "                      [ 0.2082,  0.2542, -0.1403,  ...,  0.7350, -0.4938,  0.4011],\n",
       "                      [ 0.5220, -0.0262, -0.6171,  ...,  0.5961, -0.0588, -0.4248]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.8.layer.0.SelfAttention.v.weight',\n",
       "              tensor([[ 1.0003e+00,  2.3739e-04, -1.5644e+00,  ..., -6.9264e-01,\n",
       "                        2.2104e+00, -5.2470e-01],\n",
       "                      [-1.9212e+00, -1.8175e+00, -1.9701e+00,  ..., -1.0818e+00,\n",
       "                        1.4613e+00,  4.3327e-02],\n",
       "                      [-7.3985e-01, -7.7233e-01, -9.2193e-01,  ...,  8.9892e-01,\n",
       "                        1.2392e-03, -1.1116e-01],\n",
       "                      ...,\n",
       "                      [-1.5154e+00,  2.7587e-01, -8.8984e-01,  ..., -2.1386e+00,\n",
       "                       -1.5437e+00, -1.5646e+00],\n",
       "                      [ 2.1935e+00, -1.3257e+00, -1.1804e+00,  ...,  4.5828e+00,\n",
       "                       -2.1641e+00, -3.6614e+00],\n",
       "                      [-3.7160e+00,  7.9535e-02, -8.5437e-02,  ...,  1.5809e+00,\n",
       "                       -2.5853e+00,  2.3621e+00]], device='cuda:0')),\n",
       "             ('encoder.block.8.layer.0.SelfAttention.o.weight',\n",
       "              tensor([[ 0.8087,  1.4086,  0.5093,  ...,  0.3023, -1.8617,  0.2325],\n",
       "                      [-0.6500, -0.0454, -1.7009,  ...,  0.8977,  0.2670, -1.2831],\n",
       "                      [ 2.8874, -0.5267,  0.1717,  ..., -2.2565,  2.4878,  1.0399],\n",
       "                      ...,\n",
       "                      [ 0.7755,  0.0154,  0.6600,  ..., -3.7093,  2.6305, -3.5140],\n",
       "                      [-3.1671, -0.9695,  0.6742,  ...,  1.2298, -3.3056,  2.5510],\n",
       "                      [-0.2639,  0.7114,  1.8978,  ...,  0.7457,  1.7059,  0.9178]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.8.layer.0.layer_norm.weight',\n",
       "              tensor([1.0600e-01, 9.5853e-02, 9.1993e-02, 1.0284e-01, 7.5984e-02, 9.1224e-02,\n",
       "                      1.4281e-02, 9.8507e-02, 4.2649e-02, 8.5288e-02, 9.6483e-02, 8.6319e-02,\n",
       "                      6.5845e-02, 7.4986e-02, 7.5723e-02, 8.6243e-02, 9.2418e-02, 9.2711e-02,\n",
       "                      8.8355e-02, 8.5771e-02, 7.8880e-02, 8.6447e-02, 8.4643e-03, 7.6279e-02,\n",
       "                      7.4947e-02, 9.9870e-02, 7.5657e-02, 7.8956e-02, 9.4481e-02, 7.2405e-02,\n",
       "                      7.3858e-02, 7.7199e-02, 9.0143e-02, 8.1920e-02, 1.0262e-01, 8.7888e-02,\n",
       "                      7.0843e-02, 1.0400e-01, 7.3418e-02, 8.3865e-02, 8.3188e-02, 8.8113e-02,\n",
       "                      8.7650e-02, 8.9502e-02, 6.9415e-03, 8.6575e-02, 7.5590e-02, 8.1370e-02,\n",
       "                      9.8812e-02, 3.8514e-02, 9.7743e-02, 9.1218e-02, 5.2739e-02, 8.2686e-02,\n",
       "                      1.3977e-02, 7.6671e-02, 5.6665e-02, 9.2599e-02, 8.1285e-02, 8.6016e-02,\n",
       "                      1.0205e-01, 6.3987e-02, 8.6646e-02, 8.3934e-02, 7.7912e-02, 2.5003e-02,\n",
       "                      8.9948e-02, 8.4529e-02, 1.0329e-01, 8.2058e-02, 9.1999e-02, 8.5845e-02,\n",
       "                      8.5034e-02, 8.8128e-02, 7.8217e-02, 7.9121e-02, 7.5759e-02, 9.6164e-02,\n",
       "                      7.2212e-02, 9.8345e-02, 7.5012e-02, 9.7285e-02, 8.7054e-02, 9.1009e-02,\n",
       "                      8.7025e-02, 8.8357e-02, 1.0673e-01, 9.1597e-02, 9.3440e-02, 7.7666e-02,\n",
       "                      9.0565e-02, 6.9939e-02, 8.0270e-02, 8.4792e-02, 7.8240e-02, 7.1070e-02,\n",
       "                      8.7686e-02, 8.6384e-02, 7.5937e-02, 7.7687e-02, 9.5633e-02, 8.4290e-02,\n",
       "                      6.4796e-02, 7.1528e-02, 8.4714e-02, 6.9927e-02, 7.6849e-02, 8.6547e-02,\n",
       "                      7.3199e-02, 4.3883e-02, 8.6219e-02, 8.9231e-02, 9.0425e-02, 7.2347e-02,\n",
       "                      8.8867e-02, 8.2637e-02, 8.3162e-02, 8.5273e-02, 7.9657e-02, 7.9436e-02,\n",
       "                      7.9823e-02, 9.2372e-02, 9.0781e-02, 8.0004e-02, 8.7303e-02, 9.1260e-02,\n",
       "                      1.0015e-01, 8.7503e-02, 8.2816e-02, 9.7988e-02, 5.4674e-02, 9.5011e-02,\n",
       "                      7.9850e-02, 6.4959e-02, 9.2944e-02, 7.8298e-02, 9.7064e-02, 8.9151e-02,\n",
       "                      7.8812e-02, 9.6522e-02, 7.8520e-02, 1.1445e-01, 9.7606e-02, 7.8256e-02,\n",
       "                      8.9727e-02, 1.0037e-01, 7.8007e-02, 9.7066e-02, 7.9616e-02, 9.3837e-02,\n",
       "                      8.5862e-02, 1.0048e-01, 9.5308e-02, 1.0509e-01, 7.2426e-02, 8.5604e-02,\n",
       "                      7.8857e-02, 7.0734e-02, 9.1984e-02, 8.0357e-02, 7.5228e-02, 7.8970e-02,\n",
       "                      7.0868e-02, 6.7355e-03, 8.8254e-02, 8.3977e-02, 8.3070e-02, 9.0273e-02,\n",
       "                      8.7666e-02, 8.9597e-02, 1.0821e-01, 9.4487e-02, 8.8366e-02, 6.6150e-02,\n",
       "                      9.0836e-02, 8.5199e-02, 1.0220e-01, 5.3454e-02, 8.1741e-02, 1.2809e-02,\n",
       "                      8.2467e-02, 7.7524e-02, 8.7740e-02, 7.5791e-02, 9.7961e-02, 9.9932e-02,\n",
       "                      7.6894e-02, 1.0540e-01, 8.3400e-02, 8.4220e-02, 7.7333e-02, 7.4956e-02,\n",
       "                      5.9935e-02, 9.2071e-02, 7.0989e-02, 2.7928e-02, 9.2441e-02, 9.1546e-02,\n",
       "                      8.6975e-02, 6.7713e-02, 6.4497e-02, 9.9312e-02, 8.6254e-02, 9.0608e-02,\n",
       "                      8.0421e-02, 9.1283e-02, 8.6219e-02, 8.3408e-02, 6.9833e-02, 8.3124e-02,\n",
       "                      9.1130e-02, 7.9215e-02, 9.3122e-02, 9.9288e-02, 9.3027e-02, 1.0318e-01,\n",
       "                      8.4649e-02, 8.7212e-02, 1.0728e-01, 4.8044e-02, 6.0647e-02, 9.4680e-02,\n",
       "                      8.6901e-02, 9.6677e-02, 7.0121e-02, 8.3446e-02, 9.7957e-02, 1.0041e-01,\n",
       "                      9.2013e-02, 8.9893e-02, 7.9724e-02, 8.6670e-02, 6.6354e-02, 5.8892e-02,\n",
       "                      7.5765e-02, 5.6183e-02, 5.7140e-02, 9.2140e-02, 9.3400e-02, 7.7945e-02,\n",
       "                      8.3848e-02, 1.0067e-01, 7.9319e-02, 8.9478e-02, 6.6960e-02, 8.1342e-02,\n",
       "                      9.2700e-02, 1.0333e-01, 7.7920e-02, 8.7873e-02, 5.3677e-02, 7.5365e-02,\n",
       "                      8.1292e-02, 7.7443e-02, 7.1748e-02, 7.6604e-02, 7.6338e-02, 7.4322e-02,\n",
       "                      5.3557e-02, 7.6642e-02, 8.7542e-02, 7.9277e-02, 7.7879e-02, 8.6546e-02,\n",
       "                      8.1198e-02, 7.8554e-02, 8.3102e-02, 8.9119e-02, 8.4030e-02, 7.1287e-02,\n",
       "                      4.3592e-02, 1.1098e-01, 7.9757e-02, 6.7468e-05, 1.0694e-01, 9.9946e-02,\n",
       "                      9.1419e-02, 7.6105e-02, 8.1232e-02, 9.1128e-02, 9.8104e-02, 1.0721e-01,\n",
       "                      8.5451e-02, 9.0443e-02, 9.9069e-02, 6.8479e-02, 7.3721e-02, 8.3420e-02,\n",
       "                      8.4477e-02, 5.6132e-02, 7.1129e-02, 8.8537e-02, 7.9042e-02, 7.4769e-02,\n",
       "                      9.0229e-02, 6.7642e-02, 9.3542e-02, 8.2055e-02, 8.4330e-02, 9.1773e-02,\n",
       "                      9.8793e-02, 8.9984e-02, 1.1084e-01, 1.0296e-01, 8.3176e-02, 7.1505e-02,\n",
       "                      8.8017e-02, 9.1368e-02, 8.7931e-02, 8.9139e-02, 9.2530e-02, 1.0439e-01,\n",
       "                      8.0004e-02, 6.8817e-02, 8.6262e-02, 9.0286e-02, 6.0092e-02, 9.8573e-02,\n",
       "                      6.1461e-02, 7.9485e-02, 7.3266e-02, 9.8378e-02, 8.8720e-02, 7.7021e-02,\n",
       "                      1.0099e-01, 8.3502e-02, 9.4063e-02, 1.0827e-01, 9.3990e-02, 8.5920e-02,\n",
       "                      8.1158e-02, 7.7321e-02, 8.6364e-02, 8.5195e-02, 8.4347e-02, 7.5980e-02,\n",
       "                      7.9664e-02, 7.7534e-02, 9.4787e-02, 1.0918e-01, 9.5256e-02, 9.2972e-02,\n",
       "                      8.4324e-02, 7.0865e-02, 8.8721e-02, 9.7871e-02, 8.1895e-02, 8.9929e-02,\n",
       "                      9.1066e-02, 5.4403e-02, 1.0439e-01, 8.9541e-02, 8.7056e-02, 7.9511e-02,\n",
       "                      8.2068e-02, 8.8201e-02, 9.7420e-02, 9.2558e-02, 6.7494e-02, 8.7499e-02,\n",
       "                      8.0732e-02, 1.0763e-01, 9.1623e-02, 6.5746e-02, 6.9849e-02, 8.7769e-02,\n",
       "                      5.7969e-02, 8.4037e-02, 8.2165e-02, 9.8269e-02, 7.7140e-02, 1.0091e-01,\n",
       "                      2.8404e-02, 7.7641e-02, 1.0133e-01, 8.3668e-02, 7.5986e-02, 8.6128e-02,\n",
       "                      8.8538e-02, 6.4866e-02, 9.0282e-02, 8.1787e-02, 9.7410e-02, 8.8178e-02,\n",
       "                      7.6826e-02, 3.1959e-02, 7.9305e-02, 7.8733e-02, 7.9635e-02, 1.0945e-01,\n",
       "                      7.8360e-02, 3.2362e-02, 8.5003e-02, 1.1213e-01, 1.0836e-01, 8.4712e-02,\n",
       "                      9.9442e-02, 7.8038e-02, 8.3347e-02, 8.5014e-02, 7.6466e-02, 3.3471e-04,\n",
       "                      6.9316e-02, 8.7077e-02, 7.0838e-02, 8.2607e-02, 9.8332e-02, 8.0522e-02,\n",
       "                      8.0340e-02, 7.5788e-02, 7.8379e-02, 7.6070e-02, 7.9562e-02, 8.4130e-02,\n",
       "                      8.1519e-02, 9.4332e-02, 7.2947e-02, 8.4473e-02, 2.4129e-02, 9.4195e-02,\n",
       "                      8.9081e-02, 9.4079e-02, 6.6403e-02, 4.6598e-02, 8.3881e-02, 8.5283e-02,\n",
       "                      9.0850e-02, 8.3150e-02, 1.0162e-01, 8.8635e-02, 9.7014e-02, 8.1011e-02,\n",
       "                      6.1787e-02, 7.9913e-02, 8.1712e-02, 7.7913e-02, 8.0372e-02, 7.9246e-02,\n",
       "                      9.3513e-02, 1.0814e-01, 9.1387e-02, 7.8371e-02, 1.0175e-01, 9.8519e-02,\n",
       "                      7.9114e-02, 9.0964e-02, 7.8773e-02, 9.3513e-02, 9.8102e-02, 9.6047e-02,\n",
       "                      7.8190e-02, 8.2811e-02, 8.9754e-02, 7.3654e-02, 9.1741e-02, 1.0344e-01,\n",
       "                      9.7592e-02, 8.4618e-02, 8.6436e-02, 7.4095e-02, 8.1351e-02, 7.7435e-02,\n",
       "                      8.6472e-02, 9.0487e-02, 9.8268e-02, 8.7029e-02, 5.8101e-02, 7.0804e-02,\n",
       "                      7.3867e-02, 7.4543e-02, 7.2695e-02, 7.3111e-02, 7.3848e-02, 7.4322e-02,\n",
       "                      7.8151e-02, 8.4026e-02, 7.8883e-02, 6.8059e-02, 4.4536e-02, 1.0504e-01,\n",
       "                      1.0017e-01, 5.2421e-02, 8.5274e-02, 8.0315e-02, 1.1332e-01, 8.3637e-02,\n",
       "                      8.1817e-02, 8.6034e-02, 7.3338e-02, 9.1702e-02, 8.8258e-02, 9.3062e-02,\n",
       "                      4.6475e-02, 8.3055e-02, 8.5688e-02, 9.6519e-02, 8.0080e-02, 8.8296e-02,\n",
       "                      8.4387e-02, 8.2654e-02, 8.1413e-02, 9.6204e-02, 8.8025e-02, 1.9069e-02,\n",
       "                      8.4263e-02, 9.1472e-02, 8.9196e-02, 8.7214e-02, 7.0920e-02, 9.5597e-02,\n",
       "                      9.6324e-02, 7.9409e-02, 3.8669e-02, 7.1433e-02, 9.1186e-02, 7.5614e-02,\n",
       "                      7.9554e-02, 7.3365e-02, 6.7833e-02, 9.5314e-02, 7.2487e-02, 7.7153e-02,\n",
       "                      7.4388e-02, 8.6092e-02, 9.7437e-02, 4.3652e-02, 1.0059e-01, 8.5451e-02,\n",
       "                      8.2168e-02, 1.0138e-01, 9.8052e-02, 7.8650e-02, 8.9677e-02, 9.5765e-02,\n",
       "                      9.8786e-02, 9.7945e-02, 8.4353e-02, 8.2346e-02, 1.0366e-01, 7.4995e-02,\n",
       "                      9.6095e-02, 7.9138e-02, 8.6845e-02, 1.0480e-01, 7.5927e-02, 9.0521e-02,\n",
       "                      8.8512e-02, 8.7954e-02, 8.4298e-02, 5.1591e-02, 9.9609e-02, 9.0496e-02,\n",
       "                      7.4783e-02, 1.1045e-01, 7.1997e-02, 9.1312e-02, 7.9367e-02, 8.1104e-02,\n",
       "                      9.5379e-02, 7.7440e-02, 8.8044e-02, 6.9432e-02, 9.8412e-02, 8.8440e-02,\n",
       "                      9.3479e-02, 9.1736e-02, 1.8325e-02, 8.8275e-02, 9.4870e-02, 7.3398e-02,\n",
       "                      9.3298e-02, 3.7510e-02, 7.5763e-02, 9.1626e-02, 8.9216e-02, 8.4249e-02,\n",
       "                      9.1011e-02, 1.0656e-01, 8.1300e-02, 8.3076e-02, 8.4513e-02, 7.7798e-02,\n",
       "                      7.2555e-02, 7.9991e-02, 6.7703e-02, 6.9385e-02, 7.2404e-02, 8.1436e-02,\n",
       "                      7.5921e-02, 8.5640e-02, 8.5285e-02, 9.4065e-02, 6.8741e-02, 9.7211e-02,\n",
       "                      8.6935e-02, 9.3308e-02, 8.4323e-02, 8.5409e-02, 9.3091e-02, 7.9616e-02,\n",
       "                      1.1237e-01, 7.9321e-02, 1.0520e-01, 6.3722e-02, 1.0796e-01, 7.0367e-02,\n",
       "                      7.9409e-02, 1.0192e-01, 9.3949e-02, 8.2385e-02, 8.8739e-02, 9.0757e-02,\n",
       "                      1.0233e-01, 7.1217e-02, 9.0853e-02, 1.0536e-01, 7.3972e-02, 1.0674e-01,\n",
       "                      9.8735e-02, 7.8157e-02, 9.4723e-02, 7.0986e-02, 8.3276e-02, 1.0658e-01,\n",
       "                      9.5713e-02, 9.0338e-02, 8.7365e-02, 9.0999e-02, 8.2376e-02, 6.8359e-02,\n",
       "                      8.9250e-02, 8.5840e-02, 9.1836e-02, 1.1150e-01, 7.8508e-02, 7.2645e-02,\n",
       "                      9.1783e-02, 9.1135e-02, 9.8889e-02, 8.7332e-02, 8.2151e-02, 8.4147e-02,\n",
       "                      7.9067e-02, 8.9543e-02, 1.0540e-01, 9.2247e-02, 9.5431e-02, 8.2159e-02,\n",
       "                      7.7158e-02, 7.5811e-02, 8.0314e-02, 8.6015e-02, 8.6663e-02, 7.9411e-02,\n",
       "                      9.4698e-02, 8.8749e-02, 8.7012e-02, 8.4800e-02, 8.3146e-02, 1.0343e-01,\n",
       "                      8.3697e-02, 2.2577e-03, 8.0554e-02, 1.0635e-01, 7.8164e-02, 9.5419e-02,\n",
       "                      8.5107e-02, 7.8179e-02, 9.2744e-02, 7.6712e-02, 8.8981e-02, 5.4167e-02,\n",
       "                      8.2069e-02, 7.6726e-02, 9.7865e-02, 8.8960e-02, 8.1450e-02, 8.5432e-02,\n",
       "                      8.1266e-02, 9.2739e-02, 9.0115e-02, 8.0571e-02, 9.2033e-02, 6.9627e-02,\n",
       "                      8.4706e-02, 4.9672e-02, 6.7795e-02, 9.7257e-02, 8.0563e-02, 9.5990e-02,\n",
       "                      7.0552e-02, 8.9958e-02, 8.6375e-02, 7.5419e-02, 8.5256e-02, 9.2330e-02,\n",
       "                      1.0384e-01, 7.6160e-02, 9.0550e-02, 8.9852e-02, 8.1433e-02, 6.3058e-02,\n",
       "                      9.1248e-02, 9.5514e-02, 8.3780e-02, 7.2383e-02, 8.2153e-02, 8.4737e-02,\n",
       "                      9.1429e-02, 7.5977e-02, 9.0199e-03, 9.1820e-02, 9.2923e-02, 8.2644e-02,\n",
       "                      3.3769e-02, 8.5275e-02, 8.2043e-02, 1.0227e-01, 9.0089e-02, 1.0229e-01,\n",
       "                      7.2563e-02, 6.2774e-02, 1.0921e-01, 7.6383e-02, 9.4088e-02, 9.5117e-02,\n",
       "                      9.4480e-02, 7.4258e-02, 8.6431e-02, 8.6409e-02, 9.3297e-02, 9.0385e-02,\n",
       "                      7.0658e-02, 6.7989e-02, 8.3336e-02, 9.2721e-02, 9.7003e-02, 8.9115e-02,\n",
       "                      9.0536e-02, 1.0055e-01, 9.6960e-02, 9.5649e-02, 1.0643e-01, 1.0059e-01,\n",
       "                      8.4202e-02, 8.7383e-02, 8.9326e-02, 7.9046e-02, 9.5888e-02, 9.5250e-02,\n",
       "                      7.9719e-02, 8.6857e-02, 1.0366e-01, 9.7056e-02, 9.1440e-02, 8.6399e-02,\n",
       "                      8.3253e-02, 9.0137e-02, 9.7898e-02, 1.0073e-01, 8.0397e-02, 8.2754e-02,\n",
       "                      9.2447e-02, 6.9387e-02, 9.3696e-02, 9.5279e-02, 7.6060e-02, 1.1192e-01],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.8.layer.1.DenseReluDense.wi.weight',\n",
       "              tensor([[-2.1165,  1.7400, -0.7964,  ..., -0.9200, -0.0757,  0.5935],\n",
       "                      [ 0.8557, -0.1335,  1.5656,  ...,  2.3467,  0.1845, -0.0335],\n",
       "                      [ 0.8652, -1.6264, -1.6124,  ..., -1.0982,  0.4682,  1.0303],\n",
       "                      ...,\n",
       "                      [ 0.4716,  0.7356, -0.2947,  ..., -2.8151, -0.6000,  0.6100],\n",
       "                      [-0.2458, -0.4523,  2.7191,  ...,  1.1826,  1.1228, -0.2264],\n",
       "                      [ 0.8390, -2.6314, -0.6425,  ..., -0.8700, -1.5897, -2.4474]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.8.layer.1.DenseReluDense.wo.weight',\n",
       "              tensor([[-0.2881, -0.2223, -0.0736,  ...,  0.5488,  0.0997, -0.6313],\n",
       "                      [ 0.0922, -0.1793,  0.2446,  ..., -0.5817, -0.1920, -0.8898],\n",
       "                      [ 0.5081, -1.2284, -0.2735,  ..., -0.7844, -0.3832, -0.4117],\n",
       "                      ...,\n",
       "                      [-0.0953,  0.8471,  0.4726,  ..., -0.4580, -1.8383, -0.3224],\n",
       "                      [ 0.0527, -0.5203, -0.3097,  ...,  0.1046, -0.1734, -0.5493],\n",
       "                      [ 0.1546,  1.2394, -0.4512,  ...,  0.1612,  0.3176,  0.5741]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.8.layer.1.layer_norm.weight',\n",
       "              tensor([0.7232, 0.6625, 0.6231, 0.7537, 0.6694, 0.6695, 0.0176, 0.7130, 1.0092,\n",
       "                      0.7598, 0.7905, 0.6193, 0.5238, 0.7868, 0.5452, 0.6931, 0.6907, 0.6927,\n",
       "                      0.6553, 0.7585, 0.7299, 0.6944, 1.3129, 0.6253, 0.6320, 0.7084, 0.6442,\n",
       "                      0.7254, 0.6861, 0.6205, 0.6411, 0.5763, 0.6527, 0.6547, 0.7053, 0.6126,\n",
       "                      0.7828, 0.6976, 0.5166, 1.1556, 0.6677, 0.8293, 0.6667, 0.6949, 1.0672,\n",
       "                      0.6141, 0.7987, 0.6389, 0.7546, 0.4123, 0.7703, 0.7103, 0.9133, 0.6876,\n",
       "                      0.6646, 0.6625, 1.1958, 0.6785, 0.6970, 0.6471, 0.7008, 0.5602, 0.6202,\n",
       "                      0.6578, 0.7169, 0.8930, 0.6310, 0.6903, 0.7103, 0.6407, 0.6790, 0.9858,\n",
       "                      0.6738, 0.6635, 0.6337, 0.6848, 1.0238, 0.7281, 0.6549, 0.6931, 0.5740,\n",
       "                      0.7058, 0.6142, 0.7377, 0.6732, 0.7274, 0.5907, 0.6861, 0.7434, 0.6255,\n",
       "                      0.6540, 0.6445, 0.6391, 0.7280, 0.6618, 0.5723, 0.6397, 0.6109, 0.6065,\n",
       "                      0.6170, 0.7391, 0.7109, 0.7159, 0.5713, 0.5815, 0.9316, 0.6280, 0.6129,\n",
       "                      0.6760, 1.3188, 0.6190, 0.6011, 0.6452, 0.7401, 0.6177, 0.6131, 0.7014,\n",
       "                      0.5550, 0.8033, 0.6135, 0.6256, 0.6935, 0.8656, 0.6541, 0.6245, 0.5954,\n",
       "                      0.6652, 0.7200, 0.6425, 0.6297, 0.7031, 0.7109, 0.5650, 0.5116, 0.5580,\n",
       "                      0.7292, 0.8179, 0.5529, 0.6671, 0.7011, 0.6648, 0.7723, 0.7717, 0.7479,\n",
       "                      0.6665, 0.7994, 0.4938, 0.6436, 0.6213, 0.7019, 0.6336, 0.7296, 0.6934,\n",
       "                      0.7927, 0.6531, 0.6466, 0.6257, 0.5878, 0.7098, 0.6814, 0.7373, 0.6522,\n",
       "                      0.5665, 1.8265, 0.6159, 0.6177, 0.7115, 0.7153, 0.6638, 0.6093, 0.7806,\n",
       "                      0.6206, 0.5845, 0.7601, 0.6445, 0.6967, 0.6693, 0.2652, 0.6912, 0.3800,\n",
       "                      0.6124, 0.6191, 0.7467, 0.6363, 0.5650, 0.7524, 0.5720, 0.7759, 0.6819,\n",
       "                      0.6504, 0.5820, 0.6059, 0.5503, 0.7253, 0.6124, 0.1731, 0.7158, 0.8296,\n",
       "                      0.6884, 0.6150, 0.5845, 0.7416, 0.6522, 0.6606, 0.6422, 0.7281, 0.5406,\n",
       "                      0.6951, 0.6297, 0.6084, 0.6816, 0.7239, 0.7811, 0.8185, 0.6620, 0.7589,\n",
       "                      0.6605, 0.6248, 0.6851, 0.9653, 0.5877, 0.6259, 0.5554, 0.6242, 0.7797,\n",
       "                      0.6286, 0.7025, 0.4593, 0.7531, 0.6219, 0.5794, 0.6785, 0.3968, 0.6298,\n",
       "                      0.6000, 0.4546, 0.4115, 0.7715, 0.6715, 0.5937, 0.8411, 0.7675, 0.6295,\n",
       "                      0.6118, 0.5824, 0.6487, 0.6745, 0.7183, 0.5497, 0.6472, 1.4169, 0.6271,\n",
       "                      0.5642, 0.5917, 0.6168, 0.5915, 0.7178, 0.5080, 0.2843, 0.6149, 0.6235,\n",
       "                      0.2410, 0.6439, 0.6660, 0.6661, 0.5711, 0.7699, 0.6542, 0.6606, 0.6052,\n",
       "                      0.5468, 0.7480, 0.9666, 0.8873, 0.8215, 0.7903, 0.6537, 0.6352, 0.6859,\n",
       "                      0.5832, 0.7698, 0.8544, 0.6881, 0.7957, 0.6686, 0.6206, 0.6724, 0.6697,\n",
       "                      0.6523, 1.2086, 0.6074, 0.7503, 0.5755, 0.5843, 0.6171, 0.7457, 0.8075,\n",
       "                      0.6839, 0.6364, 0.7155, 0.7700, 0.7352, 0.7425, 0.6336, 0.6900, 0.6215,\n",
       "                      0.6313, 0.6312, 0.6675, 0.7014, 0.6706, 0.8099, 0.6096, 0.5556, 0.6351,\n",
       "                      0.6126, 0.5500, 0.7368, 0.6948, 0.6774, 0.5420, 0.7678, 0.6313, 0.6652,\n",
       "                      0.8142, 0.6070, 0.8132, 0.8262, 0.6480, 0.6631, 0.5729, 0.8298, 0.6384,\n",
       "                      0.6791, 0.7187, 0.7000, 0.6196, 0.6634, 0.7900, 0.8109, 0.7185, 0.6561,\n",
       "                      0.8228, 0.7545, 0.7631, 0.7841, 0.7506, 0.7048, 0.6764, 1.2369, 0.7145,\n",
       "                      0.6468, 0.7233, 0.5709, 0.7078, 0.6245, 0.6649, 0.7294, 0.6782, 0.7710,\n",
       "                      0.6078, 0.8972, 0.6559, 0.5181, 0.6907, 0.6481, 0.5157, 0.7932, 0.6803,\n",
       "                      0.7869, 0.7111, 0.6887, 0.1357, 0.6820, 0.7409, 0.7074, 0.8039, 0.6700,\n",
       "                      0.2526, 0.6708, 0.6492, 0.7380, 0.7871, 0.6547, 0.6814, 0.7454, 0.6500,\n",
       "                      0.6359, 0.4216, 0.7933, 0.6292, 1.7033, 0.6624, 0.8139, 0.6634, 0.6865,\n",
       "                      0.7506, 0.6211, 0.6948, 0.5740, 0.5247, 0.5317, 0.5893, 0.6371, 0.6155,\n",
       "                      0.7188, 0.8272, 0.5861, 0.5440, 0.6910, 0.6202, 0.5662, 0.6967, 0.6941,\n",
       "                      0.6103, 0.5772, 0.6219, 0.6547, 0.1100, 0.8934, 0.6873, 0.7000, 0.6588,\n",
       "                      0.1702, 0.7447, 0.5729, 0.6235, 0.5881, 0.7763, 0.6482, 0.7212, 0.5626,\n",
       "                      0.5399, 0.5977, 0.7979, 0.5325, 0.6093, 0.5909, 0.6703, 0.6832, 0.7437,\n",
       "                      0.5854, 0.7280, 0.4452, 0.5151, 0.6381, 0.6186, 0.7356, 0.7548, 0.7725,\n",
       "                      0.7092, 0.6204, 0.6812, 0.5476, 0.7731, 0.7823, 0.7153, 0.7756, 0.8064,\n",
       "                      0.5910, 0.5963, 0.6280, 0.6652, 0.6909, 0.6572, 0.6398, 0.6164, 0.5821,\n",
       "                      0.5468, 0.5467, 0.5124, 0.5961, 0.6274, 0.6621, 0.6443, 0.5437, 0.6463,\n",
       "                      0.6940, 0.8910, 0.6534, 0.7739, 1.2802, 0.6317, 0.5816, 0.7771, 0.6484,\n",
       "                      0.6357, 0.9401, 0.6211, 0.6196, 0.6878, 0.7066, 1.0320, 0.6512, 0.6486,\n",
       "                      0.7497, 0.7444, 0.7218, 0.6253, 0.6219, 0.6818, 0.7183, 0.6636, 1.2880,\n",
       "                      0.7428, 0.6664, 0.6305, 0.7651, 0.5406, 0.8206, 0.6640, 0.5236, 0.9306,\n",
       "                      0.5248, 0.7315, 0.6113, 0.6663, 0.5925, 0.6524, 0.8274, 0.6853, 0.7495,\n",
       "                      0.6406, 0.6337, 0.6719, 1.1443, 0.7912, 0.7549, 0.5473, 0.6906, 0.6461,\n",
       "                      0.6646, 0.6575, 0.6600, 0.7293, 0.6750, 0.6197, 0.7350, 0.7641, 0.6873,\n",
       "                      0.6959, 0.6734, 0.7019, 0.7289, 0.7176, 0.7119, 0.6657, 0.6543, 0.5803,\n",
       "                      0.1784, 0.8103, 0.8287, 0.6702, 0.8812, 0.6029, 0.5869, 0.6718, 0.5763,\n",
       "                      0.6323, 0.7063, 0.7354, 0.6366, 0.7711, 0.8466, 0.7489, 0.6689, 0.2381,\n",
       "                      0.7494, 0.7751, 0.6121, 0.7286, 0.1723, 0.5572, 0.5901, 0.7028, 0.6915,\n",
       "                      0.6551, 0.6334, 0.6347, 0.6303, 0.7212, 0.5716, 0.6364, 0.6521, 0.5426,\n",
       "                      0.5562, 0.6646, 0.5928, 0.6445, 0.6767, 0.6781, 0.7387, 0.6386, 0.6915,\n",
       "                      0.6991, 0.5962, 0.6732, 0.7036, 0.9157, 0.6226, 0.8256, 0.5914, 0.6944,\n",
       "                      0.3461, 0.7425, 0.5809, 0.6239, 0.6805, 0.9176, 0.6642, 0.7312, 0.6358,\n",
       "                      0.8630, 0.5603, 0.6051, 0.8344, 0.7376, 0.6652, 0.6697, 0.5482, 0.6761,\n",
       "                      0.6337, 0.7111, 0.7442, 0.6254, 0.6738, 0.6424, 0.7485, 0.6501, 0.6956,\n",
       "                      0.7426, 0.6650, 0.6789, 0.6968, 0.5420, 0.1107, 0.6689, 0.6319, 0.6825,\n",
       "                      0.6436, 0.6823, 0.6082, 0.6598, 0.6785, 0.7545, 0.7195, 0.7389, 0.6804,\n",
       "                      0.6190, 0.6347, 0.6431, 0.6733, 0.6437, 0.6859, 0.6442, 0.7685, 0.6208,\n",
       "                      0.6334, 0.5565, 0.7310, 0.5778, 0.1596, 0.5781, 0.7483, 0.6418, 0.7911,\n",
       "                      0.6041, 0.7153, 0.6583, 0.6583, 0.7028, 0.8043, 0.6441, 0.6960, 0.6529,\n",
       "                      0.5959, 0.6411, 0.6581, 0.7333, 0.6272, 0.5709, 0.6747, 0.7241, 0.6470,\n",
       "                      0.7156, 1.0567, 0.4697, 0.7342, 0.6401, 0.6755, 0.6558, 0.6406, 0.8156,\n",
       "                      0.6076, 0.6706, 0.6257, 0.7276, 0.7105, 0.6004, 0.6736, 0.7393, 0.5676,\n",
       "                      0.6890, 0.7597, 0.7939, 0.5755, 0.6365, 0.8387, 0.7339, 0.6089, 0.8013,\n",
       "                      0.7106, 0.7080, 0.6725, 0.1839, 0.6518, 0.7054, 0.7706, 0.6255, 0.5741,\n",
       "                      0.7320, 0.7163, 0.8331, 0.6215, 0.7810, 0.6570, 0.5973, 0.5904, 0.6851,\n",
       "                      0.7287, 0.7246, 0.6946, 0.6056, 0.4213, 0.6003, 0.6903, 0.6907, 0.7054,\n",
       "                      0.6462, 0.6660, 0.7011, 0.8234, 0.7008, 0.6014, 0.6291, 0.6780, 0.6376,\n",
       "                      0.6882, 0.6799, 0.5922, 0.5779, 0.6716, 0.8828, 0.6228, 0.7435, 0.6458,\n",
       "                      0.7082, 0.6227, 0.5842, 0.7753, 0.6082, 0.5654, 0.6384, 0.6259, 0.6729,\n",
       "                      0.7706, 0.6588, 0.7458], device='cuda:0')),\n",
       "             ('encoder.block.9.layer.0.SelfAttention.q.weight',\n",
       "              tensor([[ 0.0807, -0.0328,  0.0526,  ..., -0.0481, -0.0879, -0.0056],\n",
       "                      [-0.0998,  0.0354,  0.0361,  ..., -0.0695, -0.0631,  0.0831],\n",
       "                      [ 0.0158,  0.0550,  0.1565,  ...,  0.0410,  0.0868,  0.0358],\n",
       "                      ...,\n",
       "                      [-0.0002,  0.0717,  0.0011,  ...,  0.0476, -0.0346,  0.0127],\n",
       "                      [ 0.1123,  0.0532,  0.0125,  ..., -0.0554,  0.1067,  0.0893],\n",
       "                      [-0.0348,  0.0317, -0.0490,  ..., -0.0154, -0.1095, -0.0088]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.9.layer.0.SelfAttention.k.weight',\n",
       "              tensor([[ 0.3232, -0.5441,  0.8867,  ..., -1.2340, -0.2159, -0.5863],\n",
       "                      [-0.6377,  0.5952, -0.6764,  ...,  0.0070,  0.4528, -0.5852],\n",
       "                      [-0.2633, -0.2540,  0.5526,  ..., -0.3529,  0.1360, -0.1993],\n",
       "                      ...,\n",
       "                      [ 0.0331, -0.1998,  0.2304,  ...,  0.5295, -0.0056, -0.0739],\n",
       "                      [ 0.2726, -0.4579,  0.0046,  ..., -1.0600, -0.0950, -0.2893],\n",
       "                      [ 0.6719, -0.6981,  0.2588,  ...,  0.2094,  0.3949,  0.5613]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.9.layer.0.SelfAttention.v.weight',\n",
       "              tensor([[ 2.3286, -1.0593, -0.3429,  ..., -0.1872, -2.1395, -2.0085],\n",
       "                      [-0.0471, -0.7942, -0.0055,  ..., -1.1165, -1.2568,  0.7389],\n",
       "                      [-2.3125, -0.8776, -0.6606,  ..., -0.7313,  1.7367, -0.0964],\n",
       "                      ...,\n",
       "                      [-0.4001, -1.1006,  1.6375,  ..., -1.6227, -1.4708, -0.6651],\n",
       "                      [ 1.5185, -0.9215, -0.3793,  ..., -0.2676,  0.1645,  0.3692],\n",
       "                      [-1.0579, -0.1996,  0.3284,  ...,  4.7228, -3.2843, -0.5622]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.9.layer.0.SelfAttention.o.weight',\n",
       "              tensor([[-1.7506,  1.6832,  0.5928,  ...,  5.0169, -2.6159,  0.3831],\n",
       "                      [ 1.6021,  0.0652, -0.0543,  ..., -1.4236,  1.1289, -1.0811],\n",
       "                      [-0.5199, -0.4778,  1.0951,  ..., -1.6273, -1.1796, -0.8628],\n",
       "                      ...,\n",
       "                      [ 1.1817,  2.1748,  0.9454,  ..., -1.2293, -1.6640, -1.5040],\n",
       "                      [ 1.7127, -0.4280, -0.9362,  ...,  2.0597,  2.6801,  2.0356],\n",
       "                      [ 3.0552, -0.1654, -0.3045,  ...,  0.8172, -4.1663,  3.0998]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.9.layer.0.layer_norm.weight',\n",
       "              tensor([ 0.0953,  0.0921,  0.0909,  0.1159,  0.0920,  0.0876,  0.0100,  0.0977,\n",
       "                       0.0456,  0.0946,  0.1126,  0.1018,  0.0583,  0.0751,  0.0796,  0.1013,\n",
       "                       0.0922,  0.0976,  0.0837,  0.0866,  0.0874,  0.1018,  0.0053,  0.0806,\n",
       "                       0.0859,  0.0879,  0.0845,  0.0871,  0.0922,  0.0817,  0.0805,  0.0910,\n",
       "                       0.0924,  0.0778,  0.0929,  0.0955,  0.0774,  0.1073,  0.0848,  0.0762,\n",
       "                       0.0796,  0.0870,  0.0958,  0.0994,  0.0145,  0.0877,  0.0755,  0.0767,\n",
       "                       0.0857,  0.0374,  0.1044,  0.0913,  0.0692,  0.0955,  0.0007,  0.0773,\n",
       "                       0.0797,  0.0928,  0.1003,  0.1048,  0.1032,  0.0867,  0.0854,  0.0950,\n",
       "                       0.0931,  0.0262,  0.0922,  0.0834,  0.1044,  0.0914,  0.0839,  0.0862,\n",
       "                       0.0800,  0.0964,  0.0694,  0.0948,  0.0790,  0.0866,  0.0745,  0.1066,\n",
       "                       0.0744,  0.1016,  0.0997,  0.1019,  0.0970,  0.0842,  0.1046,  0.0969,\n",
       "                       0.1023,  0.0839,  0.0953,  0.0913,  0.0860,  0.0870,  0.0821,  0.0774,\n",
       "                       0.0962,  0.0857,  0.0799,  0.0781,  0.0964,  0.0985,  0.0737,  0.0780,\n",
       "                       0.0881,  0.0782,  0.0851,  0.0828,  0.0889,  0.0441,  0.0821,  0.0987,\n",
       "                       0.0763,  0.0772,  0.0934,  0.0838,  0.0922,  0.0922,  0.0790,  0.0805,\n",
       "                       0.0906,  0.0965,  0.0984,  0.0823,  0.0892,  0.0939,  0.1096,  0.0821,\n",
       "                       0.0851,  0.1047,  0.0606,  0.1043,  0.0891,  0.0786,  0.0886,  0.0757,\n",
       "                       0.0968,  0.0838,  0.0914,  0.0977,  0.0852,  0.1063,  0.0935,  0.0898,\n",
       "                       0.0835,  0.1001,  0.0948,  0.0975,  0.0811,  0.0971,  0.0923,  0.0955,\n",
       "                       0.0934,  0.1072,  0.0895,  0.0930,  0.0916,  0.0930,  0.0968,  0.0889,\n",
       "                       0.0819,  0.0907,  0.0632, -0.0182,  0.0912,  0.0890,  0.0929,  0.0873,\n",
       "                       0.0840,  0.0840,  0.1100,  0.1044,  0.0968,  0.0698,  0.0959,  0.0812,\n",
       "                       0.1042,  0.0536,  0.0805,  0.0150,  0.0905,  0.0883,  0.0869,  0.0903,\n",
       "                       0.0874,  0.1011,  0.0783,  0.0994,  0.0947,  0.0924,  0.0778,  0.0853,\n",
       "                       0.0678,  0.0958,  0.0901,  0.0310,  0.0972,  0.1048,  0.0969,  0.0772,\n",
       "                       0.0704,  0.0995,  0.0866,  0.0938,  0.0804,  0.0894,  0.0903,  0.0896,\n",
       "                       0.0869,  0.0890,  0.0805,  0.0921,  0.0999,  0.0884,  0.0819,  0.0973,\n",
       "                       0.0847,  0.0882,  0.0956,  0.0506,  0.0612,  0.0958,  0.0798,  0.1013,\n",
       "                       0.0694,  0.0852,  0.1074,  0.0912,  0.0890,  0.0980,  0.0743,  0.0876,\n",
       "                       0.0674,  0.0834,  0.0923,  0.0633,  0.0739,  0.0872,  0.0956,  0.0872,\n",
       "                       0.0846,  0.0917,  0.0897,  0.0914,  0.0789,  0.0876,  0.0950,  0.0951,\n",
       "                       0.0745,  0.0979,  0.0617,  0.0843,  0.0954,  0.1007,  0.0794,  0.0872,\n",
       "                       0.0913,  0.0844,  0.0568,  0.0966,  0.1008,  0.0800,  0.0918,  0.1016,\n",
       "                       0.0891,  0.0900,  0.0954,  0.0844,  0.0891,  0.0841,  0.0604,  0.1115,\n",
       "                       0.0807,  0.0011,  0.1006,  0.1014,  0.0952,  0.0810,  0.0923,  0.0914,\n",
       "                       0.1025,  0.0923,  0.0913,  0.0924,  0.1143,  0.0810,  0.0774,  0.0851,\n",
       "                       0.0976,  0.0605,  0.0876,  0.1022,  0.0854,  0.0824,  0.0880,  0.0780,\n",
       "                       0.1020,  0.0943,  0.0887,  0.0939,  0.0940,  0.0931,  0.1094,  0.0964,\n",
       "                       0.0967,  0.0857,  0.1020,  0.0966,  0.0879,  0.0949,  0.1024,  0.1057,\n",
       "                       0.0906,  0.0687,  0.0945,  0.0920,  0.0640,  0.1048,  0.0899,  0.0881,\n",
       "                       0.0913,  0.1024,  0.0967,  0.0976,  0.0877,  0.0861,  0.1012,  0.1076,\n",
       "                       0.1083,  0.0848,  0.0922,  0.0736,  0.0870,  0.1023,  0.1003,  0.1004,\n",
       "                       0.0797,  0.0822,  0.0866,  0.1136,  0.1120,  0.1074,  0.0911,  0.0927,\n",
       "                       0.0971,  0.0956,  0.0939,  0.0879,  0.0812,  0.0589,  0.1007,  0.0871,\n",
       "                       0.0896,  0.0847,  0.0867,  0.0971,  0.0992,  0.0938,  0.0908,  0.0869,\n",
       "                       0.0935,  0.0988,  0.0933,  0.0741,  0.0899,  0.0830,  0.0513,  0.0864,\n",
       "                       0.0812,  0.0881,  0.0870,  0.1087,  0.0359,  0.0770,  0.1114,  0.0921,\n",
       "                       0.0759,  0.0889,  0.0743,  0.0637,  0.0979,  0.0898,  0.1027,  0.0911,\n",
       "                       0.0925,  0.0573,  0.0906,  0.0797,  0.0721,  0.0983,  0.1017,  0.0298,\n",
       "                       0.0926,  0.0983,  0.0967,  0.0933,  0.0973,  0.0944,  0.0886,  0.0960,\n",
       "                       0.0836,  0.0259,  0.0880,  0.0998,  0.0778,  0.0830,  0.0949,  0.0811,\n",
       "                       0.0870,  0.0740,  0.0832,  0.0799,  0.0887,  0.0935,  0.0796,  0.0909,\n",
       "                       0.0742,  0.0823,  0.0356,  0.0968,  0.0872,  0.0933,  0.0830,  0.0504,\n",
       "                       0.0980,  0.0881,  0.1074,  0.0942,  0.0903,  0.0967,  0.1036,  0.0952,\n",
       "                       0.0577,  0.0821,  0.0826,  0.0898,  0.0972,  0.0933,  0.0848,  0.1007,\n",
       "                       0.0878,  0.0823,  0.0912,  0.0936,  0.0942,  0.0930,  0.0826,  0.1032,\n",
       "                       0.0846,  0.0970,  0.0852,  0.0869,  0.0946,  0.0846,  0.0997,  0.0908,\n",
       "                       0.0985,  0.0882,  0.1028,  0.0858,  0.0806,  0.0807,  0.1001,  0.0938,\n",
       "                       0.1045,  0.0944,  0.0795,  0.0808,  0.0726,  0.0889,  0.0949,  0.0924,\n",
       "                       0.0757,  0.0860,  0.0904,  0.0904,  0.0899,  0.0812,  0.0420,  0.0928,\n",
       "                       0.1054,  0.0460,  0.0881,  0.0859,  0.1052,  0.0912,  0.0927,  0.0814,\n",
       "                       0.0844,  0.0956,  0.0947,  0.0998,  0.0606,  0.0808,  0.0928,  0.0956,\n",
       "                       0.0933,  0.1001,  0.0916,  0.0832,  0.0873,  0.0881,  0.0902,  0.0172,\n",
       "                       0.0895,  0.0955,  0.1012,  0.0964,  0.0883,  0.1014,  0.1079,  0.0901,\n",
       "                       0.0496,  0.0931,  0.0909,  0.0816,  0.0886,  0.0827,  0.0708,  0.0990,\n",
       "                       0.0838,  0.0828,  0.0698,  0.0818,  0.0995,  0.0564,  0.1002,  0.0748,\n",
       "                       0.0825,  0.1082,  0.1000,  0.0919,  0.0967,  0.0953,  0.1003,  0.0922,\n",
       "                       0.0976,  0.0903,  0.1125,  0.1005,  0.0997,  0.0878,  0.0978,  0.0982,\n",
       "                       0.0854,  0.0933,  0.0886,  0.1002,  0.0853,  0.0428,  0.0985,  0.0918,\n",
       "                       0.0799,  0.1085,  0.0850,  0.0922,  0.0890,  0.0852,  0.0913,  0.0838,\n",
       "                       0.0872,  0.0770,  0.1000,  0.0898,  0.0938,  0.0897,  0.0208,  0.0923,\n",
       "                       0.0918,  0.0901,  0.0791,  0.0522,  0.0815,  0.0858,  0.0943,  0.0938,\n",
       "                       0.0935,  0.1078,  0.0812,  0.0845,  0.0996,  0.0922,  0.0900,  0.0883,\n",
       "                       0.0875,  0.0791,  0.0784,  0.0819,  0.0835,  0.0941,  0.0889,  0.0966,\n",
       "                       0.0920,  0.1026,  0.0880,  0.0939,  0.0928,  0.0809,  0.1035,  0.0877,\n",
       "                       0.1070,  0.0937,  0.0986,  0.0762,  0.0968,  0.0944,  0.0841,  0.1001,\n",
       "                       0.0924,  0.0994,  0.1030,  0.0808,  0.0980,  0.0813,  0.0978,  0.1091,\n",
       "                       0.0816,  0.1188,  0.0874,  0.1001,  0.0996,  0.0814,  0.0952,  0.1058,\n",
       "                       0.1017,  0.0900,  0.0942,  0.0816,  0.0892,  0.0747,  0.0986,  0.0859,\n",
       "                       0.0971,  0.1091,  0.0962,  0.0613,  0.0831,  0.0920,  0.1099,  0.1048,\n",
       "                       0.0847,  0.1013,  0.0789,  0.0835,  0.1101,  0.0979,  0.0985,  0.0871,\n",
       "                       0.0841,  0.0816,  0.0812,  0.0962,  0.0946,  0.0911,  0.0871,  0.0933,\n",
       "                       0.1013,  0.0809,  0.0892,  0.0905,  0.0821,  0.0085,  0.0792,  0.1073,\n",
       "                       0.0949,  0.0821,  0.0943,  0.0975,  0.1008,  0.0903,  0.0927,  0.0635,\n",
       "                       0.0777,  0.0896,  0.1001,  0.0941,  0.0819,  0.0870,  0.0786,  0.0900,\n",
       "                       0.0986,  0.0872,  0.1016,  0.0790,  0.0873,  0.0615,  0.0789,  0.1028,\n",
       "                       0.0998,  0.1054,  0.0854,  0.0926,  0.0831,  0.0839,  0.0918,  0.0841,\n",
       "                       0.1085,  0.0708,  0.0949,  0.0874,  0.0881,  0.0731,  0.0849,  0.0941,\n",
       "                       0.1007,  0.0866,  0.0982,  0.0907,  0.1066,  0.0912,  0.0146,  0.1056,\n",
       "                       0.0903,  0.0823,  0.0448,  0.0920,  0.0951,  0.0922,  0.0855,  0.0962,\n",
       "                       0.0898,  0.0669,  0.0959,  0.0796,  0.0927,  0.0915,  0.0953,  0.0859,\n",
       "                       0.0872,  0.0910,  0.0936,  0.0894,  0.0916,  0.0791,  0.0850,  0.0990,\n",
       "                       0.0940,  0.0936,  0.0982,  0.0959,  0.1012,  0.0974,  0.1146,  0.1073,\n",
       "                       0.0898,  0.0934,  0.0924,  0.0913,  0.1041,  0.0890,  0.0887,  0.0915,\n",
       "                       0.1046,  0.0915,  0.1045,  0.0942,  0.1026,  0.0882,  0.1009,  0.1024,\n",
       "                       0.0863,  0.0950,  0.0964,  0.0847,  0.1022,  0.0998,  0.0809,  0.0963],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.9.layer.1.DenseReluDense.wi.weight',\n",
       "              tensor([[ 1.1647, -2.0227,  0.4964,  ..., -1.5401, -0.5977, -2.7619],\n",
       "                      [ 2.4569,  1.7540,  0.8330,  ...,  2.6645, -0.7915,  0.3249],\n",
       "                      [ 1.0089, -0.0133,  1.7310,  ..., -0.3242,  0.5640, -0.8857],\n",
       "                      ...,\n",
       "                      [ 3.4831,  0.2657,  0.2885,  ...,  3.0652, -1.1088,  1.3166],\n",
       "                      [ 0.6720, -1.4679,  1.0422,  ..., -0.5547,  0.2744, -0.7401],\n",
       "                      [-1.6163, -0.1905,  0.3766,  ..., -0.5776, -0.0226, -0.7603]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.9.layer.1.DenseReluDense.wo.weight',\n",
       "              tensor([[ 0.6419, -0.2756, -0.5977,  ...,  0.0236, -1.4744, -0.8054],\n",
       "                      [-0.0342,  0.7050,  1.1770,  ..., -0.7495,  0.5234, -1.3643],\n",
       "                      [ 0.0055, -0.5295, -0.8578,  ..., -0.8849, -0.8741, -0.1387],\n",
       "                      ...,\n",
       "                      [ 1.2809,  1.3963, -0.2720,  ..., -1.0729,  0.3515, -0.0030],\n",
       "                      [ 0.8642, -1.6735,  0.6042,  ...,  1.1668, -0.0575, -0.1599],\n",
       "                      [-1.1201,  0.8906,  0.7238,  ..., -0.1517, -0.2536, -0.0440]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.9.layer.1.layer_norm.weight',\n",
       "              tensor([0.8380, 0.7533, 0.6747, 0.6932, 0.6242, 0.7849, 0.0202, 0.7346, 0.6014,\n",
       "                      0.7375, 0.6788, 0.6303, 0.6437, 0.7741, 0.5880, 0.6982, 0.6558, 0.7289,\n",
       "                      0.5879, 0.6228, 0.7183, 0.6321, 1.5144, 0.6719, 0.7590, 0.7194, 0.5844,\n",
       "                      0.6991, 0.7085, 0.6236, 0.6875, 0.6195, 0.7081, 0.7351, 0.6924, 0.6901,\n",
       "                      0.7654, 0.8337, 0.5964, 1.1193, 0.6781, 0.8732, 0.5638, 0.7004, 1.0408,\n",
       "                      0.7380, 0.6733, 0.6920, 0.6502, 0.3534, 0.7797, 0.6930, 0.8332, 0.6458,\n",
       "                      0.7176, 0.6820, 1.3290, 0.7187, 0.7198, 0.7071, 0.7385, 0.6749, 0.6333,\n",
       "                      0.6312, 0.6564, 0.8327, 0.6730, 0.7729, 0.8721, 0.6653, 0.6368, 0.7348,\n",
       "                      0.6558, 0.7453, 0.7275, 0.6029, 0.8064, 0.6944, 0.6321, 0.7285, 0.6830,\n",
       "                      0.6549, 0.6845, 0.7472, 0.7074, 0.7096, 0.7932, 0.7735, 0.7489, 0.7194,\n",
       "                      0.7396, 0.6575, 0.6762, 0.6908, 0.6716, 0.6227, 0.6901, 0.7048, 0.6624,\n",
       "                      0.7010, 0.9214, 0.6772, 0.4500, 0.7728, 0.6966, 0.9952, 0.7129, 0.7107,\n",
       "                      0.7045, 1.3549, 0.6959, 0.6002, 0.7585, 0.7962, 0.6376, 0.6972, 0.7112,\n",
       "                      0.6501, 0.7942, 0.7476, 0.7160, 0.6612, 0.9733, 0.6635, 0.5796, 0.7345,\n",
       "                      0.7369, 0.6765, 0.7416, 0.7623, 0.6132, 0.7078, 0.6645, 0.6435, 0.7195,\n",
       "                      0.6063, 0.9049, 0.6899, 0.7386, 0.7225, 0.6783, 0.7943, 0.7342, 0.9075,\n",
       "                      0.6135, 0.7840, 0.7115, 0.7985, 0.6852, 0.7504, 0.6227, 0.6699, 0.6608,\n",
       "                      0.7286, 0.6038, 0.6507, 0.6337, 0.7952, 0.7621, 0.7093, 0.6801, 0.7004,\n",
       "                      0.5984, 2.1112, 0.6710, 0.6289, 0.6671, 0.6995, 0.7089, 0.3903, 0.7771,\n",
       "                      0.6834, 0.7446, 0.7337, 0.7110, 0.6354, 0.7125, 0.2370, 0.7327, 0.4090,\n",
       "                      0.6005, 0.5950, 0.8079, 0.7549, 0.6542, 0.6804, 0.6847, 0.6553, 0.7035,\n",
       "                      0.7110, 0.5935, 0.6928, 0.6812, 0.7245, 0.6852, 0.1753, 0.7226, 0.8070,\n",
       "                      0.7362, 0.6165, 0.6547, 0.8471, 0.7480, 0.7633, 0.7077, 0.7587, 0.6942,\n",
       "                      0.6318, 0.7561, 0.7074, 0.6556, 0.6315, 0.7948, 0.8971, 0.6249, 0.8283,\n",
       "                      0.6916, 0.7573, 0.6867, 0.6308, 0.6264, 0.7117, 0.7296, 0.6768, 0.7245,\n",
       "                      0.7483, 0.8358, 0.3379, 0.7428, 0.8261, 0.6081, 0.6728, 0.4410, 0.6450,\n",
       "                      0.6580, 0.5161, 0.3989, 0.7121, 0.6587, 0.4854, 0.7951, 0.8119, 0.6964,\n",
       "                      0.7024, 0.6315, 0.6646, 0.6936, 0.7002, 0.6705, 0.7446, 1.2691, 0.7078,\n",
       "                      0.5990, 0.7871, 0.6407, 0.6684, 0.5845, 0.5849, 0.2980, 0.7651, 0.7177,\n",
       "                      0.3577, 0.7390, 0.7754, 0.7314, 0.6354, 0.7087, 0.6314, 0.6429, 0.7076,\n",
       "                      0.5703, 0.7762, 0.8145, 0.7653, 0.7839, 0.7290, 0.7466, 0.4934, 0.7157,\n",
       "                      0.5485, 0.8794, 0.8724, 0.7987, 0.7611, 0.7843, 0.5570, 0.6293, 0.6856,\n",
       "                      0.5716, 1.1789, 0.7083, 0.7823, 0.5677, 0.6467, 0.6399, 0.8012, 0.7086,\n",
       "                      0.7052, 0.6316, 0.7685, 0.7137, 0.6725, 0.6703, 0.6803, 0.7157, 0.6540,\n",
       "                      0.6305, 0.7680, 0.7069, 0.6494, 0.7632, 0.7964, 0.7500, 0.5226, 0.7058,\n",
       "                      0.7110, 0.5732, 0.7835, 0.7481, 0.6389, 0.6188, 0.8381, 0.7295, 0.6744,\n",
       "                      0.7374, 0.6640, 0.6166, 0.7777, 0.6134, 0.6789, 0.6043, 0.7725, 0.7767,\n",
       "                      0.7004, 0.7011, 0.7510, 0.6481, 0.7357, 0.7757, 0.9198, 0.7400, 0.6483,\n",
       "                      0.7618, 0.8136, 0.7258, 0.7890, 0.5657, 0.7787, 0.6609, 1.2581, 0.7899,\n",
       "                      0.6577, 0.7429, 0.6151, 0.6433, 0.7145, 0.7403, 0.6341, 0.6613, 0.6831,\n",
       "                      0.6940, 0.8235, 0.7529, 0.6165, 0.6752, 0.7484, 0.5273, 0.7916, 0.6487,\n",
       "                      0.8327, 0.8350, 0.7533, 0.1710, 0.6879, 0.7798, 0.7038, 0.7457, 0.7472,\n",
       "                      0.4147, 0.6010, 0.6644, 0.6455, 0.7610, 0.7587, 0.6542, 0.6846, 0.7084,\n",
       "                      0.5773, 0.5419, 0.7591, 0.7239, 1.7117, 0.7518, 0.8036, 0.7180, 0.7319,\n",
       "                      0.7596, 0.6960, 0.6289, 0.6785, 0.6825, 0.6206, 0.7305, 0.6771, 0.6561,\n",
       "                      0.6831, 0.7815, 0.6395, 0.6788, 0.7159, 0.7126, 0.6119, 0.7434, 0.7474,\n",
       "                      0.6096, 0.6148, 0.6457, 0.6488, 0.1393, 0.9125, 0.8231, 0.7107, 0.6131,\n",
       "                      0.1950, 0.7108, 0.6406, 0.7152, 0.6357, 0.7336, 0.7230, 0.7694, 0.6689,\n",
       "                      0.4014, 0.7179, 0.7603, 0.6408, 0.7533, 0.7030, 0.6246, 0.7191, 0.7486,\n",
       "                      0.7198, 0.7223, 0.5120, 0.6884, 0.7469, 0.6773, 0.7831, 0.6827, 0.7172,\n",
       "                      0.6823, 0.6925, 0.6436, 0.6144, 0.7613, 0.7339, 0.7397, 0.6984, 0.6805,\n",
       "                      0.6899, 0.6129, 0.6549, 0.6739, 0.7986, 0.6103, 0.6702, 0.5850, 0.7163,\n",
       "                      0.6836, 0.7316, 0.6077, 0.6666, 0.6976, 0.5733, 0.7338, 0.6643, 0.7441,\n",
       "                      0.6943, 0.7009, 0.6400, 0.7857, 1.2781, 0.7765, 0.6493, 0.8286, 0.6240,\n",
       "                      0.7582, 1.0338, 0.7564, 0.6746, 0.6801, 0.6749, 1.1472, 0.7146, 0.7839,\n",
       "                      0.7592, 0.6915, 0.6937, 0.7181, 0.7111, 0.6143, 0.7126, 0.7369, 1.1689,\n",
       "                      0.7035, 0.7511, 0.5789, 0.6822, 0.6791, 0.7333, 0.7483, 0.6721, 0.7542,\n",
       "                      0.6415, 0.7689, 0.6651, 0.6419, 0.6785, 0.5728, 0.7447, 0.7090, 0.8032,\n",
       "                      0.6441, 0.6724, 0.7194, 1.0818, 0.6641, 0.6204, 0.5993, 0.7090, 0.6817,\n",
       "                      0.7422, 0.6914, 0.6884, 0.7029, 0.6845, 0.7361, 0.5882, 0.7834, 0.7054,\n",
       "                      0.7294, 0.6824, 0.7218, 0.7145, 0.7089, 0.6587, 0.6628, 0.6852, 0.6723,\n",
       "                      0.1690, 0.7958, 0.8166, 0.6265, 0.8451, 0.6539, 0.7420, 0.6477, 0.6178,\n",
       "                      0.7979, 0.6612, 0.7351, 0.6993, 0.8242, 0.6759, 0.7640, 0.7205, 0.3117,\n",
       "                      0.6880, 0.6900, 0.7238, 0.7473, 0.1173, 0.6574, 0.6709, 0.7178, 0.7229,\n",
       "                      0.6463, 0.7530, 0.6094, 0.5888, 0.6185, 0.6194, 0.6819, 0.5969, 0.6358,\n",
       "                      0.6500, 0.6801, 0.7529, 0.6187, 0.7422, 0.7310, 0.6994, 0.6383, 0.7485,\n",
       "                      0.6335, 0.6354, 0.6803, 0.7016, 0.7977, 0.7468, 0.8616, 0.7124, 0.7011,\n",
       "                      0.4142, 0.7299, 0.6395, 0.6553, 0.7214, 0.7725, 0.7502, 0.7181, 0.7654,\n",
       "                      0.7301, 0.6200, 0.6866, 0.8306, 0.7846, 0.7349, 0.7554, 0.7193, 0.7060,\n",
       "                      0.6621, 0.7030, 0.6516, 0.7745, 0.6792, 0.6124, 0.7746, 0.6415, 0.6901,\n",
       "                      0.8208, 0.6560, 0.7009, 0.8166, 0.7128, 0.2448, 0.7324, 0.6754, 0.5869,\n",
       "                      0.6410, 0.6567, 0.6636, 0.7268, 0.8082, 0.6875, 0.5846, 0.7496, 0.6926,\n",
       "                      0.6763, 0.6285, 0.6414, 0.7359, 0.6317, 0.7322, 0.6378, 0.7036, 0.6790,\n",
       "                      0.6239, 0.5561, 0.6435, 0.6144, 0.1576, 0.6683, 0.7428, 0.7022, 0.7378,\n",
       "                      0.7391, 0.6839, 0.7030, 0.6942, 0.7800, 0.8547, 0.7941, 0.7037, 0.6850,\n",
       "                      0.6732, 0.6741, 0.7373, 0.7526, 0.6871, 0.6901, 0.6761, 0.6255, 0.7178,\n",
       "                      0.6468, 0.9318, 0.6292, 0.7536, 0.6496, 0.6768, 0.7512, 0.7222, 0.6546,\n",
       "                      0.6613, 0.6728, 0.6506, 0.8004, 0.6891, 0.6444, 0.7639, 0.8044, 0.6900,\n",
       "                      0.7271, 0.7018, 0.8049, 0.6041, 0.7035, 0.7955, 0.7031, 0.6137, 0.7858,\n",
       "                      0.7728, 0.6960, 0.6573, 0.1814, 0.7362, 0.6988, 0.6585, 0.7064, 0.6016,\n",
       "                      0.7125, 0.6250, 0.7805, 0.6840, 0.7100, 0.6419, 0.7562, 0.6799, 0.6859,\n",
       "                      0.7385, 0.7097, 0.7036, 0.6219, 0.5162, 0.6475, 0.6931, 0.6849, 0.7037,\n",
       "                      0.7495, 0.7853, 0.7324, 0.8680, 0.7353, 0.6829, 0.6815, 0.8372, 0.6313,\n",
       "                      0.7316, 0.6992, 0.4820, 0.7054, 0.6585, 0.9507, 0.6265, 0.7115, 0.7201,\n",
       "                      0.7090, 0.6275, 0.7104, 0.7526, 0.7907, 0.4923, 0.6841, 0.7286, 0.6005,\n",
       "                      0.7015, 0.6913, 0.6957], device='cuda:0')),\n",
       "             ('encoder.block.10.layer.0.SelfAttention.q.weight',\n",
       "              tensor([[-0.1196, -0.1165,  0.0163,  ...,  0.0696,  0.0498, -0.1232],\n",
       "                      [-0.0203, -0.0007, -0.0654,  ...,  0.0560,  0.0262, -0.0120],\n",
       "                      [ 0.1838,  0.0209, -0.0221,  ..., -0.0125,  0.0052,  0.0464],\n",
       "                      ...,\n",
       "                      [-0.0296, -0.1375,  0.0581,  ..., -0.0181,  0.0051,  0.0228],\n",
       "                      [-0.0751,  0.0636, -0.0681,  ...,  0.0410, -0.0287, -0.0088],\n",
       "                      [-0.0256,  0.0096,  0.0793,  ...,  0.0120, -0.0318, -0.0676]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.10.layer.0.SelfAttention.k.weight',\n",
       "              tensor([[-0.7437,  0.4465,  0.0965,  ...,  0.7512, -0.0997,  0.2436],\n",
       "                      [ 0.0894, -0.8283, -0.4079,  ..., -0.1462, -1.5980,  0.3384],\n",
       "                      [ 0.8135,  0.1573,  0.7864,  ...,  0.2277,  0.2794, -0.1295],\n",
       "                      ...,\n",
       "                      [ 0.3201, -1.0178, -0.1528,  ...,  0.0735,  0.2161,  0.2559],\n",
       "                      [ 0.3600,  0.2484, -0.9435,  ...,  0.2565,  0.0111,  0.0388],\n",
       "                      [ 0.1039, -0.1513,  0.1165,  ...,  0.3952, -0.2850, -0.1030]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.10.layer.0.SelfAttention.v.weight',\n",
       "              tensor([[ 0.6608, -2.3980, -1.4000,  ..., -2.2612, -0.8257, -0.9941],\n",
       "                      [-1.1437, -0.5916, -1.7915,  ..., -0.5238,  1.0132,  1.7253],\n",
       "                      [-1.8648,  0.2513, -0.2721,  ...,  1.1524, -1.7303, -0.1835],\n",
       "                      ...,\n",
       "                      [ 1.8227,  0.4427,  0.9304,  ...,  1.8349, -0.8726, -0.6949],\n",
       "                      [-1.5275, -0.7073,  0.7843,  ..., -0.0069,  1.3250,  1.7642],\n",
       "                      [ 0.2888,  0.5240,  0.6367,  ..., -1.0146, -2.3370,  0.7308]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.10.layer.0.SelfAttention.o.weight',\n",
       "              tensor([[ 0.1909, -0.4991,  0.7189,  ..., -1.3808,  5.2149,  1.0973],\n",
       "                      [ 1.1372, -0.7240,  1.4067,  ..., -3.3152,  1.2777, -1.0637],\n",
       "                      [ 1.9462,  1.2904, -3.3512,  ..., -1.1169, -1.0027, -2.0415],\n",
       "                      ...,\n",
       "                      [-0.5359,  3.0051,  2.1028,  ..., -0.6920, -0.7861, -1.0555],\n",
       "                      [ 1.4316, -2.6019,  0.3409,  ...,  1.2401,  0.3141,  2.0086],\n",
       "                      [ 0.8714, -2.2947, -0.4234,  ...,  1.0622, -2.9307, -2.4042]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.10.layer.0.layer_norm.weight',\n",
       "              tensor([ 9.6729e-02,  1.0159e-01,  1.0069e-01,  1.0415e-01,  8.6078e-02,\n",
       "                       8.5484e-02,  1.4471e-02,  1.0532e-01,  4.3395e-02,  8.6218e-02,\n",
       "                       9.6025e-02,  1.0168e-01,  7.4100e-02,  8.0290e-02,  7.1738e-02,\n",
       "                       9.0799e-02,  8.5330e-02,  8.6616e-02,  7.8525e-02,  7.8847e-02,\n",
       "                       7.5420e-02,  8.8615e-02,  1.3731e-02,  7.7256e-02,  8.0371e-02,\n",
       "                       9.6749e-02,  8.7536e-02,  8.6974e-02,  1.0709e-01,  8.0491e-02,\n",
       "                       8.5962e-02,  8.9490e-02,  1.0169e-01,  8.2430e-02,  9.1912e-02,\n",
       "                       9.7154e-02,  6.7067e-02,  9.6024e-02,  7.1411e-02,  7.3683e-02,\n",
       "                       8.7720e-02,  9.1487e-02,  9.1940e-02,  9.6391e-02,  2.4665e-02,\n",
       "                       8.8770e-02,  7.6702e-02,  9.0272e-02,  9.5325e-02,  3.3224e-02,\n",
       "                       9.9682e-02,  8.1169e-02,  6.4690e-02,  8.4773e-02, -1.8721e-04,\n",
       "                       8.6246e-02,  7.3967e-02,  8.6973e-02,  1.0080e-01,  8.2575e-02,\n",
       "                       9.3833e-02,  7.5681e-02,  7.8179e-02,  8.1890e-02,  9.6241e-02,\n",
       "                       2.1423e-02,  7.9623e-02,  9.1748e-02,  9.6060e-02,  9.0002e-02,\n",
       "                       7.7156e-02,  7.4591e-02,  7.8252e-02,  8.6662e-02,  7.2955e-02,\n",
       "                       8.2027e-02,  7.1283e-02,  9.5606e-02,  8.0726e-02,  9.7957e-02,\n",
       "                       8.2027e-02,  8.7132e-02,  8.8328e-02,  1.1600e-01,  9.3760e-02,\n",
       "                       8.8443e-02,  9.6385e-02,  8.9212e-02,  9.4398e-02,  8.2704e-02,\n",
       "                       9.0881e-02,  9.1436e-02,  7.4260e-02,  8.5795e-02,  9.5895e-02,\n",
       "                       8.6071e-02,  9.0429e-02,  9.3429e-02,  9.3519e-02,  7.6291e-02,\n",
       "                       1.0957e-01,  9.2796e-02,  7.6119e-02,  8.0699e-02,  8.8285e-02,\n",
       "                       7.2539e-02,  9.1828e-02,  7.3064e-02,  7.7500e-02,  4.0391e-02,\n",
       "                       8.2139e-02,  8.9966e-02,  8.9155e-02,  6.5226e-02,  9.2648e-02,\n",
       "                       8.7818e-02,  9.0368e-02,  8.3765e-02,  8.0068e-02,  7.1286e-02,\n",
       "                       8.2672e-02,  8.7240e-02,  9.1345e-02,  8.7647e-02,  8.8870e-02,\n",
       "                       9.0472e-02,  9.7050e-02,  8.3843e-02,  9.2397e-02,  9.3898e-02,\n",
       "                       5.1997e-02,  1.0282e-01,  7.9047e-02,  7.4635e-02,  8.7187e-02,\n",
       "                       8.6118e-02,  1.0251e-01,  8.5060e-02,  9.3624e-02,  9.6727e-02,\n",
       "                       9.0115e-02,  1.0326e-01,  9.3941e-02,  9.3817e-02,  8.5749e-02,\n",
       "                       9.8750e-02,  8.0165e-02,  9.6517e-02,  9.5787e-02,  1.0259e-01,\n",
       "                       9.6193e-02,  8.5145e-02,  9.0923e-02,  9.4164e-02,  8.3536e-02,\n",
       "                       9.2496e-02,  8.5987e-02,  9.9159e-02,  8.8455e-02,  7.8702e-02,\n",
       "                       7.7095e-02,  8.7552e-02,  8.0858e-02,  1.9791e-02,  8.2719e-02,\n",
       "                       7.9602e-02,  9.2915e-02,  9.0222e-02,  7.8748e-02,  6.6169e-02,\n",
       "                       1.1407e-01,  1.0037e-01,  9.3315e-02,  7.1161e-02,  7.9771e-02,\n",
       "                       8.8508e-02,  1.0381e-01,  4.3799e-02,  8.8766e-02,  1.0946e-02,\n",
       "                       7.9858e-02,  8.4012e-02,  8.9882e-02,  9.2081e-02,  9.6071e-02,\n",
       "                       8.5700e-02,  6.8049e-02,  9.3064e-02,  8.8766e-02,  8.2528e-02,\n",
       "                       8.4697e-02,  9.2001e-02,  7.4029e-02,  9.6263e-02,  9.1859e-02,\n",
       "                       3.0336e-02,  8.2871e-02,  1.0008e-01,  9.3451e-02,  7.0947e-02,\n",
       "                       7.6633e-02,  9.0471e-02,  7.6960e-02,  8.7477e-02,  8.4513e-02,\n",
       "                       9.4809e-02,  8.9242e-02,  8.1868e-02,  8.6722e-02,  9.7077e-02,\n",
       "                       9.0160e-02,  7.6722e-02,  9.0086e-02,  9.7010e-02,  9.4419e-02,\n",
       "                       9.5761e-02,  8.3488e-02,  8.3368e-02,  9.5466e-02,  4.2697e-02,\n",
       "                       6.7557e-02,  9.2853e-02,  8.0121e-02,  9.5884e-02,  7.9305e-02,\n",
       "                       8.2365e-02,  9.3911e-02,  7.2618e-02,  9.3018e-02,  9.7752e-02,\n",
       "                       8.6458e-02,  8.2749e-02,  6.1268e-02,  7.3762e-02,  8.2049e-02,\n",
       "                       7.3374e-02,  7.4137e-02,  1.0084e-01,  9.0632e-02,  6.1301e-02,\n",
       "                       8.6007e-02,  1.0555e-01,  9.3719e-02,  9.2211e-02,  8.3220e-02,\n",
       "                       8.6824e-02,  9.6238e-02,  9.7246e-02,  8.4380e-02,  8.9400e-02,\n",
       "                       4.7580e-02,  7.5583e-02,  8.4801e-02,  8.1024e-02,  8.3665e-02,\n",
       "                       8.7781e-02,  7.8088e-02,  9.1361e-02,  5.3756e-02,  8.6612e-02,\n",
       "                       9.9867e-02,  8.9716e-02,  8.3840e-02,  8.6637e-02,  7.1120e-02,\n",
       "                       9.7544e-02,  8.8160e-02,  9.0009e-02,  8.2307e-02,  9.3464e-02,\n",
       "                       5.6507e-02,  1.0180e-01,  7.2807e-02,  4.6427e-05,  9.4691e-02,\n",
       "                       9.5697e-02,  8.6994e-02,  7.6310e-02,  9.4181e-02,  8.6606e-02,\n",
       "                       9.7363e-02,  1.0976e-01,  9.1806e-02,  8.9853e-02,  1.0435e-01,\n",
       "                       6.7820e-02,  7.0326e-02,  9.2681e-02,  9.4640e-02,  5.3622e-02,\n",
       "                       8.1871e-02,  8.7557e-02,  8.2734e-02,  8.2751e-02,  8.8735e-02,\n",
       "                       7.5889e-02,  1.0026e-01,  8.4970e-02,  8.2979e-02,  8.4651e-02,\n",
       "                       1.0399e-01,  8.5691e-02,  1.1336e-01,  8.4302e-02,  8.1664e-02,\n",
       "                       8.7584e-02,  9.1543e-02,  9.3276e-02,  9.1543e-02,  8.5252e-02,\n",
       "                       1.0201e-01,  1.1302e-01,  9.0572e-02,  7.7567e-02,  9.3733e-02,\n",
       "                       9.1481e-02,  8.1523e-02,  1.0406e-01,  8.4970e-02,  8.5878e-02,\n",
       "                       7.5813e-02,  8.9617e-02,  8.5764e-02,  7.4886e-02,  9.7584e-02,\n",
       "                       8.0652e-02,  9.8016e-02,  9.9594e-02,  8.8630e-02,  8.9889e-02,\n",
       "                       8.9472e-02,  7.3932e-02,  8.5634e-02,  9.1031e-02,  8.1584e-02,\n",
       "                       8.7342e-02,  7.4751e-02,  9.2456e-02,  9.5167e-02,  9.8414e-02,\n",
       "                       1.0789e-01,  1.0491e-01,  8.2342e-02,  8.6181e-02,  9.3058e-02,\n",
       "                       8.6577e-02,  9.3475e-02,  8.8473e-02,  8.5732e-02,  5.0664e-02,\n",
       "                       9.9694e-02,  8.2716e-02,  9.2100e-02,  8.7468e-02,  8.9272e-02,\n",
       "                       9.3246e-02,  9.2674e-02,  9.1605e-02,  8.3424e-02,  9.6865e-02,\n",
       "                       8.1321e-02,  9.0281e-02,  8.9271e-02,  7.5290e-02,  7.7184e-02,\n",
       "                       7.6897e-02,  4.4513e-02,  8.3543e-02,  7.8771e-02,  9.3382e-02,\n",
       "                       7.7443e-02,  1.0458e-01,  3.3793e-02,  7.4647e-02,  9.9180e-02,\n",
       "                       8.2634e-02,  7.0037e-02,  9.4430e-02,  5.0797e-02,  7.4911e-02,\n",
       "                       9.2438e-02,  9.0733e-02,  9.5293e-02,  9.1012e-02,  9.6434e-02,\n",
       "                       4.9866e-02,  8.3431e-02,  6.2935e-02,  8.4786e-02,  1.0037e-01,\n",
       "                       8.4847e-02,  1.4896e-02,  9.1001e-02,  1.0970e-01,  1.0711e-01,\n",
       "                       8.4292e-02,  9.7472e-02,  8.2970e-02,  8.6615e-02,  8.0938e-02,\n",
       "                       9.2376e-02,  2.6005e-02,  6.9285e-02,  1.0553e-01,  8.3778e-02,\n",
       "                       8.7145e-02,  9.2494e-02,  8.8323e-02,  9.0517e-02,  8.5439e-02,\n",
       "                       8.5079e-02,  7.8307e-02,  8.5023e-02,  8.9300e-02,  9.0694e-02,\n",
       "                       9.0987e-02,  6.8278e-02,  7.0947e-02,  3.4641e-02,  9.3846e-02,\n",
       "                       8.8084e-02,  8.7690e-02,  9.2122e-02,  4.5507e-02,  8.6238e-02,\n",
       "                       9.4253e-02,  8.4754e-02,  9.1769e-02,  1.0187e-01,  9.7346e-02,\n",
       "                       8.2761e-02,  8.2931e-02,  6.4059e-02,  7.9555e-02,  8.2014e-02,\n",
       "                       8.1359e-02,  8.2961e-02,  8.6324e-02,  9.3631e-02,  1.0004e-01,\n",
       "                       9.6840e-02,  7.8380e-02,  9.0547e-02,  7.1374e-02,  7.8836e-02,\n",
       "                       9.3009e-02,  7.2628e-02,  9.4485e-02,  9.6387e-02,  9.6100e-02,\n",
       "                       8.3005e-02,  8.3454e-02,  9.7081e-02,  9.8189e-02,  8.6572e-02,\n",
       "                       1.0354e-01,  1.0320e-01,  1.0307e-01,  8.7625e-02,  8.4462e-02,\n",
       "                       9.1983e-02,  8.1055e-02,  8.3078e-02,  8.9005e-02,  9.6800e-02,\n",
       "                       8.4114e-02,  6.1806e-02,  7.7573e-02,  7.6510e-02,  8.7306e-02,\n",
       "                       7.3069e-02,  8.3114e-02,  7.9171e-02,  8.4029e-02,  9.4037e-02,\n",
       "                       9.4286e-02,  8.3602e-02,  8.6403e-02,  3.5899e-02,  9.5981e-02,\n",
       "                       1.0350e-01,  4.0909e-02,  7.8455e-02,  8.5381e-02,  1.0899e-01,\n",
       "                       8.9856e-02,  9.6820e-02,  7.4615e-02,  7.8788e-02,  9.1960e-02,\n",
       "                       1.0103e-01,  8.7735e-02,  5.6598e-02,  7.5267e-02,  8.2398e-02,\n",
       "                       9.3748e-02,  8.6259e-02,  9.0399e-02,  9.2176e-02,  7.5571e-02,\n",
       "                       7.3615e-02,  9.8266e-02,  8.2060e-02,  1.0373e-02,  8.8541e-02,\n",
       "                       9.5948e-02,  8.7723e-02,  8.0967e-02,  9.0094e-02,  8.9103e-02,\n",
       "                       9.7330e-02,  8.5496e-02,  4.4113e-02,  8.8918e-02,  1.0079e-01,\n",
       "                       8.0443e-02,  9.5042e-02,  8.6438e-02,  8.8817e-02,  9.7857e-02,\n",
       "                       7.0853e-02,  7.2737e-02,  7.6514e-02,  7.5370e-02,  8.5927e-02,\n",
       "                       4.3341e-02,  9.3397e-02,  8.3161e-02,  8.1808e-02,  1.0815e-01,\n",
       "                       9.3183e-02,  7.9537e-02,  9.1074e-02,  8.8472e-02,  8.5818e-02,\n",
       "                       9.5063e-02,  8.7517e-02,  9.1054e-02,  9.5203e-02,  8.9561e-02,\n",
       "                       9.5662e-02,  8.0767e-02,  8.6928e-02,  1.0022e-01,  8.6422e-02,\n",
       "                       9.2612e-02,  9.1303e-02,  9.8965e-02,  8.4521e-02,  4.2430e-02,\n",
       "                       8.9031e-02,  9.4386e-02,  8.0204e-02,  1.0806e-01,  8.2196e-02,\n",
       "                       9.7189e-02,  9.4270e-02,  8.4339e-02,  9.1561e-02,  7.7391e-02,\n",
       "                       8.5841e-02,  7.2699e-02,  9.1020e-02,  8.7059e-02,  9.7350e-02,\n",
       "                       9.4124e-02,  1.3131e-02,  9.4101e-02,  8.9794e-02,  9.5293e-02,\n",
       "                       9.2958e-02,  5.3064e-02,  7.3273e-02,  8.3154e-02,  9.3480e-02,\n",
       "                       8.8681e-02,  9.6128e-02,  1.0142e-01,  9.0392e-02,  8.2282e-02,\n",
       "                       9.8423e-02,  9.3855e-02,  7.2341e-02,  7.3115e-02,  7.7526e-02,\n",
       "                       8.8046e-02,  8.2711e-02,  8.7149e-02,  8.1930e-02,  8.7235e-02,\n",
       "                       8.8683e-02,  1.0100e-01,  8.9594e-02,  9.7004e-02,  8.5590e-02,\n",
       "                       1.0061e-01,  8.4154e-02,  8.6495e-02,  8.3072e-02,  7.9188e-02,\n",
       "                       9.8569e-02,  8.8700e-02,  1.0417e-01,  7.9371e-02,  9.9388e-02,\n",
       "                       7.3797e-02,  8.9989e-02,  1.0242e-01,  9.0018e-02,  9.1782e-02,\n",
       "                       9.8943e-02,  9.0768e-02,  8.8900e-02,  9.2347e-02,  9.0217e-02,\n",
       "                       1.0155e-01,  7.8901e-02,  1.0747e-01,  9.1166e-02,  9.4134e-02,\n",
       "                       9.9914e-02,  7.9546e-02,  8.6429e-02,  1.1065e-01,  9.2961e-02,\n",
       "                       8.9070e-02,  9.4899e-02,  8.3980e-02,  8.7396e-02,  7.6669e-02,\n",
       "                       1.0614e-01,  6.8302e-02,  9.5882e-02,  1.0203e-01,  9.0651e-02,\n",
       "                       5.4450e-02,  8.6061e-02,  9.6103e-02,  9.7905e-02,  9.0517e-02,\n",
       "                       8.2684e-02,  9.3840e-02,  8.9348e-02,  8.0110e-02,  1.0353e-01,\n",
       "                       1.0101e-01,  1.0451e-01,  8.3389e-02,  7.5781e-02,  7.4717e-02,\n",
       "                       7.8037e-02,  8.4322e-02,  9.8907e-02,  7.0314e-02,  9.9065e-02,\n",
       "                       9.6853e-02,  9.4207e-02,  1.0329e-01,  9.8262e-02,  9.8022e-02,\n",
       "                       7.9574e-02,  7.6745e-03,  8.3287e-02,  1.0504e-01,  8.1389e-02,\n",
       "                       8.7798e-02,  9.2839e-02,  8.7637e-02,  1.0614e-01,  8.3267e-02,\n",
       "                       9.4204e-02,  6.8870e-02,  8.1143e-02,  7.1270e-02,  1.0712e-01,\n",
       "                       8.7291e-02,  9.0322e-02,  7.7541e-02,  8.2350e-02,  9.0872e-02,\n",
       "                       9.2906e-02,  8.9496e-02,  8.5198e-02,  6.9431e-02,  9.5446e-02,\n",
       "                       5.3550e-02,  7.6572e-02,  9.4830e-02,  9.3056e-02,  1.0463e-01,\n",
       "                       8.0866e-02,  9.5801e-02,  7.4862e-02,  7.6619e-02,  8.1888e-02,\n",
       "                       8.4949e-02,  1.0226e-01,  7.8387e-02,  8.9361e-02,  9.2570e-02,\n",
       "                       8.5722e-02,  7.6532e-02,  8.8845e-02,  8.9708e-02,  9.5339e-02,\n",
       "                       9.7177e-02,  8.5102e-02,  8.7496e-02,  9.9489e-02,  8.1635e-02,\n",
       "                       8.9422e-03,  9.1610e-02,  9.2885e-02,  8.4907e-02,  4.0155e-02,\n",
       "                       8.8518e-02,  8.6347e-02,  1.0029e-01,  9.9749e-02,  8.7082e-02,\n",
       "                       8.2540e-02,  7.2344e-02,  9.7778e-02,  7.4328e-02,  1.0319e-01,\n",
       "                       1.0477e-01,  8.8400e-02,  7.4982e-02,  9.0421e-02,  9.4233e-02,\n",
       "                       1.0513e-01,  9.4377e-02,  9.3423e-02,  9.5173e-02,  9.5390e-02,\n",
       "                       9.5369e-02,  8.7023e-02,  8.4520e-02,  1.0266e-01,  9.7158e-02,\n",
       "                       9.6024e-02,  9.2374e-02,  1.0913e-01,  9.5487e-02,  9.6198e-02,\n",
       "                       9.2573e-02,  1.0121e-01,  8.5763e-02,  9.2445e-02,  8.1576e-02,\n",
       "                       8.7747e-02,  8.7284e-02,  1.0340e-01,  8.4639e-02,  9.9166e-02,\n",
       "                       9.1942e-02,  9.0839e-02,  9.2300e-02,  1.0756e-01,  1.0978e-01,\n",
       "                       7.4011e-02,  1.0777e-01,  9.0275e-02,  7.5010e-02,  1.0438e-01,\n",
       "                       9.5476e-02,  8.2580e-02,  1.0421e-01], device='cuda:0')),\n",
       "             ('encoder.block.10.layer.1.DenseReluDense.wi.weight',\n",
       "              tensor([[-0.4027, -1.5256, -0.0652,  ..., -1.7219,  0.9100,  2.2852],\n",
       "                      [ 0.6809, -3.6598, -0.0324,  ..., -1.8957, -0.0386,  1.0119],\n",
       "                      [-0.7337,  0.3183, -0.5063,  ...,  1.7502,  2.2695, -0.7838],\n",
       "                      ...,\n",
       "                      [-0.4199,  0.8541, -2.3152,  ..., -2.9968,  0.4568,  1.3203],\n",
       "                      [-3.1787, -1.7136, -1.4622,  ..., -1.6330,  0.8658, -0.3580],\n",
       "                      [ 1.4102,  0.3386,  1.6450,  ..., -2.1828, -1.7882, -4.3904]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.10.layer.1.DenseReluDense.wo.weight',\n",
       "              tensor([[-2.2155,  1.1133, -0.2892,  ...,  0.6146,  0.6649,  0.3961],\n",
       "                      [-0.0263, -1.1047,  0.0325,  ...,  1.7547,  0.8466,  0.3038],\n",
       "                      [ 1.2802,  0.9645,  0.5658,  ..., -0.0409,  0.2462,  0.5431],\n",
       "                      ...,\n",
       "                      [-0.6971,  0.4801,  0.6949,  ..., -1.2450, -0.0983, -1.0877],\n",
       "                      [ 1.9139, -0.1596,  1.1070,  ..., -0.0549,  0.4372, -0.3452],\n",
       "                      [ 0.3442,  1.3318, -1.5336,  ...,  1.2266, -0.3660,  1.3016]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.10.layer.1.layer_norm.weight',\n",
       "              tensor([0.7137, 0.7156, 0.7365, 0.6563, 0.6779, 0.7131, 0.0137, 0.7389, 0.4151,\n",
       "                      0.6708, 0.6649, 0.5533, 0.5456, 0.7554, 0.6674, 0.7719, 0.6681, 0.7965,\n",
       "                      0.4706, 0.7430, 0.7377, 0.7565, 1.7990, 0.6951, 0.6226, 0.7546, 0.7259,\n",
       "                      0.6661, 0.7492, 0.7281, 0.7056, 0.7332, 0.6735, 0.6240, 0.7025, 0.6274,\n",
       "                      0.6713, 0.7858, 0.6690, 0.8632, 0.6649, 0.7302, 0.3903, 0.7068, 0.6422,\n",
       "                      0.7244, 0.5879, 0.6632, 0.6662, 0.3109, 0.7345, 0.7316, 0.8099, 0.6724,\n",
       "                      0.7870, 0.6697, 1.4333, 0.6779, 0.7052, 0.7729, 0.6092, 0.6889, 0.7742,\n",
       "                      0.6966, 0.7226, 0.5440, 0.7062, 0.7015, 0.7696, 0.7370, 0.7607, 0.4843,\n",
       "                      0.5954, 0.7622, 0.7099, 0.8021, 0.8393, 0.6931, 0.6606, 0.7138, 0.7069,\n",
       "                      0.7441, 0.6905, 0.7059, 0.7067, 0.7425, 0.7297, 0.7416, 0.7124, 0.6366,\n",
       "                      0.7643, 0.6668, 0.7719, 0.7532, 0.7138, 0.6527, 0.7297, 0.6297, 0.6105,\n",
       "                      0.7065, 0.7680, 0.7217, 0.3268, 0.7829, 0.6881, 0.9331, 0.7137, 0.7632,\n",
       "                      0.7304, 1.1399, 0.6983, 0.6795, 0.7159, 0.7299, 0.7564, 0.6391, 0.7283,\n",
       "                      0.6590, 0.5785, 0.5835, 0.7835, 0.6308, 0.8144, 0.7792, 0.4542, 0.6781,\n",
       "                      0.6803, 0.7615, 0.6586, 0.7974, 0.4247, 0.6760, 0.6616, 0.6278, 0.7091,\n",
       "                      0.4782, 0.7345, 0.7369, 0.7420, 0.7026, 0.7171, 0.6967, 0.7255, 0.7415,\n",
       "                      0.6043, 0.7931, 0.6513, 0.7039, 0.6978, 0.7023, 0.6754, 0.6443, 0.6716,\n",
       "                      0.7902, 0.7063, 0.7498, 0.6991, 0.6880, 0.6828, 0.6918, 0.7786, 0.6486,\n",
       "                      0.5779, 2.2627, 0.7127, 0.7081, 0.6687, 0.6502, 0.6719, 0.3530, 0.6620,\n",
       "                      0.7464, 0.8196, 0.6461, 0.7259, 0.6604, 0.6796, 0.1921, 0.7118, 0.4073,\n",
       "                      0.7258, 0.7758, 0.7552, 0.7093, 0.6557, 0.7987, 0.6568, 0.6233, 0.6820,\n",
       "                      0.6635, 0.6745, 0.6515, 0.7460, 0.6573, 0.6623, 0.1871, 0.6887, 0.7344,\n",
       "                      0.6584, 0.6147, 0.6716, 0.7094, 0.7526, 0.6929, 0.6423, 0.6989, 0.7416,\n",
       "                      0.6175, 0.7414, 0.7501, 0.7037, 0.7104, 0.6154, 0.8478, 0.7304, 0.7544,\n",
       "                      0.6973, 0.8170, 0.6462, 0.3868, 0.5709, 0.7183, 0.7522, 0.6423, 0.6357,\n",
       "                      0.6535, 0.6187, 0.3866, 0.6902, 0.6328, 0.6536, 0.6994, 0.5409, 0.6041,\n",
       "                      0.7175, 0.6045, 0.4950, 0.7397, 0.6952, 0.3388, 0.6805, 0.8087, 0.7221,\n",
       "                      0.6976, 0.6659, 0.7190, 0.6627, 0.6631, 0.6394, 0.6994, 1.0102, 0.6580,\n",
       "                      0.6667, 0.7262, 0.7188, 0.6834, 0.7401, 0.6164, 0.2247, 0.6354, 0.6918,\n",
       "                      0.4384, 0.6981, 0.7405, 0.7308, 0.7148, 0.7627, 0.6795, 0.7112, 0.6301,\n",
       "                      0.4358, 0.7696, 0.7764, 0.7122, 0.7113, 0.7101, 0.7503, 0.3965, 0.7180,\n",
       "                      0.6519, 0.8688, 0.7775, 0.7025, 0.8695, 0.7331, 0.4354, 0.7356, 0.6866,\n",
       "                      0.5517, 1.0468, 0.7029, 0.7451, 0.5960, 0.6754, 0.8309, 0.7318, 0.6007,\n",
       "                      0.7980, 0.7028, 0.6828, 0.8245, 0.5816, 0.6362, 0.5859, 0.7162, 0.6819,\n",
       "                      0.6686, 0.7150, 0.6888, 0.6892, 0.7850, 0.7739, 0.6429, 0.5268, 0.6874,\n",
       "                      0.6605, 0.6308, 0.7192, 0.6697, 0.7172, 0.6224, 0.7815, 0.6921, 0.6799,\n",
       "                      0.6627, 0.7308, 0.5389, 0.7421, 0.7263, 0.7393, 0.5713, 0.5670, 0.6759,\n",
       "                      0.6637, 0.6708, 0.6976, 0.7256, 0.7041, 0.7340, 0.8568, 0.7565, 0.6926,\n",
       "                      0.7016, 0.7437, 0.7523, 0.8089, 0.4929, 0.6999, 0.5623, 1.0455, 0.7193,\n",
       "                      0.6882, 0.8105, 0.6310, 0.6556, 0.7102, 0.7115, 0.6372, 0.7204, 0.5311,\n",
       "                      0.7576, 0.7512, 0.6220, 0.7104, 0.7841, 0.6376, 0.3441, 0.8514, 0.6326,\n",
       "                      0.8576, 0.7284, 0.7408, 0.1483, 0.6556, 0.6474, 0.6570, 0.7248, 0.6746,\n",
       "                      0.3679, 0.5851, 0.7158, 0.7272, 0.6760, 0.6699, 0.7285, 0.5180, 0.7266,\n",
       "                      0.4677, 0.6875, 0.7357, 0.6741, 1.8205, 0.7017, 0.7840, 0.7061, 0.7053,\n",
       "                      0.7310, 0.7237, 0.7399, 0.7171, 0.6420, 0.6556, 0.7049, 0.7493, 0.7076,\n",
       "                      0.7518, 0.7048, 0.6958, 0.6788, 0.6766, 0.7060, 0.6895, 0.6929, 0.7528,\n",
       "                      0.6848, 0.6883, 0.5639, 0.6579, 0.1238, 0.8835, 0.6136, 0.6715, 0.6270,\n",
       "                      0.1448, 0.7501, 0.6928, 0.6868, 0.6706, 0.7403, 0.6310, 0.7503, 0.7298,\n",
       "                      0.3170, 0.6953, 0.5787, 0.6844, 0.7002, 0.6661, 0.5547, 0.7038, 0.6889,\n",
       "                      0.6254, 0.6919, 0.2938, 0.7200, 0.6662, 0.7609, 0.7422, 0.6592, 0.7050,\n",
       "                      0.7078, 0.7422, 0.7031, 0.7309, 0.6735, 0.7232, 0.7478, 0.7305, 0.7329,\n",
       "                      0.6616, 0.6809, 0.6804, 0.6483, 0.7194, 0.6923, 0.7208, 0.6818, 0.5950,\n",
       "                      0.7272, 0.7207, 0.6479, 0.7015, 0.6521, 0.6696, 0.6881, 0.6473, 0.8335,\n",
       "                      0.7193, 0.5602, 0.6592, 0.7037, 1.2045, 0.6480, 0.6704, 0.6753, 0.6718,\n",
       "                      0.7264, 0.9823, 0.6897, 0.7135, 0.6423, 0.8170, 0.9719, 0.7579, 0.7105,\n",
       "                      0.7100, 0.6758, 0.6727, 0.6899, 0.7128, 0.6797, 0.6868, 0.8590, 0.9315,\n",
       "                      0.6553, 0.7294, 0.6784, 0.6053, 0.7382, 0.6980, 0.6985, 0.7024, 0.5324,\n",
       "                      0.7595, 0.7770, 0.7230, 0.6430, 0.7043, 0.3695, 0.7929, 0.6048, 0.6710,\n",
       "                      0.6206, 0.6588, 0.7344, 0.7649, 0.6254, 0.5486, 0.6682, 0.6760, 0.6388,\n",
       "                      0.7650, 0.6891, 0.8078, 0.7049, 0.5969, 0.6622, 0.7205, 0.6822, 0.6336,\n",
       "                      0.7468, 0.6731, 0.7196, 0.6765, 0.7176, 0.7318, 0.6611, 0.7644, 0.7849,\n",
       "                      0.0817, 0.6308, 0.7634, 0.6462, 0.8052, 0.7374, 0.7365, 0.6860, 0.6639,\n",
       "                      0.7866, 0.5982, 0.7173, 0.6882, 0.6697, 0.7624, 0.7695, 0.7114, 0.3763,\n",
       "                      0.6904, 0.7709, 0.7353, 0.7169, 0.1071, 0.6348, 0.7387, 0.7233, 0.7813,\n",
       "                      0.7386, 0.7805, 0.7325, 0.4789, 0.7472, 0.6421, 0.7656, 0.6650, 0.7168,\n",
       "                      0.6038, 0.7682, 0.6870, 0.6596, 0.6605, 0.7335, 0.7518, 0.7905, 0.7637,\n",
       "                      0.5972, 0.6836, 0.7573, 0.6723, 0.7080, 0.6281, 0.7935, 0.6749, 0.7267,\n",
       "                      0.3915, 0.7399, 0.6588, 0.6877, 0.6810, 0.6930, 0.8032, 0.7925, 0.6889,\n",
       "                      0.6260, 0.6569, 0.6653, 0.7364, 0.6881, 0.6417, 0.7201, 0.6359, 0.7399,\n",
       "                      0.7146, 0.6609, 0.6115, 0.7016, 0.7128, 0.6842, 0.6627, 0.7173, 0.4720,\n",
       "                      0.7641, 0.7861, 0.6854, 0.7503, 0.7215, 0.2102, 0.6646, 0.6563, 0.4361,\n",
       "                      0.7287, 0.7313, 0.7508, 0.7918, 0.8163, 0.7148, 0.6429, 0.6546, 0.6470,\n",
       "                      0.6388, 0.7532, 0.7121, 0.6867, 0.7131, 0.6339, 0.5815, 0.4809, 0.6603,\n",
       "                      0.5089, 0.4141, 0.6795, 0.5139, 0.2064, 0.7101, 0.7597, 0.7017, 0.7437,\n",
       "                      0.7076, 0.7980, 0.6839, 0.6619, 0.7688, 0.7399, 0.7639, 0.7465, 0.6623,\n",
       "                      0.7292, 0.6399, 0.6936, 0.7182, 0.6721, 0.7376, 0.7000, 0.7774, 0.6365,\n",
       "                      0.6908, 0.8431, 0.6954, 0.7843, 0.6681, 0.6851, 0.7305, 0.6956, 0.5439,\n",
       "                      0.5890, 0.6642, 0.7141, 0.7446, 0.7209, 0.6685, 0.7947, 0.8188, 0.7364,\n",
       "                      0.6626, 0.8265, 0.7643, 0.7656, 0.7021, 0.7620, 0.6772, 0.6804, 0.6772,\n",
       "                      0.7155, 0.6997, 0.7410, 0.2296, 0.7435, 0.7042, 0.7789, 0.7190, 0.3859,\n",
       "                      0.7264, 0.6576, 0.7675, 0.6573, 0.7272, 0.7324, 0.6295, 0.6853, 0.7492,\n",
       "                      0.7021, 0.6442, 0.7091, 0.6273, 0.6238, 0.6747, 0.7282, 0.6570, 0.7497,\n",
       "                      0.6995, 0.7270, 0.7335, 0.7793, 0.6631, 0.7512, 0.7504, 0.6766, 0.6634,\n",
       "                      0.7912, 0.7293, 0.4114, 0.6754, 0.6679, 0.8196, 0.6883, 0.7111, 0.6249,\n",
       "                      0.7220, 0.7038, 0.5914, 0.6891, 0.6007, 0.6048, 0.7265, 0.7116, 0.7051,\n",
       "                      0.6682, 0.6764, 0.6797], device='cuda:0')),\n",
       "             ('encoder.block.11.layer.0.SelfAttention.q.weight',\n",
       "              tensor([[ 0.0767,  0.0007,  0.1134,  ...,  0.0781, -0.0597, -0.0191],\n",
       "                      [-0.0215, -0.1479,  0.0152,  ..., -0.0520, -0.0463,  0.0150],\n",
       "                      [-0.1082, -0.0671, -0.0128,  ..., -0.1033,  0.1492, -0.0023],\n",
       "                      ...,\n",
       "                      [ 0.0696, -0.0964,  0.0246,  ...,  0.0237, -0.2111, -0.0250],\n",
       "                      [-0.0059,  0.0500, -0.0548,  ...,  0.0993,  0.0551, -0.0094],\n",
       "                      [ 0.0462,  0.0047,  0.0393,  ..., -0.0117,  0.0329,  0.0789]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.11.layer.0.SelfAttention.k.weight',\n",
       "              tensor([[-0.4525, -0.3282,  0.6122,  ..., -0.4845,  0.5211,  0.0035],\n",
       "                      [-0.5165, -0.5532,  0.2800,  ..., -0.7497, -0.3069, -0.0130],\n",
       "                      [-0.7320,  0.5379, -0.4493,  ..., -0.3056, -0.2486, -1.0377],\n",
       "                      ...,\n",
       "                      [ 0.5242, -0.5322, -0.0237,  ...,  0.7308, -1.0735,  0.1870],\n",
       "                      [ 0.2070,  0.1741, -0.2529,  ...,  0.4240, -0.7314,  0.0575],\n",
       "                      [-0.1057,  0.2167,  0.0576,  ...,  0.2479,  0.1796, -0.6025]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.11.layer.0.SelfAttention.v.weight',\n",
       "              tensor([[ 0.7637,  0.7697,  0.5772,  ...,  2.5785,  1.2646,  0.4689],\n",
       "                      [-0.4442, -5.1158,  1.2289,  ...,  2.5320, -1.5206, -1.7396],\n",
       "                      [ 2.8915,  1.1531, -2.9916,  ..., -1.1199, -1.7817,  1.1309],\n",
       "                      ...,\n",
       "                      [ 0.1389, -2.0005, -0.4073,  ...,  2.5251,  0.3160, -1.6087],\n",
       "                      [ 0.4665,  3.3509,  4.1350,  ..., -1.5757, -2.9061, -0.6027],\n",
       "                      [ 1.0908, -0.2337,  0.9785,  ...,  1.1667, -1.8185,  2.8445]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.11.layer.0.SelfAttention.o.weight',\n",
       "              tensor([[-3.1992e+00,  1.8509e+00, -3.5136e+00,  ..., -1.4059e+00,\n",
       "                       -1.4666e+00, -1.2948e-01],\n",
       "                      [ 1.2243e+00,  9.3953e+00, -3.1520e+00,  ...,  4.9624e+00,\n",
       "                       -1.5862e-01, -3.1167e+00],\n",
       "                      [-3.0598e+00, -2.9485e+00,  7.0328e+00,  ...,  8.4690e-01,\n",
       "                       -4.8654e+00,  2.2935e-01],\n",
       "                      ...,\n",
       "                      [-5.2429e+00, -8.9020e-01,  1.9764e-01,  ...,  4.1137e+00,\n",
       "                       -9.8378e-03,  3.4256e+00],\n",
       "                      [-7.2194e-01,  3.2314e+00, -2.2946e-01,  ...,  1.7059e+00,\n",
       "                        3.7546e+00,  5.5545e+00],\n",
       "                      [ 4.2715e+00,  1.3672e+00,  2.5136e+00,  ...,  6.1746e-03,\n",
       "                        5.1079e+00,  1.5708e+00]], device='cuda:0')),\n",
       "             ('encoder.block.11.layer.0.layer_norm.weight',\n",
       "              tensor([ 7.4341e-02,  8.9435e-02,  6.6053e-02,  7.9160e-02,  7.5653e-02,\n",
       "                       7.2422e-02,  1.7632e-02,  9.2023e-02,  3.1423e-02,  8.4086e-02,\n",
       "                       8.5400e-02,  8.6843e-02,  6.1749e-02,  6.5987e-02,  6.6614e-02,\n",
       "                       7.6752e-02,  8.0863e-02,  7.2350e-02,  5.9873e-02,  6.7242e-02,\n",
       "                       6.5325e-02,  7.3754e-02,  2.3170e-02,  6.7791e-02,  7.4968e-02,\n",
       "                       7.5469e-02,  7.0487e-02,  7.0039e-02,  7.9506e-02,  7.5374e-02,\n",
       "                       6.9959e-02,  7.7758e-02,  7.6069e-02,  6.5788e-02,  7.4224e-02,\n",
       "                       7.6455e-02,  4.7882e-02,  6.9450e-02,  7.2959e-02,  4.7542e-02,\n",
       "                       7.0295e-02,  7.1686e-02,  8.1606e-02,  8.3185e-02, -3.0209e-02,\n",
       "                       7.0928e-02,  6.8822e-02,  6.9109e-02,  8.1735e-02,  2.9625e-02,\n",
       "                       7.8325e-02,  6.8849e-02,  5.9505e-02,  7.2502e-02, -7.5813e-04,\n",
       "                       6.7861e-02,  5.9274e-02,  7.6586e-02,  8.7327e-02,  7.8571e-02,\n",
       "                       8.5817e-02,  6.8805e-02,  7.0827e-02,  7.4725e-02,  7.4029e-02,\n",
       "                       1.4277e-02,  7.3796e-02,  7.2937e-02,  6.6786e-02,  6.8786e-02,\n",
       "                       6.9974e-02,  6.9095e-02,  7.5235e-02,  7.5660e-02,  7.1406e-02,\n",
       "                       6.9351e-02,  6.3399e-02,  7.4541e-02,  6.9582e-02,  8.8996e-02,\n",
       "                       6.9224e-02,  7.1336e-02,  7.4267e-02,  7.9509e-02,  8.4523e-02,\n",
       "                       8.0457e-02,  7.4030e-02,  7.9822e-02,  7.7524e-02,  7.9080e-02,\n",
       "                       7.8732e-02,  7.0449e-02,  6.5926e-02,  7.4070e-02,  6.8023e-02,\n",
       "                       6.2335e-02,  6.9634e-02,  7.0383e-02,  8.1596e-02,  5.7121e-02,\n",
       "                       8.7097e-02,  7.1737e-02,  6.3746e-02,  7.2515e-02,  7.8748e-02,\n",
       "                       6.1560e-02,  7.8131e-02,  6.6576e-02,  7.4225e-02,  3.1233e-02,\n",
       "                       6.5158e-02,  7.5268e-02,  7.4482e-02,  6.1115e-02,  8.1750e-02,\n",
       "                       7.5618e-02,  7.3603e-02,  6.5582e-02,  6.6389e-02,  7.4006e-02,\n",
       "                       6.5661e-02,  7.6892e-02,  7.3631e-02,  7.0139e-02,  6.1448e-02,\n",
       "                       7.8551e-02,  8.1521e-02,  7.1630e-02,  7.3781e-02,  7.8856e-02,\n",
       "                       3.7738e-02,  7.9018e-02,  7.6165e-02,  6.5267e-02,  8.5389e-02,\n",
       "                       6.7980e-02,  7.3246e-02,  7.5077e-02,  7.6103e-02,  8.2747e-02,\n",
       "                       6.6960e-02,  8.8777e-02,  7.1216e-02,  6.6127e-02,  7.3730e-02,\n",
       "                       8.0934e-02,  6.5270e-02,  7.9764e-02,  7.0340e-02,  7.6046e-02,\n",
       "                       6.9853e-02,  7.7704e-02,  8.6867e-02,  7.7875e-02,  6.9454e-02,\n",
       "                       8.6298e-02,  7.1861e-02,  7.9738e-02,  6.5327e-02,  6.8524e-02,\n",
       "                       6.2671e-02,  6.9118e-02,  6.6506e-02, -3.9736e-04,  6.9896e-02,\n",
       "                       6.5576e-02,  7.2537e-02,  7.2127e-02,  7.9519e-02,  4.2486e-02,\n",
       "                       8.9276e-02,  8.7189e-02,  8.5189e-02,  5.6677e-02,  8.7577e-02,\n",
       "                       8.6979e-02,  8.2918e-02,  3.0811e-02,  7.1838e-02, -8.0285e-03,\n",
       "                       7.0532e-02,  7.2055e-02,  6.9877e-02,  7.7200e-02,  7.4147e-02,\n",
       "                       7.3147e-02,  7.0677e-02,  7.9721e-02,  6.8092e-02,  7.5401e-02,\n",
       "                       7.3181e-02,  8.0808e-02,  6.5589e-02,  7.8802e-02,  6.7032e-02,\n",
       "                       2.7082e-02,  8.0701e-02,  7.2817e-02,  7.3484e-02,  6.5235e-02,\n",
       "                       6.8835e-02,  8.0543e-02,  6.6847e-02,  7.4848e-02,  8.1147e-02,\n",
       "                       8.2806e-02,  6.6211e-02,  7.2237e-02,  7.6720e-02,  7.3412e-02,\n",
       "                       7.8682e-02,  6.9274e-02,  7.5186e-02,  7.2054e-02,  7.2340e-02,\n",
       "                       7.4607e-02,  7.2393e-02,  7.2424e-02,  7.0779e-02,  2.8697e-02,\n",
       "                       4.4355e-02,  8.3498e-02,  7.0943e-02,  7.4908e-02,  7.1241e-02,\n",
       "                       7.6265e-02,  8.0947e-02,  4.4293e-02,  7.0266e-02,  7.6814e-02,\n",
       "                       7.3723e-02,  6.8867e-02,  6.1452e-02,  7.2460e-02,  7.2896e-02,\n",
       "                       7.4104e-02,  6.9847e-02,  6.6993e-02,  6.4717e-02,  4.1594e-02,\n",
       "                       6.3223e-02,  6.7968e-02,  6.6260e-02,  7.6736e-02,  7.4187e-02,\n",
       "                       7.5086e-02,  6.6698e-02,  7.1534e-02,  7.2947e-02,  6.8441e-02,\n",
       "                       3.4743e-02,  5.7192e-02,  6.8472e-02,  7.2012e-02,  6.9151e-02,\n",
       "                       7.0580e-02,  6.8395e-02,  7.8893e-02,  3.3779e-02,  7.0495e-02,\n",
       "                       8.1139e-02,  6.2728e-02,  7.2610e-02,  6.5394e-02,  8.0063e-02,\n",
       "                       8.2360e-02,  7.4033e-02,  7.1626e-02,  7.1438e-02,  7.0141e-02,\n",
       "                       5.2854e-02,  8.1766e-02,  5.9038e-02, -6.6203e-05,  7.7340e-02,\n",
       "                       7.8169e-02,  7.2820e-02,  7.4637e-02,  7.8287e-02,  7.4316e-02,\n",
       "                       7.9876e-02,  7.9594e-02,  6.4027e-02,  7.2070e-02,  8.4952e-02,\n",
       "                       5.9933e-02,  7.2999e-02,  6.6883e-02,  7.7668e-02,  4.5342e-02,\n",
       "                       7.8610e-02,  6.3779e-02,  7.2702e-02,  7.1775e-02,  7.1898e-02,\n",
       "                       6.9538e-02,  8.2059e-02,  7.4910e-02,  6.3343e-02,  6.7441e-02,\n",
       "                       7.7196e-02,  6.6261e-02,  8.6986e-02,  7.6427e-02,  7.8169e-02,\n",
       "                       7.5017e-02,  8.1545e-02,  6.8703e-02,  8.4339e-02,  7.6780e-02,\n",
       "                       7.9704e-02,  9.0631e-02,  7.5583e-02,  6.4764e-02,  7.5425e-02,\n",
       "                       7.5441e-02,  6.1139e-02,  8.5078e-02,  7.3372e-02,  7.2739e-02,\n",
       "                       6.6907e-02,  8.0144e-02,  7.3088e-02,  6.6737e-02,  7.4955e-02,\n",
       "                       7.3890e-02,  8.7113e-02,  7.9164e-02,  7.6662e-02,  7.7184e-02,\n",
       "                       7.1412e-02,  5.8124e-02,  6.5576e-02,  7.3910e-02,  7.4539e-02,\n",
       "                       7.0592e-02,  7.7908e-02,  7.0099e-02,  7.5548e-02,  8.4669e-02,\n",
       "                       8.6269e-02,  8.2727e-02,  7.3318e-02,  6.9204e-02,  6.7345e-02,\n",
       "                       7.3151e-02,  8.2393e-02,  7.7308e-02,  7.1789e-02,  3.7838e-02,\n",
       "                       7.7852e-02,  6.8285e-02,  7.2469e-02,  7.3437e-02,  7.0814e-02,\n",
       "                       7.2406e-02,  7.7578e-02,  7.2449e-02,  7.8453e-02,  6.9723e-02,\n",
       "                       7.0499e-02,  8.1421e-02,  7.9363e-02,  7.1979e-02,  7.7493e-02,\n",
       "                       6.8471e-02,  4.0764e-02,  6.5536e-02,  6.9514e-02,  7.2642e-02,\n",
       "                       6.8496e-02,  8.0026e-02,  3.1499e-02,  7.2091e-02,  8.3731e-02,\n",
       "                       7.4576e-02,  5.6581e-02,  7.5795e-02,  3.4207e-02,  6.8813e-02,\n",
       "                       7.1401e-02,  6.9623e-02,  8.1005e-02,  7.1868e-02,  7.3657e-02,\n",
       "                       4.2787e-02,  7.5683e-02,  5.2751e-02,  8.3348e-02,  7.4315e-02,\n",
       "                       6.4312e-02,  4.0579e-03,  8.3313e-02,  8.1528e-02,  8.2879e-02,\n",
       "                       7.5237e-02,  8.2422e-02,  6.7261e-02,  7.0335e-02,  7.8944e-02,\n",
       "                       8.2258e-02,  2.6766e-02,  7.2533e-02,  8.7372e-02,  7.1032e-02,\n",
       "                       7.5334e-02,  7.7341e-02,  7.5509e-02,  6.7853e-02,  7.7324e-02,\n",
       "                       7.3326e-02,  7.0505e-02,  6.3470e-02,  7.4600e-02,  7.5490e-02,\n",
       "                       7.4175e-02,  6.9150e-02,  7.3686e-02,  3.7716e-02,  7.3569e-02,\n",
       "                       7.1778e-02,  7.7039e-02,  6.3225e-02,  3.5864e-02,  7.3745e-02,\n",
       "                       7.4368e-02,  6.7243e-02,  7.1416e-02,  7.1946e-02,  7.6510e-02,\n",
       "                       7.7162e-02,  7.8778e-02,  3.5131e-02,  7.5180e-02,  5.9412e-02,\n",
       "                       6.8693e-02,  7.0828e-02,  7.5500e-02,  7.3836e-02,  7.8444e-02,\n",
       "                       7.9464e-02,  6.1865e-02,  7.4580e-02,  3.7000e-02,  7.6616e-02,\n",
       "                       8.0017e-02,  7.2894e-02,  7.7286e-02,  7.7137e-02,  7.0869e-02,\n",
       "                       6.2812e-02,  7.2418e-02,  7.7509e-02,  7.7137e-02,  7.4988e-02,\n",
       "                       8.0336e-02,  8.1750e-02,  8.5365e-02,  8.6951e-02,  7.4083e-02,\n",
       "                       7.5492e-02,  6.4259e-02,  6.9123e-02,  7.7005e-02,  7.7095e-02,\n",
       "                       7.5726e-02,  6.2732e-02,  8.4612e-02,  7.1799e-02,  7.7893e-02,\n",
       "                       7.9187e-02,  8.2691e-02,  6.8750e-02,  6.7923e-02,  8.3487e-02,\n",
       "                       8.0680e-02,  7.8589e-02,  7.5645e-02,  3.1175e-02,  8.3445e-02,\n",
       "                       7.1292e-02,  3.9015e-02,  6.9593e-02,  7.1283e-02,  8.3781e-02,\n",
       "                       6.7781e-02,  6.9688e-02,  6.6215e-02,  6.5879e-02,  7.7267e-02,\n",
       "                       8.1304e-02,  6.6351e-02,  3.6438e-02,  7.3351e-02,  6.4102e-02,\n",
       "                       7.4665e-02,  8.0537e-02,  6.5707e-02,  6.8452e-02,  6.9514e-02,\n",
       "                       7.2051e-02,  7.7245e-02,  6.6919e-02,  9.2613e-03,  6.7424e-02,\n",
       "                       7.8336e-02,  8.1390e-02,  7.4990e-02,  7.1714e-02,  6.4331e-02,\n",
       "                       7.7010e-02,  7.3737e-02,  3.5421e-02,  7.5296e-02,  7.3213e-02,\n",
       "                       7.4274e-02,  6.9605e-02,  6.1883e-02,  6.5384e-02,  8.2046e-02,\n",
       "                       5.9363e-02,  6.7433e-02,  6.5981e-02,  7.0573e-02,  7.1526e-02,\n",
       "                       3.1245e-02,  7.7494e-02,  6.4790e-02,  7.6889e-02,  7.8475e-02,\n",
       "                       7.8592e-02,  6.8665e-02,  7.2425e-02,  7.6058e-02,  7.9171e-02,\n",
       "                       7.1061e-02,  7.6928e-02,  7.1647e-02,  6.8921e-02,  6.6206e-02,\n",
       "                       7.1321e-02,  6.6710e-02,  7.4249e-02,  8.1037e-02,  6.7920e-02,\n",
       "                       7.3516e-02,  7.4525e-02,  7.7464e-02,  7.0472e-02,  2.8710e-02,\n",
       "                       6.4496e-02,  7.0831e-02,  6.5098e-02,  9.0845e-02,  7.4807e-02,\n",
       "                       7.8142e-02,  7.3474e-02,  7.1738e-02,  7.6207e-02,  6.4497e-02,\n",
       "                       7.2366e-02,  5.6487e-02,  7.2966e-02,  6.8719e-02,  8.5053e-02,\n",
       "                       7.4136e-02,  1.0224e-02,  7.5309e-02,  7.3287e-02,  6.8138e-02,\n",
       "                       7.3655e-02,  5.1719e-02,  7.3853e-02,  7.6609e-02,  7.2687e-02,\n",
       "                       6.6123e-02,  8.0703e-02,  8.3543e-02,  7.1682e-02,  5.4419e-02,\n",
       "                       8.8632e-02,  7.5408e-02,  6.6013e-02,  7.1036e-02,  6.6880e-02,\n",
       "                       7.2479e-02,  6.5743e-02,  6.9252e-02,  7.6747e-02,  6.9514e-02,\n",
       "                       7.9684e-02,  8.2496e-02,  8.6420e-02,  7.1270e-02,  6.9870e-02,\n",
       "                       7.6084e-02,  7.4950e-02,  7.0163e-02,  7.2190e-02,  6.5446e-02,\n",
       "                       9.0702e-02,  7.8786e-02,  7.5304e-02,  6.2724e-02,  8.7023e-02,\n",
       "                       7.3675e-02,  7.6981e-02,  8.6654e-02,  7.1070e-02,  8.0054e-02,\n",
       "                       8.1346e-02,  7.5097e-02,  7.1265e-02,  7.2725e-02,  7.6010e-02,\n",
       "                       7.7989e-02,  6.5597e-02,  8.4836e-02,  7.1988e-02,  6.5844e-02,\n",
       "                       7.4969e-02,  6.9569e-02,  7.4356e-02,  9.0217e-02,  7.8245e-02,\n",
       "                       7.1515e-02,  7.1821e-02,  7.2757e-02,  7.4350e-02,  5.4884e-02,\n",
       "                       8.2386e-02,  7.4355e-02,  7.4669e-02,  7.8811e-02,  7.6291e-02,\n",
       "                       3.6580e-02,  7.2823e-02,  7.2867e-02,  3.8734e-02,  7.3818e-02,\n",
       "                       6.9041e-02,  8.2170e-02,  6.5187e-02,  6.3810e-02,  7.3091e-02,\n",
       "                       7.8347e-02,  7.3701e-02,  6.4323e-02,  6.9219e-02,  6.4247e-02,\n",
       "                       7.2541e-02,  7.4152e-02,  7.7467e-02,  7.7966e-02,  7.1578e-02,\n",
       "                       8.0029e-02,  7.8624e-02,  7.5641e-02,  7.5406e-02,  7.5709e-02,\n",
       "                       6.3745e-02,  1.3827e-02,  7.1977e-02,  8.2967e-02,  6.5216e-02,\n",
       "                       5.9549e-02,  7.0428e-02,  6.8777e-02,  8.4381e-02,  7.4549e-02,\n",
       "                       7.9216e-02,  5.4082e-02,  7.2401e-02,  7.0255e-02,  8.7289e-02,\n",
       "                       7.8459e-02,  7.0654e-02,  6.1903e-02,  6.5011e-02,  7.4278e-02,\n",
       "                       8.1746e-02,  7.9355e-02,  7.3043e-02,  6.3635e-02,  7.0511e-02,\n",
       "                       4.2375e-02,  6.4908e-02,  8.1130e-02,  7.5774e-02,  7.3817e-02,\n",
       "                       7.3478e-02,  7.0564e-02,  7.7551e-02,  7.0003e-02,  7.2549e-02,\n",
       "                       6.9448e-02,  8.1198e-02,  6.8918e-02,  7.5775e-02,  7.9851e-02,\n",
       "                       6.7751e-02,  6.6301e-02,  7.6551e-02,  8.0200e-02,  8.0003e-02,\n",
       "                       7.3519e-02,  6.9065e-02,  7.1736e-02,  7.8539e-02,  7.0814e-02,\n",
       "                       4.7014e-03,  7.3738e-02,  7.9002e-02,  7.4314e-02,  3.6801e-02,\n",
       "                       6.9833e-02,  6.7645e-02,  8.3125e-02,  6.8871e-02,  7.0974e-02,\n",
       "                       7.6378e-02,  6.0215e-02,  7.8080e-02,  6.3695e-02,  8.2331e-02,\n",
       "                       8.9131e-02,  7.9231e-02,  5.7799e-02,  7.5450e-02,  7.1856e-02,\n",
       "                       7.8986e-02,  7.7741e-02,  6.7429e-02,  9.2249e-02,  8.1168e-02,\n",
       "                       8.4755e-02,  7.1344e-02,  7.1724e-02,  8.1901e-02,  8.5379e-02,\n",
       "                       7.6140e-02,  7.9676e-02,  7.7568e-02,  7.9957e-02,  7.4585e-02,\n",
       "                       6.8160e-02,  7.3730e-02,  6.9635e-02,  7.4074e-02,  4.0994e-02,\n",
       "                       7.5628e-02,  7.4120e-02,  8.5156e-02,  7.0246e-02,  7.1313e-02,\n",
       "                       8.2990e-02,  7.5701e-02,  7.4679e-02,  8.5222e-02,  8.3264e-02,\n",
       "                       6.7991e-02,  8.5643e-02,  8.1503e-02,  5.7105e-02,  7.1740e-02,\n",
       "                       7.0811e-02,  7.1234e-02,  8.1776e-02], device='cuda:0')),\n",
       "             ('encoder.block.11.layer.1.DenseReluDense.wi.weight',\n",
       "              tensor([[ 0.5113, -0.6345,  1.5726,  ..., -0.6371, -0.1929, -0.3321],\n",
       "                      [-0.9572, -1.8787, -3.5604,  ..., -0.4436, -1.5668, -3.0176],\n",
       "                      [ 0.6742, -0.4250,  0.1284,  ...,  2.8121, -2.8182,  2.3631],\n",
       "                      ...,\n",
       "                      [ 0.4958,  0.9692,  2.5557,  ..., -1.8378, -1.8038,  0.6047],\n",
       "                      [ 3.6623, -1.9435, -0.7163,  ..., -0.0168,  0.2227, -3.8741],\n",
       "                      [-0.9910, -0.2511,  1.5890,  ...,  1.8769,  1.3801, -1.2840]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.11.layer.1.DenseReluDense.wo.weight',\n",
       "              tensor([[ 0.0163,  0.4670, -0.5193,  ..., -0.1592,  0.1176,  0.8452],\n",
       "                      [-0.9832, -0.1265, -1.9270,  ..., -0.7031, -0.3406,  0.4384],\n",
       "                      [ 0.4366,  0.7206,  0.7118,  ...,  0.5526,  0.9429,  1.3054],\n",
       "                      ...,\n",
       "                      [-0.2244,  0.7269, -0.0307,  ...,  0.4920,  0.4742,  0.2605],\n",
       "                      [ 0.0497, -0.7730,  0.5397,  ..., -0.2616,  0.1527,  0.1106],\n",
       "                      [ 0.1101, -0.1935,  0.4857,  ...,  1.0054, -0.4039, -0.2725]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.block.11.layer.1.layer_norm.weight',\n",
       "              tensor([0.4951, 0.4890, 0.4421, 0.4281, 0.5108, 0.4569, 0.0173, 0.4977, 0.2832,\n",
       "                      0.5383, 0.4659, 0.4554, 0.5297, 0.4424, 0.4732, 0.5174, 0.4603, 0.4564,\n",
       "                      0.2565, 0.4910, 0.4596, 0.4675, 1.4500, 0.4598, 0.4851, 0.4595, 0.4812,\n",
       "                      0.4101, 0.5450, 0.4865, 0.4754, 0.5483, 0.5048, 0.4511, 0.4521, 0.5092,\n",
       "                      0.4850, 0.4729, 0.4100, 0.3905, 0.4329, 0.4550, 0.3985, 0.4749, 0.3790,\n",
       "                      0.5393, 0.4361, 0.5125, 0.4763, 0.1747, 0.4333, 0.4828, 0.4997, 0.4949,\n",
       "                      0.5479, 0.4266, 0.5843, 0.5013, 0.5139, 0.5723, 0.5152, 0.4811, 0.5180,\n",
       "                      0.4737, 0.4828, 0.1840, 0.5525, 0.5145, 0.4816, 0.4537, 0.4765, 0.3408,\n",
       "                      0.4714, 0.4650, 0.5287, 0.4844, 0.5033, 0.5069, 0.4499, 0.4878, 0.5258,\n",
       "                      0.4777, 0.4591, 0.4671, 0.4986, 0.4786, 0.4078, 0.4555, 0.5169, 0.5489,\n",
       "                      0.5199, 0.4624, 0.4457, 0.4955, 0.5572, 0.5367, 0.4853, 0.5348, 0.4579,\n",
       "                      0.4641, 0.4976, 0.4309, 0.3781, 0.4224, 0.4232, 0.5523, 0.4954, 0.5071,\n",
       "                      0.4824, 0.6213, 0.4701, 0.4627, 0.4460, 0.4881, 0.5250, 0.4946, 0.4522,\n",
       "                      0.5182, 0.3450, 0.4596, 0.5102, 0.4922, 0.5536, 0.5095, 0.3523, 0.5023,\n",
       "                      0.4927, 0.4101, 0.4145, 0.5053, 0.2608, 0.5090, 0.4861, 0.5155, 0.4393,\n",
       "                      0.4083, 0.5107, 0.4525, 0.4860, 0.5071, 0.4491, 0.4150, 0.4505, 0.4695,\n",
       "                      0.5398, 0.5375, 0.4294, 0.4923, 0.4840, 0.4817, 0.4766, 0.4611, 0.4957,\n",
       "                      0.5185, 0.5203, 0.4472, 0.4605, 0.4814, 0.4673, 0.4373, 0.5668, 0.4833,\n",
       "                      0.4029, 1.4541, 0.5001, 0.5110, 0.4851, 0.4740, 0.4980, 0.3070, 0.4818,\n",
       "                      0.4944, 0.4751, 0.4026, 0.5205, 0.4493, 0.4246, 0.0788, 0.4171, 0.3616,\n",
       "                      0.4354, 0.4911, 0.5006, 0.5088, 0.4718, 0.4654, 0.4702, 0.4445, 0.4799,\n",
       "                      0.4564, 0.4960, 0.5022, 0.4824, 0.5013, 0.5017, 0.1331, 0.4754, 0.4858,\n",
       "                      0.5265, 0.5230, 0.4829, 0.4676, 0.4677, 0.5135, 0.5038, 0.4766, 0.5093,\n",
       "                      0.4500, 0.5064, 0.4757, 0.4585, 0.5206, 0.4054, 0.5538, 0.5036, 0.5001,\n",
       "                      0.4730, 0.4143, 0.4510, 0.3277, 0.3538, 0.4304, 0.4727, 0.3839, 0.4692,\n",
       "                      0.4662, 0.4736, 0.3510, 0.4843, 0.5602, 0.5098, 0.4569, 0.4400, 0.5520,\n",
       "                      0.5146, 0.4972, 0.5020, 0.4379, 0.4467, 0.2696, 0.4618, 0.4812, 0.5155,\n",
       "                      0.4666, 0.4431, 0.5135, 0.5102, 0.5187, 0.5455, 0.4737, 0.5269, 0.4900,\n",
       "                      0.4543, 0.5254, 0.4418, 0.4490, 0.4862, 0.4941, 0.1784, 0.4845, 0.4387,\n",
       "                      0.3451, 0.4678, 0.4464, 0.4593, 0.4577, 0.4787, 0.4419, 0.5092, 0.4593,\n",
       "                      0.2729, 0.4753, 0.4985, 0.3392, 0.4465, 0.4997, 0.5304, 0.3115, 0.5031,\n",
       "                      0.4700, 0.5750, 0.4765, 0.5040, 0.4547, 0.4781, 0.4130, 0.4925, 0.5398,\n",
       "                      0.4680, 0.6187, 0.5447, 0.5036, 0.4923, 0.4876, 0.4796, 0.6188, 0.4906,\n",
       "                      0.4916, 0.4876, 0.4843, 0.5013, 0.4647, 0.4492, 0.4545, 0.4838, 0.4807,\n",
       "                      0.4414, 0.4978, 0.4887, 0.4373, 0.5168, 0.5082, 0.4800, 0.4514, 0.4340,\n",
       "                      0.4974, 0.5188, 0.4842, 0.5179, 0.4907, 0.4351, 0.4393, 0.5039, 0.5075,\n",
       "                      0.4536, 0.5056, 0.4071, 0.4275, 0.4283, 0.4633, 0.5012, 0.4124, 0.5046,\n",
       "                      0.4713, 0.4874, 0.4479, 0.4862, 0.4660, 0.5215, 0.5679, 0.4960, 0.4912,\n",
       "                      0.4934, 0.5104, 0.4264, 0.4356, 0.3806, 0.4443, 0.3642, 0.6403, 0.3943,\n",
       "                      0.4542, 0.4794, 0.4314, 0.4500, 0.4803, 0.4441, 0.4635, 0.4832, 0.3302,\n",
       "                      0.5248, 0.4980, 0.4518, 0.5169, 0.5021, 0.4940, 0.2217, 0.5592, 0.4637,\n",
       "                      0.4783, 0.5421, 0.4458, 0.1538, 0.4528, 0.5534, 0.4573, 0.4737, 0.4800,\n",
       "                      0.3477, 0.4347, 0.4849, 0.4629, 0.4478, 0.5141, 0.4937, 0.3150, 0.4173,\n",
       "                      0.3229, 0.5158, 0.4904, 0.4560, 1.0200, 0.5189, 0.4531, 0.5236, 0.4912,\n",
       "                      0.4444, 0.4792, 0.4506, 0.4552, 0.5451, 0.4584, 0.4786, 0.4971, 0.4739,\n",
       "                      0.5202, 0.5182, 0.4897, 0.3838, 0.4979, 0.4468, 0.4691, 0.4562, 0.5025,\n",
       "                      0.4765, 0.5115, 0.4724, 0.5235, 0.1645, 0.5282, 0.3858, 0.4320, 0.4403,\n",
       "                      0.1109, 0.4712, 0.5155, 0.4843, 0.4717, 0.5128, 0.4594, 0.4330, 0.4886,\n",
       "                      0.2615, 0.5572, 0.3550, 0.5250, 0.4827, 0.4951, 0.3820, 0.4621, 0.5156,\n",
       "                      0.4145, 0.4551, 0.3445, 0.4549, 0.4523, 0.5080, 0.5550, 0.4223, 0.5106,\n",
       "                      0.4655, 0.5101, 0.5070, 0.4653, 0.4810, 0.5483, 0.4756, 0.5175, 0.5577,\n",
       "                      0.5576, 0.4615, 0.4910, 0.4385, 0.4317, 0.4821, 0.4925, 0.4652, 0.4817,\n",
       "                      0.5255, 0.4743, 0.5176, 0.5225, 0.5197, 0.4332, 0.4611, 0.5124, 0.4963,\n",
       "                      0.5177, 0.2338, 0.4846, 0.4473, 0.6244, 0.4985, 0.5348, 0.4982, 0.4639,\n",
       "                      0.4944, 0.6698, 0.4871, 0.4975, 0.4776, 0.5160, 0.5367, 0.4174, 0.4147,\n",
       "                      0.5246, 0.5082, 0.5229, 0.4732, 0.5195, 0.4697, 0.4226, 0.5132, 0.3913,\n",
       "                      0.4846, 0.5146, 0.4685, 0.4421, 0.5574, 0.5340, 0.4626, 0.4779, 0.3467,\n",
       "                      0.4725, 0.4695, 0.4746, 0.5000, 0.4736, 0.3376, 0.4265, 0.5093, 0.4209,\n",
       "                      0.4919, 0.5220, 0.4507, 0.3989, 0.4821, 0.3863, 0.5119, 0.4549, 0.4268,\n",
       "                      0.4866, 0.4981, 0.5044, 0.4728, 0.4720, 0.4827, 0.4942, 0.4629, 0.4265,\n",
       "                      0.4545, 0.4317, 0.4775, 0.4340, 0.4975, 0.5150, 0.4492, 0.5029, 0.4988,\n",
       "                      0.0862, 0.4201, 0.4694, 0.4899, 0.5810, 0.4957, 0.4884, 0.4134, 0.4543,\n",
       "                      0.4659, 0.4358, 0.5458, 0.5238, 0.4606, 0.4834, 0.5587, 0.4769, 0.4083,\n",
       "                      0.5274, 0.5034, 0.5556, 0.4412, 0.1487, 0.5326, 0.5045, 0.4910, 0.4947,\n",
       "                      0.4461, 0.4188, 0.4556, 0.3078, 0.4749, 0.5101, 0.4716, 0.5013, 0.4793,\n",
       "                      0.4833, 0.5171, 0.5126, 0.5232, 0.4889, 0.5122, 0.4944, 0.4434, 0.4934,\n",
       "                      0.4326, 0.4629, 0.4781, 0.5318, 0.4103, 0.4375, 0.4737, 0.4661, 0.4897,\n",
       "                      0.4652, 0.5225, 0.5026, 0.4994, 0.4292, 0.4627, 0.5062, 0.5054, 0.5054,\n",
       "                      0.4564, 0.4821, 0.4609, 0.4991, 0.4584, 0.4395, 0.4785, 0.4446, 0.4903,\n",
       "                      0.4819, 0.4818, 0.4180, 0.4824, 0.4691, 0.5111, 0.3937, 0.4433, 0.3396,\n",
       "                      0.5912, 0.5760, 0.4526, 0.4707, 0.5251, 0.2038, 0.5214, 0.5017, 0.3273,\n",
       "                      0.4825, 0.4398, 0.4661, 0.4847, 0.5154, 0.5017, 0.4588, 0.4528, 0.5123,\n",
       "                      0.4671, 0.4530, 0.5052, 0.4799, 0.4852, 0.4792, 0.4455, 0.3554, 0.4507,\n",
       "                      0.4386, 0.3812, 0.4571, 0.3036, 0.1556, 0.4507, 0.4572, 0.4106, 0.5028,\n",
       "                      0.5125, 0.4851, 0.4467, 0.5513, 0.5147, 0.5091, 0.4735, 0.5276, 0.5147,\n",
       "                      0.4812, 0.5299, 0.4775, 0.5179, 0.4470, 0.4579, 0.5247, 0.4735, 0.4559,\n",
       "                      0.4420, 0.5454, 0.5011, 0.4981, 0.5153, 0.4494, 0.4952, 0.4644, 0.3438,\n",
       "                      0.4380, 0.4811, 0.5043, 0.4589, 0.4948, 0.4868, 0.5059, 0.5427, 0.4222,\n",
       "                      0.5034, 0.5043, 0.5486, 0.5419, 0.4360, 0.4730, 0.4420, 0.4773, 0.3860,\n",
       "                      0.5390, 0.4612, 0.4881, 0.1727, 0.4449, 0.4955, 0.4457, 0.4564, 0.2970,\n",
       "                      0.5143, 0.4786, 0.5036, 0.4782, 0.5223, 0.4621, 0.4638, 0.4890, 0.4954,\n",
       "                      0.4558, 0.4374, 0.4902, 0.4507, 0.4737, 0.4912, 0.5020, 0.4964, 0.5009,\n",
       "                      0.4678, 0.5207, 0.5181, 0.5103, 0.4424, 0.4184, 0.4746, 0.4871, 0.4688,\n",
       "                      0.5272, 0.4743, 0.3422, 0.4491, 0.4889, 0.5705, 0.5163, 0.4696, 0.5169,\n",
       "                      0.4560, 0.4637, 0.4509, 0.4989, 0.4479, 0.5079, 0.4942, 0.4962, 0.4581,\n",
       "                      0.4614, 0.4604, 0.5314], device='cuda:0')),\n",
       "             ('encoder.final_layer_norm.weight',\n",
       "              tensor([0.2092, 0.2105, 0.1952, 0.1906, 0.2707, 0.2111, 0.0423, 0.2166, 0.1069,\n",
       "                      0.2387, 0.2099, 0.1827, 0.1983, 0.1902, 0.2469, 0.2169, 0.2062, 0.2250,\n",
       "                      0.1096, 0.2409, 0.2279, 0.2303, 0.0540, 0.2393, 0.2419, 0.2231, 0.2495,\n",
       "                      0.2057, 0.2195, 0.2570, 0.2320, 0.2221, 0.2217, 0.2712, 0.2014, 0.2282,\n",
       "                      0.1519, 0.1829, 0.2615, 0.1048, 0.2031, 0.1920, 0.1451, 0.2252, 0.1185,\n",
       "                      0.2294, 0.1719, 0.2305, 0.2149, 0.0625, 0.1896, 0.2252, 0.1898, 0.2515,\n",
       "                      0.0087, 0.2245, 0.0564, 0.2334, 0.2551, 0.2551, 0.2194, 0.2605, 0.2260,\n",
       "                      0.2465, 0.2489, 0.0397, 0.2413, 0.2352, 0.1789, 0.2461, 0.2427, 0.1470,\n",
       "                      0.2430, 0.2146, 0.2480, 0.2576, 0.1763, 0.2301, 0.2546, 0.2185, 0.2503,\n",
       "                      0.2288, 0.2363, 0.1994, 0.2232, 0.2395, 0.1863, 0.2221, 0.2242, 0.2596,\n",
       "                      0.2164, 0.2471, 0.2321, 0.2117, 0.2456, 0.2610, 0.2452, 0.2575, 0.2493,\n",
       "                      0.2243, 0.1893, 0.2275, 0.1292, 0.2506, 0.2277, 0.1432, 0.2362, 0.2458,\n",
       "                      0.2538, 0.1139, 0.2156, 0.2431, 0.2200, 0.2293, 0.2299, 0.2454, 0.2320,\n",
       "                      0.2362, 0.1598, 0.2513, 0.2606, 0.2503, 0.1836, 0.2422, 0.1382, 0.2399,\n",
       "                      0.2405, 0.2268, 0.2142, 0.1929, 0.0881, 0.2189, 0.2554, 0.2444, 0.2260,\n",
       "                      0.1658, 0.2264, 0.2387, 0.2111, 0.2097, 0.2348, 0.1725, 0.1729, 0.1912,\n",
       "                      0.2095, 0.2015, 0.2346, 0.2090, 0.2464, 0.2145, 0.2394, 0.2210, 0.2240,\n",
       "                      0.2005, 0.2537, 0.2674, 0.2124, 0.2640, 0.1820, 0.2147, 0.2551, 0.2270,\n",
       "                      0.2182, 0.0352, 0.2501, 0.2391, 0.2359, 0.1952, 0.2126, 0.1161, 0.1943,\n",
       "                      0.2357, 0.2345, 0.1641, 0.2387, 0.1907, 0.2294, 0.0592, 0.1859, 0.0027,\n",
       "                      0.2509, 0.2498, 0.2233, 0.2731, 0.2131, 0.1927, 0.2492, 0.1961, 0.2231,\n",
       "                      0.2206, 0.2362, 0.2578, 0.2483, 0.2359, 0.2520, 0.0518, 0.2368, 0.1891,\n",
       "                      0.2211, 0.2532, 0.2731, 0.2018, 0.2380, 0.2383, 0.2513, 0.2037, 0.2223,\n",
       "                      0.2314, 0.2367, 0.2354, 0.1979, 0.2287, 0.2150, 0.2194, 0.2366, 0.2090,\n",
       "                      0.2271, 0.2304, 0.2167, 0.1243, 0.1593, 0.2238, 0.2428, 0.2208, 0.2151,\n",
       "                      0.2296, 0.1880, 0.1201, 0.2339, 0.2222, 0.2191, 0.2134, 0.1450, 0.2576,\n",
       "                      0.2629, 0.2274, 0.1642, 0.2114, 0.1890, 0.1116, 0.1904, 0.2271, 0.2504,\n",
       "                      0.2307, 0.2475, 0.2405, 0.2352, 0.2222, 0.2589, 0.2288, 0.1194, 0.2464,\n",
       "                      0.2310, 0.2486, 0.2568, 0.2361, 0.2491, 0.2242, 0.0831, 0.2374, 0.1896,\n",
       "                      0.1206, 0.2389, 0.2462, 0.2280, 0.2353, 0.2273, 0.2358, 0.2343, 0.2350,\n",
       "                      0.1316, 0.1986, 0.1655, 0.0020, 0.1900, 0.2251, 0.2401, 0.1560, 0.2456,\n",
       "                      0.2455, 0.1969, 0.2028, 0.2493, 0.2115, 0.2199, 0.1813, 0.2654, 0.2439,\n",
       "                      0.1809, 0.1365, 0.2491, 0.2253, 0.2620, 0.2706, 0.2490, 0.2682, 0.2063,\n",
       "                      0.2432, 0.2417, 0.2132, 0.2075, 0.2411, 0.1983, 0.2104, 0.2222, 0.2496,\n",
       "                      0.2298, 0.2475, 0.2326, 0.2019, 0.2325, 0.2125, 0.2411, 0.1951, 0.2123,\n",
       "                      0.2304, 0.1886, 0.1968, 0.2180, 0.2419, 0.2637, 0.1825, 0.2148, 0.2556,\n",
       "                      0.1961, 0.2335, 0.1584, 0.2004, 0.2304, 0.2327, 0.2516, 0.1206, 0.2072,\n",
       "                      0.2487, 0.2374, 0.2423, 0.2539, 0.2496, 0.2257, 0.1903, 0.2218, 0.2247,\n",
       "                      0.2208, 0.2435, 0.2266, 0.2315, 0.1800, 0.2302, 0.1685, 0.1138, 0.1883,\n",
       "                      0.2466, 0.2418, 0.2404, 0.2005, 0.2170, 0.2297, 0.2057, 0.2546, 0.1314,\n",
       "                      0.2392, 0.1844, 0.1484, 0.2701, 0.2522, 0.2293, 0.0873, 0.2177, 0.2514,\n",
       "                      0.2320, 0.2223, 0.2059, 0.0463, 0.2266, 0.1784, 0.2524, 0.1287, 0.2464,\n",
       "                      0.1214, 0.1741, 0.2343, 0.2207, 0.1767, 0.2233, 0.2462, 0.1301, 0.2442,\n",
       "                      0.1244, 0.2692, 0.2063, 0.2356, 0.0164, 0.2190, 0.1966, 0.2124, 0.2397,\n",
       "                      0.2199, 0.2580, 0.2146, 0.2389, 0.2526, 0.0077, 0.2734, 0.2426, 0.2505,\n",
       "                      0.2396, 0.2176, 0.2374, 0.2467, 0.2495, 0.2462, 0.2539, 0.2119, 0.2329,\n",
       "                      0.2600, 0.2324, 0.2324, 0.2490, 0.0660, 0.2044, 0.1472, 0.2220, 0.2741,\n",
       "                      0.0620, 0.2261, 0.2447, 0.2189, 0.2540, 0.2255, 0.2126, 0.2069, 0.2551,\n",
       "                      0.1014, 0.2596, 0.1396, 0.2398, 0.2285, 0.2424, 0.1539, 0.2062, 0.2169,\n",
       "                      0.1814, 0.1991, 0.1330, 0.2531, 0.2188, 0.2540, 0.2293, 0.1904, 0.2291,\n",
       "                      0.2273, 0.2492, 0.2325, 0.2516, 0.2203, 0.2087, 0.2147, 0.2250, 0.2075,\n",
       "                      0.2473, 0.2551, 0.2582, 0.2532, 0.2192, 0.2206, 0.2220, 0.2298, 0.2425,\n",
       "                      0.2550, 0.2578, 0.2591, 0.2639, 0.2227, 0.2672, 0.2249, 0.2523, 0.2504,\n",
       "                      0.2491, 0.0467, 0.2156, 0.2057, 0.1079, 0.2324, 0.2411, 0.1903, 0.2471,\n",
       "                      0.2379, 0.1709, 0.2651, 0.2271, 0.2306, 0.2063, 0.0956, 0.2463, 0.2339,\n",
       "                      0.1834, 0.2129, 0.2383, 0.2494, 0.2560, 0.2470, 0.2089, 0.1895, 0.0405,\n",
       "                      0.2380, 0.2044, 0.2242, 0.1985, 0.2731, 0.2263, 0.2199, 0.2644, 0.1097,\n",
       "                      0.2488, 0.2147, 0.2636, 0.2217, 0.2249, 0.1333, 0.2012, 0.2707, 0.2097,\n",
       "                      0.2523, 0.2258, 0.2377, 0.1057, 0.2071, 0.1530, 0.2340, 0.2051, 0.1880,\n",
       "                      0.2413, 0.2256, 0.2207, 0.2261, 0.1927, 0.2126, 0.2368, 0.1969, 0.2241,\n",
       "                      0.2103, 0.2533, 0.2080, 0.1809, 0.2382, 0.2428, 0.2122, 0.2317, 0.2296,\n",
       "                      0.0530, 0.1702, 0.2083, 0.2507, 0.1931, 0.2543, 0.2344, 0.2121, 0.2359,\n",
       "                      0.2238, 0.1629, 0.2075, 0.2390, 0.1873, 0.1845, 0.2173, 0.2093, 0.0012,\n",
       "                      0.2262, 0.2152, 0.2467, 0.2157, 0.0923, 0.2576, 0.2415, 0.2139, 0.2200,\n",
       "                      0.2463, 0.1912, 0.2431, 0.1078, 0.2289, 0.2692, 0.2424, 0.2366, 0.2604,\n",
       "                      0.2587, 0.2552, 0.2435, 0.2567, 0.2574, 0.2296, 0.1988, 0.2509, 0.2179,\n",
       "                      0.2156, 0.2375, 0.2039, 0.2383, 0.1534, 0.2355, 0.2017, 0.2452, 0.2251,\n",
       "                      0.1539, 0.2204, 0.2596, 0.2503, 0.2052, 0.1765, 0.2269, 0.2217, 0.2573,\n",
       "                      0.2124, 0.2502, 0.2149, 0.1987, 0.2241, 0.2043, 0.2387, 0.2274, 0.2104,\n",
       "                      0.2447, 0.2394, 0.1811, 0.2202, 0.2297, 0.2305, 0.1711, 0.2286, 0.1423,\n",
       "                      0.2193, 0.2366, 0.2131, 0.1846, 0.2604, 0.1031, 0.2272, 0.2201, 0.1278,\n",
       "                      0.2341, 0.2043, 0.2118, 0.2390, 0.2314, 0.2010, 0.1938, 0.1870, 0.2535,\n",
       "                      0.2355, 0.2436, 0.2312, 0.2487, 0.2312, 0.2294, 0.1731, 0.1493, 0.2311,\n",
       "                      0.2009, 0.1710, 0.2054, 0.1200, 0.0352, 0.2343, 0.1959, 0.2270, 0.2014,\n",
       "                      0.2398, 0.2520, 0.2286, 0.2536, 0.2327, 0.1992, 0.2408, 0.2351, 0.2203,\n",
       "                      0.2481, 0.2318, 0.2389, 0.2333, 0.2347, 0.2322, 0.2421, 0.2196, 0.2566,\n",
       "                      0.2364, 0.1290, 0.2280, 0.2072, 0.2553, 0.2329, 0.2599, 0.1982, 0.1568,\n",
       "                      0.2492, 0.2430, 0.2251, 0.1915, 0.2568, 0.2342, 0.2236, 0.2124, 0.2518,\n",
       "                      0.2173, 0.2284, 0.2382, 0.2711, 0.2279, 0.1902, 0.1877, 0.2466, 0.0946,\n",
       "                      0.2154, 0.2281, 0.2434, 0.0702, 0.2238, 0.2057, 0.2055, 0.2306, 0.1107,\n",
       "                      0.2488, 0.1985, 0.2071, 0.2418, 0.2245, 0.2048, 0.2233, 0.2330, 0.2548,\n",
       "                      0.1988, 0.2199, 0.2354, 0.2341, 0.2295, 0.2390, 0.2374, 0.2329, 0.2165,\n",
       "                      0.2310, 0.2194, 0.2266, 0.1978, 0.1919, 0.2198, 0.2470, 0.1993, 0.2178,\n",
       "                      0.2530, 0.1974, 0.1331, 0.2257, 0.2337, 0.1865, 0.2200, 0.1932, 0.2379,\n",
       "                      0.2299, 0.2491, 0.1962, 0.2096, 0.2081, 0.2199, 0.2173, 0.2355, 0.2142,\n",
       "                      0.2120, 0.2394, 0.2068], device='cuda:0')),\n",
       "             ('classifier.dense.weight',\n",
       "              tensor([[-0.0294, -0.0061,  0.0287,  ..., -0.0174,  0.0181,  0.0088],\n",
       "                      [ 0.0081, -0.0055,  0.0253,  ...,  0.0184,  0.0036, -0.0282],\n",
       "                      [-0.0306,  0.0108, -0.0198,  ...,  0.0335, -0.0120, -0.0224],\n",
       "                      ...,\n",
       "                      [ 0.0162,  0.0097, -0.0048,  ..., -0.0242, -0.0050, -0.0196],\n",
       "                      [ 0.0227, -0.0164, -0.0241,  ...,  0.0163, -0.0322, -0.0026],\n",
       "                      [-0.0291,  0.0105, -0.0116,  ..., -0.0078,  0.0203,  0.0322]],\n",
       "                     device='cuda:0')),\n",
       "             ('classifier.dense.bias',\n",
       "              tensor([ 1.4245e-02, -1.0800e-02, -8.8113e-03, -2.6412e-02,  1.8688e-02,\n",
       "                       3.8719e-03,  2.4844e-02,  7.2357e-03,  4.1678e-03, -2.5318e-02,\n",
       "                       2.4328e-02, -3.2843e-02, -1.3491e-02,  2.5733e-02,  1.0513e-02,\n",
       "                       1.4628e-02, -5.4919e-03, -1.4722e-02,  3.1178e-02, -1.1491e-02,\n",
       "                      -2.5469e-02, -1.2718e-02,  1.8120e-02, -3.3862e-02,  1.8837e-02,\n",
       "                      -3.8577e-03, -1.1289e-02, -1.8135e-03,  2.0751e-02,  2.4156e-02,\n",
       "                       1.3114e-02, -1.0819e-02,  3.4357e-02, -1.9173e-02, -2.8934e-02,\n",
       "                      -3.6069e-02,  5.4281e-03, -2.1045e-02, -2.5387e-02, -9.9000e-03,\n",
       "                      -1.2632e-02, -2.1811e-02,  1.2513e-02,  1.2363e-02, -1.1622e-02,\n",
       "                       2.4779e-02, -2.8905e-02, -1.4726e-02,  1.7544e-02,  2.1571e-02,\n",
       "                      -8.5843e-03, -4.5284e-03,  8.6806e-03,  2.4258e-02, -5.3483e-03,\n",
       "                       4.0258e-03,  1.4300e-02,  2.9308e-02, -1.3704e-02, -1.4690e-03,\n",
       "                      -1.7996e-02, -2.4728e-02,  5.7407e-03,  2.6745e-02,  2.7005e-02,\n",
       "                       2.1889e-02, -2.2889e-02, -1.3528e-02, -3.3261e-02, -2.6937e-02,\n",
       "                      -3.5817e-02, -2.9618e-02, -3.3189e-02,  1.5659e-02,  2.9387e-02,\n",
       "                      -2.6053e-02,  1.2409e-02,  3.1455e-02,  2.0048e-02, -1.9712e-02,\n",
       "                       1.5069e-02, -1.3981e-02,  3.6602e-03,  9.7973e-03,  2.0358e-02,\n",
       "                       1.4630e-03, -1.5414e-02, -1.5382e-03,  1.2006e-02,  8.1155e-03,\n",
       "                      -2.6134e-02,  2.5369e-02,  2.7334e-02,  1.8843e-02, -1.9835e-03,\n",
       "                      -3.0104e-02,  1.5775e-02, -2.2384e-02,  2.4304e-02, -2.8648e-02,\n",
       "                      -2.8818e-03, -2.3172e-02, -3.7756e-03, -3.5555e-02, -1.3230e-03,\n",
       "                      -1.2661e-02, -3.3809e-02, -1.8447e-02, -3.5214e-02,  2.5770e-02,\n",
       "                       2.3652e-02, -2.7159e-04,  3.5259e-02, -1.5875e-02,  1.7057e-02,\n",
       "                       1.3551e-02,  1.4960e-02,  1.7758e-02,  2.0343e-02,  2.7419e-02,\n",
       "                       2.8949e-02,  3.1515e-02,  6.7114e-03, -1.9340e-02, -1.9643e-02,\n",
       "                      -2.9126e-02,  2.0823e-02, -1.8479e-02, -2.1343e-03, -5.7386e-03,\n",
       "                      -3.2412e-02, -1.7017e-02,  1.4132e-02,  2.4712e-02,  2.3650e-02,\n",
       "                       2.5370e-02, -3.6060e-02,  2.7442e-02,  2.8527e-02,  1.5277e-02,\n",
       "                      -2.8281e-02,  3.4371e-02, -1.9206e-02,  2.0881e-02,  9.5254e-04,\n",
       "                      -3.0978e-02,  1.0935e-02,  1.3033e-02,  2.6069e-02,  5.4140e-03,\n",
       "                       3.2572e-02, -3.5792e-02, -2.6330e-02,  1.5414e-02, -1.0148e-02,\n",
       "                       1.3518e-02, -1.9844e-02, -4.9193e-03, -3.9892e-03,  1.1349e-02,\n",
       "                      -3.2525e-02,  3.3411e-02,  3.3934e-02,  1.9902e-02, -2.3061e-02,\n",
       "                       5.0988e-03,  9.2278e-03,  1.2710e-02,  1.3723e-02,  9.4468e-03,\n",
       "                      -1.8182e-02,  8.1985e-03, -3.4688e-02,  1.7298e-02, -1.3533e-02,\n",
       "                       1.3792e-02,  1.5834e-02, -7.7677e-03,  2.6976e-02, -1.5179e-02,\n",
       "                       2.4554e-02,  7.7216e-03,  2.2522e-02, -8.2191e-04, -2.9446e-02,\n",
       "                       2.1235e-02, -2.8500e-02, -1.5043e-02,  3.2077e-02, -1.7496e-02,\n",
       "                       2.2356e-02,  4.9046e-03, -1.1133e-02, -2.6248e-02,  9.8948e-03,\n",
       "                      -1.8263e-02,  8.5484e-03,  9.2698e-03, -2.2229e-02,  2.6190e-02,\n",
       "                      -2.6372e-02, -4.5654e-04,  2.0794e-02, -2.0520e-02, -1.5181e-02,\n",
       "                      -1.5862e-02, -1.6577e-02,  2.7171e-02, -2.3143e-02,  3.5098e-02,\n",
       "                      -1.0534e-02,  2.2800e-02, -2.1620e-02,  1.6902e-02,  5.4386e-03,\n",
       "                      -1.2347e-02,  7.3945e-04,  1.1042e-02, -6.8142e-03,  2.4673e-02,\n",
       "                       2.0889e-02, -1.1670e-02,  3.3257e-02, -2.4925e-02,  3.1733e-02,\n",
       "                       6.6452e-03, -1.8893e-02,  1.4431e-02,  1.6094e-02,  3.2248e-02,\n",
       "                       3.3640e-02,  2.7090e-02, -3.8736e-03, -1.6967e-02,  2.4478e-02,\n",
       "                      -3.1781e-02,  2.7942e-02, -2.4093e-02, -6.4694e-03,  1.8727e-02,\n",
       "                       7.7943e-03, -2.4450e-02,  1.5485e-02,  1.4297e-02,  1.4378e-02,\n",
       "                       1.1868e-02,  1.7512e-02,  3.1923e-02,  2.3700e-02, -2.3887e-02,\n",
       "                      -1.3309e-02, -3.4372e-02, -2.6739e-02,  3.2734e-02,  2.7279e-02,\n",
       "                      -1.0809e-02, -5.1623e-03,  9.2264e-03,  2.4321e-03, -3.3029e-02,\n",
       "                       3.0069e-02, -2.1573e-03,  1.9645e-03, -5.3767e-03, -3.1166e-02,\n",
       "                       1.0051e-03,  2.4556e-02, -5.4860e-03,  2.4957e-02,  2.9079e-02,\n",
       "                      -5.3613e-03, -1.7689e-02, -1.2029e-02,  3.2012e-02,  3.3818e-02,\n",
       "                      -2.2952e-02, -2.1432e-02, -1.7755e-02,  2.3386e-02,  8.6796e-03,\n",
       "                      -6.6355e-03,  3.1107e-02,  3.2561e-02,  1.3469e-02, -1.6789e-02,\n",
       "                      -2.5386e-02,  1.3706e-03, -1.7415e-02,  3.1274e-02, -3.5367e-02,\n",
       "                      -1.7346e-02, -5.7572e-03, -3.0980e-02,  6.0389e-04,  1.9045e-02,\n",
       "                      -2.5916e-02, -1.0595e-02,  2.4269e-02,  8.3795e-03,  3.1438e-02,\n",
       "                      -1.8774e-02, -5.5314e-03,  1.6360e-02, -1.0592e-02, -1.9955e-02,\n",
       "                       1.5635e-02, -1.8798e-02,  1.2443e-02,  2.7298e-02, -1.1251e-02,\n",
       "                       3.2633e-02, -2.6755e-02,  9.6517e-03, -5.1102e-03,  6.1162e-03,\n",
       "                       3.3238e-02, -1.6439e-02,  1.7025e-03,  2.8239e-02, -1.2866e-03,\n",
       "                      -1.7351e-02, -1.2160e-02,  1.2381e-02,  1.5246e-02,  2.3675e-02,\n",
       "                       6.0468e-03, -1.9297e-04, -2.8762e-02, -1.3128e-02, -1.8220e-02,\n",
       "                       1.6410e-02, -2.9474e-02,  9.7097e-03, -1.7025e-02,  3.1112e-02,\n",
       "                       1.4778e-02,  1.6752e-02, -3.1336e-02, -2.0032e-02,  1.6662e-02,\n",
       "                      -2.9680e-02,  1.8395e-02,  5.7822e-03,  3.9252e-03, -6.1316e-03,\n",
       "                      -2.3808e-02,  7.6176e-03, -1.7302e-02, -1.9764e-02,  2.8553e-03,\n",
       "                       3.5931e-02, -2.7199e-02,  1.8557e-02, -1.7393e-02, -3.2819e-02,\n",
       "                       2.1482e-02,  7.0879e-03,  8.4378e-03, -1.7720e-02,  1.1006e-02,\n",
       "                      -2.4199e-02,  2.1608e-02, -1.9328e-02,  2.0353e-02, -3.3556e-02,\n",
       "                       7.4051e-03, -4.5342e-03, -2.0094e-02, -1.4074e-02,  2.0814e-02,\n",
       "                      -2.7562e-02,  3.0692e-02,  2.9606e-02,  3.2230e-02, -5.5604e-03,\n",
       "                       2.6578e-02, -3.2622e-02, -2.3368e-02,  1.5369e-02, -2.3331e-02,\n",
       "                       1.8180e-02,  9.1466e-04,  2.9135e-02,  2.9727e-02, -2.1908e-02,\n",
       "                       1.9762e-02,  2.1239e-02, -2.2108e-02, -1.3188e-02,  1.6484e-03,\n",
       "                       1.5286e-02, -4.0412e-03,  2.4745e-02, -2.5465e-02,  3.9115e-03,\n",
       "                      -9.8753e-03,  9.4730e-03, -2.3069e-02, -1.9809e-02, -2.0127e-02,\n",
       "                      -3.5667e-02, -3.3881e-02,  1.2555e-02, -1.5383e-02, -1.3452e-02,\n",
       "                      -2.4244e-02, -2.7009e-02,  3.0774e-02,  2.6562e-02,  3.1321e-02,\n",
       "                      -3.4282e-02,  1.2017e-02,  2.6011e-02,  3.2839e-02, -3.0715e-02,\n",
       "                      -5.0474e-03, -7.7824e-03, -2.6583e-02, -1.9354e-02, -3.1411e-02,\n",
       "                       2.6131e-02, -9.9192e-03, -1.3709e-02, -1.7574e-02, -2.5844e-02,\n",
       "                       1.6131e-02,  3.3924e-02, -3.3891e-02, -4.5666e-03, -3.1609e-02,\n",
       "                      -8.3935e-03,  1.5221e-02, -1.6889e-02,  1.4199e-02,  2.2354e-03,\n",
       "                      -3.1865e-02, -3.3249e-02, -3.5282e-02,  3.4922e-02, -3.4044e-02,\n",
       "                       1.6328e-02,  3.5805e-02, -2.3211e-02,  1.9054e-02, -3.4086e-02,\n",
       "                       1.6115e-03, -3.5864e-02,  2.4899e-02, -2.4489e-03, -5.8451e-03,\n",
       "                      -7.3161e-03, -2.6588e-02, -6.2590e-03,  3.2616e-02,  4.2458e-03,\n",
       "                      -4.5594e-03, -6.1558e-03,  1.7341e-02, -1.3801e-02,  3.2741e-02,\n",
       "                       1.7440e-02,  7.4554e-03, -1.4571e-02,  1.3542e-02, -2.1862e-02,\n",
       "                       3.2282e-02, -1.6991e-03,  1.0328e-02, -3.4260e-02, -6.0636e-03,\n",
       "                      -3.5355e-02,  2.5878e-02, -1.1565e-02, -2.2132e-02, -1.2286e-02,\n",
       "                       6.5690e-03,  1.3895e-02, -3.5130e-02, -2.5023e-02,  2.0448e-02,\n",
       "                       2.8300e-02,  2.3096e-02, -2.3894e-02,  3.4957e-02, -6.1395e-03,\n",
       "                      -3.2741e-02, -3.2987e-02, -7.6573e-03,  6.8864e-03,  3.0499e-02,\n",
       "                       8.3018e-03, -2.9482e-02,  3.5495e-02,  9.2903e-03,  1.9464e-02,\n",
       "                      -6.8987e-03,  2.7001e-02,  9.8220e-03,  4.7669e-03, -1.7176e-02,\n",
       "                       5.0340e-04,  4.3069e-03,  7.1411e-03,  3.4367e-02, -4.3251e-03,\n",
       "                      -3.3121e-02, -2.4145e-02, -3.4847e-02,  7.5983e-03,  9.2695e-03,\n",
       "                      -3.3280e-02,  1.1203e-02, -3.3159e-02,  2.4578e-03, -2.0611e-02,\n",
       "                      -2.0776e-02,  2.1594e-02, -8.9692e-03,  2.6109e-02, -3.1533e-02,\n",
       "                      -1.7741e-02,  2.1178e-03, -3.6004e-02, -1.0751e-02,  2.0393e-02,\n",
       "                      -1.2308e-02, -3.0165e-02, -2.9258e-02, -3.4217e-02,  8.9536e-03,\n",
       "                      -1.5550e-03, -6.4439e-03, -5.3643e-03,  1.8034e-02, -1.3097e-02,\n",
       "                      -1.7654e-02, -6.8523e-03, -6.7132e-03,  2.8975e-02, -1.5344e-02,\n",
       "                       9.0899e-03, -1.6351e-02, -2.3377e-02,  5.4676e-03, -3.2940e-02,\n",
       "                       1.6704e-02, -2.2764e-02, -3.2314e-02, -3.0219e-02,  1.2448e-02,\n",
       "                      -6.3154e-03, -1.1866e-02,  8.4167e-03, -2.3726e-02, -3.2551e-02,\n",
       "                       1.7258e-02,  1.6504e-02,  1.6245e-02, -4.4385e-03, -2.9234e-02,\n",
       "                       2.6139e-03,  1.4017e-02, -1.2335e-03,  9.4718e-03,  3.5562e-02,\n",
       "                       1.6879e-02,  2.2585e-02,  2.7462e-02,  2.9953e-02,  1.9022e-03,\n",
       "                      -1.6742e-02, -3.2390e-02,  3.5590e-02, -4.1414e-04,  2.8418e-02,\n",
       "                      -2.1822e-02, -2.0404e-02, -2.6281e-02, -3.0590e-02,  2.7823e-02,\n",
       "                       4.9408e-03,  3.4317e-03, -1.3334e-02, -5.0006e-03,  1.5362e-02,\n",
       "                       3.4999e-02, -1.6405e-02, -1.5796e-02,  3.4683e-02, -2.8543e-02,\n",
       "                      -2.4109e-02,  2.2481e-02,  2.2294e-03, -3.0006e-02,  2.5488e-02,\n",
       "                      -3.3136e-02, -2.9804e-02,  3.2804e-02, -3.3247e-02, -2.8855e-02,\n",
       "                       1.4078e-02,  9.9334e-03, -1.5841e-02,  4.7650e-03, -3.5792e-02,\n",
       "                       1.9347e-02,  1.8622e-03, -3.1130e-02, -3.3424e-02,  1.6924e-02,\n",
       "                      -1.4469e-02,  2.4354e-02,  3.5717e-02,  2.0936e-02,  2.4246e-02,\n",
       "                      -3.3981e-02,  2.6295e-02,  1.5823e-02, -5.2656e-03,  2.1092e-02,\n",
       "                       2.7283e-02,  9.4534e-03,  2.4588e-02,  7.0055e-03, -6.0687e-05,\n",
       "                       3.1830e-02, -2.8377e-03, -2.6942e-02, -1.9727e-02,  3.0773e-02,\n",
       "                       1.4288e-02,  9.1136e-04, -1.6032e-02,  2.0875e-02,  7.2566e-03,\n",
       "                       2.3987e-03, -2.2888e-02, -2.9366e-02,  9.5880e-03, -1.9827e-02,\n",
       "                       3.2023e-02, -1.6669e-02,  3.2948e-03,  4.8124e-03, -8.3892e-03,\n",
       "                      -3.1319e-02, -3.3668e-03,  2.1547e-02, -1.8461e-02,  4.8598e-03,\n",
       "                      -6.6799e-03,  5.0189e-03, -2.8344e-02, -6.1231e-03, -7.5424e-03,\n",
       "                       1.3462e-02,  3.3862e-02, -2.1390e-02,  2.1772e-02, -2.8873e-02,\n",
       "                      -2.2273e-02, -2.8765e-02, -2.4956e-02, -1.2540e-02, -3.4272e-02,\n",
       "                      -2.0485e-03,  3.4092e-02, -2.9855e-03, -3.3507e-02, -1.8816e-02,\n",
       "                      -2.2433e-02, -7.6841e-03, -1.4874e-02, -4.0738e-03,  3.7979e-03,\n",
       "                      -3.0618e-02,  5.5687e-03,  3.3856e-02,  2.1670e-02, -1.8955e-02,\n",
       "                       3.3161e-02, -2.2774e-02,  9.0015e-03,  2.3434e-02, -4.5726e-03,\n",
       "                       1.6895e-04,  2.2265e-02,  1.4813e-03, -4.6787e-03,  2.4937e-04,\n",
       "                      -2.2973e-02,  1.6665e-02, -1.3795e-02,  1.2993e-03, -3.0113e-02,\n",
       "                      -3.2682e-02, -2.8245e-03, -4.3407e-03, -2.7115e-02,  1.9698e-02,\n",
       "                      -2.0630e-02, -2.0096e-02,  2.6754e-02,  1.4251e-02,  3.5184e-02,\n",
       "                      -6.6028e-03, -2.2523e-02,  1.5797e-02,  5.0689e-03,  8.7856e-03,\n",
       "                      -3.1800e-02, -2.9972e-02,  3.2469e-02, -2.9286e-02, -7.1523e-03,\n",
       "                       1.7112e-02, -1.6978e-02, -1.6839e-02, -1.1854e-02, -2.5129e-02,\n",
       "                      -3.1883e-02,  2.6553e-02, -2.1298e-02, -2.8419e-02,  1.0229e-02,\n",
       "                       6.6078e-03, -1.7447e-02, -3.5477e-02,  1.5489e-02,  6.7291e-04,\n",
       "                      -1.6617e-02, -8.4382e-03,  1.0599e-02,  2.6176e-03,  6.3263e-03,\n",
       "                      -1.8803e-02,  2.5744e-02,  9.9413e-03,  5.5121e-03, -3.8103e-03,\n",
       "                       1.7514e-02, -1.1449e-02, -3.4945e-02,  2.1761e-02, -8.1637e-03,\n",
       "                      -1.1286e-02,  2.9387e-02, -2.2952e-02, -2.4079e-02, -1.8401e-02,\n",
       "                      -1.4207e-02, -3.2443e-02,  7.2422e-03,  1.2693e-02,  3.3727e-02,\n",
       "                      -2.4124e-02, -2.4933e-03,  1.9875e-02, -1.3810e-02, -9.9844e-04,\n",
       "                      -3.0026e-02,  2.2539e-02,  2.5088e-02, -1.1933e-03,  2.6495e-02,\n",
       "                       2.3776e-02,  9.0351e-03,  9.4347e-03], device='cuda:0')),\n",
       "             ('classifier.last_dense.weight',\n",
       "              tensor([[-0.0081, -0.0165,  0.0294,  ...,  0.0146, -0.0080, -0.0199],\n",
       "                      [-0.0254,  0.0312, -0.0233,  ...,  0.0141, -0.0041,  0.0313],\n",
       "                      [ 0.0072, -0.0031,  0.0166,  ..., -0.0169,  0.0344, -0.0234],\n",
       "                      ...,\n",
       "                      [-0.0300, -0.0356,  0.0324,  ..., -0.0340,  0.0128,  0.0053],\n",
       "                      [ 0.0277,  0.0016, -0.0175,  ..., -0.0073, -0.0224, -0.0321],\n",
       "                      [-0.0065,  0.0308, -0.0336,  ..., -0.0214, -0.0125,  0.0225]],\n",
       "                     device='cuda:0')),\n",
       "             ('classifier.last_dense.bias',\n",
       "              tensor([ 2.8898e-02, -1.2413e-02, -3.1787e-02, -2.5065e-02, -3.4208e-02,\n",
       "                      -1.5428e-02,  3.4726e-02,  2.4542e-02, -1.8967e-02,  2.5561e-02,\n",
       "                      -4.0279e-03,  3.5995e-02, -2.8420e-02,  1.7254e-02,  2.8542e-02,\n",
       "                      -2.8999e-02,  5.9054e-03,  3.3765e-02, -3.0530e-02, -2.3328e-02,\n",
       "                      -9.1454e-03,  5.8235e-04,  6.4550e-05,  2.6056e-02, -1.7255e-02,\n",
       "                      -2.2026e-02,  1.9626e-02, -3.4510e-02, -4.2760e-03,  2.4517e-02,\n",
       "                      -2.6992e-02, -1.4221e-02,  3.2853e-02,  2.6131e-02, -1.4664e-02,\n",
       "                       2.7336e-02, -7.3973e-03, -1.2994e-02, -2.3865e-03,  1.0204e-02,\n",
       "                      -3.0286e-02,  3.2885e-02, -2.9637e-02,  3.3079e-02, -2.8271e-02,\n",
       "                      -1.7363e-02, -1.2778e-02, -9.2706e-03, -2.1872e-02,  3.4547e-02,\n",
       "                       2.8346e-02,  7.2893e-03, -1.3563e-02,  1.8058e-02,  7.6887e-03,\n",
       "                       1.1422e-02,  1.1388e-02, -1.5556e-02, -4.8864e-04,  3.7764e-04,\n",
       "                      -2.9694e-02, -3.0391e-03, -5.1215e-03, -3.5021e-02], device='cuda:0')),\n",
       "             ('classifier.out_proj.weight',\n",
       "              tensor([[ 7.1944e-02,  2.3986e-02,  5.5221e-02,  2.3842e-07,  4.9448e-03,\n",
       "                        1.1872e-01, -8.8611e-02,  8.5155e-02, -8.1394e-02,  4.3453e-02,\n",
       "                        8.3852e-02, -3.5051e-02, -3.1633e-02,  1.6040e-02,  8.8016e-02,\n",
       "                       -9.8540e-02,  9.6912e-02,  1.1990e-01, -4.4376e-03, -1.0660e-01,\n",
       "                       -7.6444e-02, -6.8202e-03, -1.7777e-02,  6.4003e-02, -9.4649e-02,\n",
       "                       -6.1332e-02,  2.5503e-02, -1.9810e-02, -8.4232e-02, -1.1919e-01,\n",
       "                        8.9418e-02,  1.1792e-01, -3.6199e-02, -4.8530e-02, -8.2247e-02,\n",
       "                       -1.3656e-02, -2.9225e-02, -9.9300e-02,  4.2478e-03,  2.2097e-03,\n",
       "                        1.0913e-01, -2.7383e-02,  1.1647e-01, -3.5787e-02,  6.7224e-02,\n",
       "                       -4.9989e-02,  6.0975e-03, -9.7938e-02,  8.6907e-02, -1.1795e-01,\n",
       "                       -7.3715e-02, -3.7611e-05,  9.8425e-02,  8.6029e-02,  3.6849e-02,\n",
       "                       -6.3872e-02, -2.2988e-02, -2.2071e-02, -1.0531e-01, -9.7049e-02,\n",
       "                       -2.0768e-02, -4.1688e-02, -7.2904e-02,  2.6389e-02],\n",
       "                      [-7.0209e-02, -1.2354e-01,  1.1065e-01,  1.1044e-01, -7.4084e-02,\n",
       "                       -5.4644e-02, -3.8616e-02, -2.6444e-02,  2.0454e-02,  1.1224e-02,\n",
       "                        4.8888e-02, -4.5516e-02,  1.2132e-01, -3.4135e-02,  7.0322e-02,\n",
       "                       -6.9515e-02,  8.1574e-02, -1.1384e-01, -2.0012e-02,  3.5008e-02,\n",
       "                       -9.1615e-03, -1.2171e-01,  4.9354e-02,  1.2342e-01,  5.9035e-02,\n",
       "                       -1.1244e-01, -1.0923e-01,  1.2381e-01, -4.2290e-02, -8.6093e-02,\n",
       "                        5.0250e-02,  2.4682e-02, -7.6639e-03, -3.7191e-02,  3.3429e-02,\n",
       "                        1.1841e-01, -1.2152e-03,  8.7144e-02, -9.9352e-02,  8.3514e-02,\n",
       "                        2.6657e-02, -1.5769e-03, -4.6214e-02,  1.9640e-02,  1.0933e-01,\n",
       "                        1.1263e-01,  7.7509e-02,  1.1975e-01, -6.0384e-02,  1.0411e-01,\n",
       "                        7.1098e-02, -7.0624e-02,  9.5778e-02,  1.0533e-01, -1.0074e-01,\n",
       "                        1.9107e-03,  8.8469e-02,  1.0836e-01, -4.5628e-02,  1.1467e-01,\n",
       "                        1.1498e-01, -3.8141e-02,  1.1764e-01, -1.1223e-01]], device='cuda:0')),\n",
       "             ('classifier.out_proj.bias',\n",
       "              tensor([ 0.0928, -0.0985], device='cuda:0'))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "temivef_train_NOTE_TARGET1_FT_path = '/home/ugrads/a/aa_ron_su/JSS_SUBMISSION/data/till_end_mimic_iv_extra_features_train_NOTE_TARGET1_FT_rad.csv'\n",
    "train = pd.read_csv(temivef_train_NOTE_TARGET1_FT_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def group_train_test(ID):\n",
    "    ID             = ID.astype(int)\n",
    "    ID_unique_srtd = np.unique(ID)\n",
    "    np.random.shuffle(ID_unique_srtd)    \n",
    "\n",
    "    num_train_ids = int(.80 * len(ID_unique_srtd))\n",
    "    train_ids = ID_unique_srtd[:num_train_ids]\n",
    "    val_ids = ID_unique_srtd[num_train_ids:]\n",
    "\n",
    "    train = ID[ID.isin(train_ids)]\n",
    "    val = ID[ID.isin(val_ids)]\n",
    "\n",
    "    assert(len(train) + len(val) == len(ID))\n",
    "    assert(len(train_ids) + len(val_ids) == len(ID_unique_srtd))\n",
    "    assert(len(train_ids) + len(val_ids) == len(ID_unique_srtd))\n",
    "\n",
    "    return list(train.index), list(val.index)\n",
    "\n",
    "train_idxs, val_idxs = group_train_test(train['ICUSTAY_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train, val from ./Clinical-T5-Base_test_out/1//data_cache/\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "out_dir = './Clinical-T5-Base_test_out/1/'\n",
    "\n",
    "print(f'loading train, val from', f'{out_dir}/data_cache/')\n",
    "# train_data, val_data = Dataset, Dataset\n",
    "train_data = Dataset.load_from_disk(f'{out_dir}/data_cache/tokenized_train_data')\n",
    "val_data = Dataset.load_from_disk(f'{out_dir}/data_cache/tokenized_val_data')\n",
    "\n",
    "train_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "train_data = train_data.remove_columns('text')\n",
    "val_data = val_data.remove_columns('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # argmax(pred.predictions, axis=1)\n",
    "    #pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'checkpoint_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer, TrainingArguments\n\u001b[1;32m      2\u001b[0m \u001b[39m# Load the TrainingArguments object\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m training_args \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(checkpoint_dir, \u001b[39m\"\u001b[39m\u001b[39mtraining_args.bin\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m      5\u001b[0m \u001b[39m# Create a new Trainer instance with the loaded TrainingArguments\u001b[39;00m\n\u001b[1;32m      6\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      7\u001b[0m     model\u001b[39m=\u001b[39mclassifier,\n\u001b[1;32m      8\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     eval_dataset\u001b[39m=\u001b[39mval_data\n\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'checkpoint_dir' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "# Load the TrainingArguments object\n",
    "training_args = torch.load(os.path.join(checkpoint_dir, \"training_args.bin\"))\n",
    "\n",
    "# Create a new Trainer instance with the loaded TrainingArguments\n",
    "trainer = Trainer(\n",
    "    model=classifier,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data\n",
    ")\n",
    "\n",
    "# Load the model and other states into the Trainer\n",
    "# config = Autoconfig.from_pretrained(config_file)\n",
    "trainer.model = classifier\n",
    "# trainer.optimizer.load_state_dict(torch.load(os.path.join(checkpoint_dir, \"optimizer.pt\")))\n",
    "# trainer.lr_scheduler.load_state_dict(torch.load(os.path.join(checkpoint_dir, \"scheduler.pt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "T5Model.forward() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mevaluate()\n",
      "File \u001b[0;32m~/miniconda3/envs/clinical1/lib/python3.10/site-packages/transformers/trainer.py:2796\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2793\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   2795\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 2796\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m   2797\u001b[0m     eval_dataloader,\n\u001b[1;32m   2798\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   2799\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   2800\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   2801\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   2802\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[1;32m   2803\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[1;32m   2804\u001b[0m )\n\u001b[1;32m   2806\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[1;32m   2807\u001b[0m output\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mupdate(\n\u001b[1;32m   2808\u001b[0m     speed_metrics(\n\u001b[1;32m   2809\u001b[0m         metric_key_prefix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2813\u001b[0m     )\n\u001b[1;32m   2814\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/clinical1/lib/python3.10/site-packages/transformers/trainer.py:2974\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2971\u001b[0m         batch_size \u001b[39m=\u001b[39m observed_batch_size\n\u001b[1;32m   2973\u001b[0m \u001b[39m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 2974\u001b[0m loss, logits, labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprediction_step(model, inputs, prediction_loss_only, ignore_keys\u001b[39m=\u001b[39;49mignore_keys)\n\u001b[1;32m   2975\u001b[0m inputs_decode \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_input(inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39minclude_inputs_for_metrics \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2977\u001b[0m \u001b[39mif\u001b[39;00m is_torch_tpu_available():\n",
      "File \u001b[0;32m~/miniconda3/envs/clinical1/lib/python3.10/site-packages/transformers/trainer.py:3217\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   3215\u001b[0m \u001b[39mif\u001b[39;00m has_labels:\n\u001b[1;32m   3216\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3217\u001b[0m         loss, outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs, return_outputs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   3218\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m   3220\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mdict\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/clinical1/lib/python3.10/site-packages/transformers/trainer.py:2540\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2538\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2539\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2540\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2541\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2542\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2543\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/clinical1/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: T5Model.forward() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_steps=None,\n",
       "evaluation_strategy=epoch,\n",
       "fp16=True,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=8,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=False,\n",
       "group_by_length=False,\n",
       "half_precision_backend=cuda_amp,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=True,\n",
       "local_rank=-1,\n",
       "log_level=passive,\n",
       "log_level_replica=passive,\n",
       "log_on_each_node=True,\n",
       "logging_dir=Clinical-T5-Base_test_out/3/logs,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=8,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_type=linear,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=loss,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=1,\n",
       "optim=adamw_hf,\n",
       "output_dir=Clinical-T5-Base_test_out/3/results,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=4,\n",
       "per_device_train_batch_size=2,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=[],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=Clinical-T5-Base_radiology_run3,\n",
       "save_on_each_node=False,\n",
       "save_steps=500,\n",
       "save_strategy=epoch,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=200,\n",
       "weight_decay=0.01,\n",
       "xpu_backend=None,\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Trainer: evaluation requires an eval_dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mevaluate()\n",
      "File \u001b[0;32m~/miniconda3/envs/clinical1/lib/python3.10/site-packages/transformers/trainer.py:2792\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2789\u001b[0m \u001b[39m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m   2790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_memory_tracker\u001b[39m.\u001b[39mstart()\n\u001b[0;32m-> 2792\u001b[0m eval_dataloader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_eval_dataloader(eval_dataset)\n\u001b[1;32m   2793\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   2795\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n",
      "File \u001b[0;32m~/miniconda3/envs/clinical1/lib/python3.10/site-packages/transformers/trainer.py:922\u001b[0m, in \u001b[0;36mTrainer.get_eval_dataloader\u001b[0;34m(self, eval_dataset)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    912\u001b[0m \u001b[39mReturns the evaluation [`~torch.utils.data.DataLoader`].\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[39m        by the `model.forward()` method are automatically removed. It must implement `__len__`.\u001b[39;00m\n\u001b[1;32m    920\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    921\u001b[0m \u001b[39mif\u001b[39;00m eval_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 922\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTrainer: evaluation requires an eval_dataset.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    923\u001b[0m eval_dataset \u001b[39m=\u001b[39m eval_dataset \u001b[39mif\u001b[39;00m eval_dataset \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_dataset\n\u001b[1;32m    924\u001b[0m data_collator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_collator\n",
      "\u001b[0;31mValueError\u001b[0m: Trainer: evaluation requires an eval_dataset."
     ]
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [clin_t5_config, myconfig] # clin_t5_config, T5EncoderForSequenceClassification_config\n",
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clinical1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
